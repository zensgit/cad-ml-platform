#!/usr/bin/env python3
"""
Resilience Performance Benchmark
éŸ§æ€§å±‚æ€§èƒ½åŸºå‡†æµ‹è¯• - å¯¹æ¯”å¯ç”¨/ç¦ç”¨ Resilience ç»„ä»¶çš„æ€§èƒ½å·®å¼‚

æµ‹è¯•ç»´åº¦ï¼š
1. å•çº¿ç¨‹ vs å¤šçº¿ç¨‹
2. å„ç»„ä»¶ç‹¬ç«‹æ€§èƒ½
3. ç»„åˆä½¿ç”¨æ€§èƒ½
4. ä¸åŒè´Ÿè½½ä¸‹çš„è¡¨ç°
5. é”äº‰ç”¨åˆ†æ
"""

import asyncio
import time
import statistics
import json
import sys
from typing import Dict, List, Any, Callable, Optional
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from threading import Lock
import tracemalloc
import psutil
import os
from datetime import datetime

# å¯¼å…¥ Resilience ç»„ä»¶
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from src.core.resilience import (
        circuit_breaker,
        rate_limit,
        retry,
        bulkhead,
        with_resilience
    )
    RESILIENCE_AVAILABLE = True
except ImportError:
    RESILIENCE_AVAILABLE = False
    print("Warning: Resilience components not available, using mock")


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    config: Dict[str, Any]
    metrics: Dict[str, float]
    overhead: float  # æ€§èƒ½å¼€é”€ç™¾åˆ†æ¯”
    verdict: str  # PASS/FAIL/WARNING


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    total_time: float = 0.0
    avg_latency: float = 0.0
    p50_latency: float = 0.0
    p95_latency: float = 0.0
    p99_latency: float = 0.0
    throughput: float = 0.0
    error_rate: float = 0.0
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    lock_wait_time: float = 0.0


class ResilienceBenchmark:
    """éŸ§æ€§å±‚æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.results: List[BenchmarkResult] = []
        self.baseline_metrics: Dict[str, PerformanceMetrics] = {}
        self.resilience_metrics: Dict[str, PerformanceMetrics] = {}

        # æ€§èƒ½é˜ˆå€¼
        self.thresholds = {
            "max_overhead_p95": 0.05,  # P95 å¢å¹… â‰¤5%
            "max_overhead_avg": 0.03,  # å¹³å‡å¢å¹… â‰¤3%
            "max_memory_overhead_mb": 50,  # å†…å­˜å¢åŠ  â‰¤50MB
            "max_lock_wait_ms": 10  # é”ç­‰å¾… â‰¤10ms
        }

    # ========== æµ‹è¯•ç›®æ ‡å‡½æ•° ==========

    def target_function_fast(self, x: int) -> int:
        """å¿«é€Ÿç›®æ ‡å‡½æ•°ï¼ˆ~1msï¼‰"""
        time.sleep(0.001)  # 1ms
        return x * 2

    def target_function_medium(self, x: int) -> int:
        """ä¸­é€Ÿç›®æ ‡å‡½æ•°ï¼ˆ~10msï¼‰"""
        time.sleep(0.01)  # 10ms
        # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
        result = sum(i ** 2 for i in range(100))
        return x * result

    def target_function_slow(self, x: int) -> int:
        """æ…¢é€Ÿç›®æ ‡å‡½æ•°ï¼ˆ~100msï¼‰"""
        time.sleep(0.1)  # 100ms
        # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
        result = sum(i ** 2 for i in range(1000))
        return x * result

    async def async_target_function(self, x: int) -> int:
        """å¼‚æ­¥ç›®æ ‡å‡½æ•°"""
        await asyncio.sleep(0.01)
        return x * 2

    # ========== åŸºå‡†æµ‹è¯•æ‰§è¡Œ ==========

    def run_baseline(
        self,
        func: Callable,
        iterations: int = 1000,
        concurrency: int = 1
    ) -> PerformanceMetrics:
        """è¿è¡ŒåŸºçº¿æµ‹è¯•ï¼ˆæ—  Resilienceï¼‰"""
        metrics = PerformanceMetrics()
        latencies = []
        errors = 0

        # è®°å½•å¼€å§‹çŠ¶æ€
        tracemalloc.start()
        start_time = time.time()
        process = psutil.Process()
        start_cpu = process.cpu_percent()

        if concurrency == 1:
            # å•çº¿ç¨‹æ‰§è¡Œ
            for i in range(iterations):
                try:
                    op_start = time.perf_counter()
                    func(i)
                    op_time = time.perf_counter() - op_start
                    latencies.append(op_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                except Exception:
                    errors += 1

        else:
            # å¤šçº¿ç¨‹æ‰§è¡Œ
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = []
                for i in range(iterations):
                    op_start = time.perf_counter()
                    future = executor.submit(func, i)
                    futures.append((future, op_start))

                for future, op_start in futures:
                    try:
                        future.result(timeout=5)
                        op_time = time.perf_counter() - op_start
                        latencies.append(op_time * 1000)
                    except Exception:
                        errors += 1

        # è®¡ç®—æŒ‡æ ‡
        total_time = time.time() - start_time
        current_mem, peak_mem = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        metrics.total_time = total_time
        metrics.throughput = iterations / total_time if total_time > 0 else 0
        metrics.error_rate = errors / iterations if iterations > 0 else 0
        metrics.memory_usage_mb = peak_mem / 1024 / 1024
        metrics.cpu_usage_percent = process.cpu_percent() - start_cpu

        if latencies:
            metrics.avg_latency = statistics.mean(latencies)
            metrics.p50_latency = statistics.quantiles(latencies, n=2)[0] if len(latencies) > 1 else latencies[0]
            metrics.p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) > 19 else metrics.p50_latency
            metrics.p99_latency = statistics.quantiles(latencies, n=100)[98] if len(latencies) > 99 else metrics.p95_latency

        return metrics

    def run_with_circuit_breaker(
        self,
        func: Callable,
        iterations: int = 1000,
        concurrency: int = 1
    ) -> PerformanceMetrics:
        """è¿è¡Œå¸¦ Circuit Breaker çš„æµ‹è¯•"""
        if not RESILIENCE_AVAILABLE:
            return self.run_baseline(func, iterations, concurrency)

        @circuit_breaker(
            failure_threshold=5,
            recovery_timeout=30,
            half_open_max_calls=3
        )
        def protected_func(x):
            return func(x)

        return self._run_protected(protected_func, iterations, concurrency)

    def run_with_rate_limiter(
        self,
        func: Callable,
        iterations: int = 1000,
        concurrency: int = 1
    ) -> PerformanceMetrics:
        """è¿è¡Œå¸¦ Rate Limiter çš„æµ‹è¯•"""
        if not RESILIENCE_AVAILABLE:
            return self.run_baseline(func, iterations, concurrency)

        @rate_limit(
            rate=100,  # 100 req/s
            burst=150
        )
        def protected_func(x):
            return func(x)

        return self._run_protected(protected_func, iterations, concurrency)

    def run_with_retry(
        self,
        func: Callable,
        iterations: int = 1000,
        concurrency: int = 1
    ) -> PerformanceMetrics:
        """è¿è¡Œå¸¦ Retry çš„æµ‹è¯•"""
        if not RESILIENCE_AVAILABLE:
            return self.run_baseline(func, iterations, concurrency)

        @retry(
            max_attempts=3,
            delay=0.1
        )
        def protected_func(x):
            return func(x)

        return self._run_protected(protected_func, iterations, concurrency)

    def run_with_bulkhead(
        self,
        func: Callable,
        iterations: int = 1000,
        concurrency: int = 1
    ) -> PerformanceMetrics:
        """è¿è¡Œå¸¦ Bulkhead çš„æµ‹è¯•"""
        if not RESILIENCE_AVAILABLE:
            return self.run_baseline(func, iterations, concurrency)

        @bulkhead(
            max_concurrent_calls=10,
            max_wait_duration=1.0
        )
        def protected_func(x):
            return func(x)

        return self._run_protected(protected_func, iterations, concurrency)

    def run_with_all_resilience(
        self,
        func: Callable,
        iterations: int = 1000,
        concurrency: int = 1
    ) -> PerformanceMetrics:
        """è¿è¡Œå¸¦æ‰€æœ‰ Resilience ç»„ä»¶çš„æµ‹è¯•"""
        if not RESILIENCE_AVAILABLE:
            return self.run_baseline(func, iterations, concurrency)

        @with_resilience(
            circuit_breaker_config={
                "failure_threshold": 5,
                "recovery_timeout": 30
            },
            rate_limiter_config={
                "rate": 100,
                "burst": 150
            },
            retry_config={
                "max_attempts": 3,
                "delay": 0.1
            },
            bulkhead_config={
                "max_concurrent_calls": 10
            }
        )
        def protected_func(x):
            return func(x)

        return self._run_protected(protected_func, iterations, concurrency)

    def _run_protected(
        self,
        func: Callable,
        iterations: int,
        concurrency: int
    ) -> PerformanceMetrics:
        """è¿è¡Œå—ä¿æŠ¤çš„å‡½æ•°"""
        metrics = PerformanceMetrics()
        latencies = []
        errors = 0
        lock_waits = []

        tracemalloc.start()
        start_time = time.time()
        process = psutil.Process()
        start_cpu = process.cpu_percent()

        if concurrency == 1:
            for i in range(iterations):
                try:
                    op_start = time.perf_counter()
                    func(i)
                    op_time = time.perf_counter() - op_start
                    latencies.append(op_time * 1000)
                except Exception:
                    errors += 1

        else:
            # æµ‹é‡é”ç­‰å¾…æ—¶é—´
            lock = Lock()
            lock_wait_total = 0

            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = []
                for i in range(iterations):
                    op_start = time.perf_counter()

                    # æ¨¡æ‹Ÿé”äº‰ç”¨æµ‹é‡
                    lock_start = time.perf_counter()
                    with lock:
                        lock_wait = time.perf_counter() - lock_start
                        lock_waits.append(lock_wait * 1000)
                        future = executor.submit(func, i)

                    futures.append((future, op_start))

                for future, op_start in futures:
                    try:
                        future.result(timeout=5)
                        op_time = time.perf_counter() - op_start
                        latencies.append(op_time * 1000)
                    except Exception:
                        errors += 1

        total_time = time.time() - start_time
        current_mem, peak_mem = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        metrics.total_time = total_time
        metrics.throughput = iterations / total_time if total_time > 0 else 0
        metrics.error_rate = errors / iterations if iterations > 0 else 0
        metrics.memory_usage_mb = peak_mem / 1024 / 1024
        metrics.cpu_usage_percent = process.cpu_percent() - start_cpu

        if latencies:
            metrics.avg_latency = statistics.mean(latencies)
            metrics.p50_latency = statistics.quantiles(latencies, n=2)[0] if len(latencies) > 1 else latencies[0]
            metrics.p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) > 19 else metrics.p50_latency
            metrics.p99_latency = statistics.quantiles(latencies, n=100)[98] if len(latencies) > 99 else metrics.p95_latency

        if lock_waits:
            metrics.lock_wait_time = statistics.mean(lock_waits)

        return metrics

    # ========== åŸºå‡†æµ‹è¯•å¥—ä»¶ ==========

    def benchmark_single_thread(self):
        """å•çº¿ç¨‹åŸºå‡†æµ‹è¯•"""
        print("\nğŸ”§ Running single-thread benchmarks...")

        test_functions = [
            ("fast", self.target_function_fast, 1000),
            ("medium", self.target_function_medium, 500),
            ("slow", self.target_function_slow, 100)
        ]

        for func_name, func, iterations in test_functions:
            print(f"\n  Testing {func_name} function...")

            # åŸºçº¿
            baseline = self.run_baseline(func, iterations, 1)
            self.baseline_metrics[f"single_{func_name}"] = baseline

            # Circuit Breaker
            cb_metrics = self.run_with_circuit_breaker(func, iterations, 1)
            self._record_result(f"single_{func_name}_circuit_breaker", baseline, cb_metrics)

            # Rate Limiter
            rl_metrics = self.run_with_rate_limiter(func, iterations, 1)
            self._record_result(f"single_{func_name}_rate_limiter", baseline, rl_metrics)

            # Retry
            retry_metrics = self.run_with_retry(func, iterations, 1)
            self._record_result(f"single_{func_name}_retry", baseline, retry_metrics)

            # Bulkhead
            bulkhead_metrics = self.run_with_bulkhead(func, iterations, 1)
            self._record_result(f"single_{func_name}_bulkhead", baseline, bulkhead_metrics)

            # All combined
            all_metrics = self.run_with_all_resilience(func, iterations, 1)
            self._record_result(f"single_{func_name}_all", baseline, all_metrics)

    def benchmark_multi_thread(self):
        """å¤šçº¿ç¨‹åŸºå‡†æµ‹è¯•"""
        print("\nğŸ”§ Running multi-thread benchmarks...")

        test_configs = [
            ("low_concurrency", 4),
            ("medium_concurrency", 10),
            ("high_concurrency", 20)
        ]

        func = self.target_function_medium
        iterations = 500

        for config_name, concurrency in test_configs:
            print(f"\n  Testing with {config_name} ({concurrency} threads)...")

            # åŸºçº¿
            baseline = self.run_baseline(func, iterations, concurrency)
            self.baseline_metrics[f"multi_{config_name}"] = baseline

            # Circuit Breaker
            cb_metrics = self.run_with_circuit_breaker(func, iterations, concurrency)
            self._record_result(f"multi_{config_name}_circuit_breaker", baseline, cb_metrics)

            # Rate Limiter
            rl_metrics = self.run_with_rate_limiter(func, iterations, concurrency)
            self._record_result(f"multi_{config_name}_rate_limiter", baseline, rl_metrics)

            # Bulkhead (ç‰¹åˆ«é‡è¦çš„å¹¶å‘æµ‹è¯•)
            bulkhead_metrics = self.run_with_bulkhead(func, iterations, concurrency)
            self._record_result(f"multi_{config_name}_bulkhead", baseline, bulkhead_metrics)

            # All combined
            all_metrics = self.run_with_all_resilience(func, iterations, concurrency)
            self._record_result(f"multi_{config_name}_all", baseline, all_metrics)

    def benchmark_stress_test(self):
        """å‹åŠ›æµ‹è¯•"""
        print("\nğŸ”§ Running stress tests...")

        # é«˜è´Ÿè½½æµ‹è¯•
        print("\n  High load test...")
        baseline = self.run_baseline(self.target_function_fast, 10000, 20)
        resilience = self.run_with_all_resilience(self.target_function_fast, 10000, 20)
        self._record_result("stress_high_load", baseline, resilience)

        # é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
        print("\n  Long running test...")
        baseline = self.run_baseline(self.target_function_medium, 1000, 10)
        resilience = self.run_with_all_resilience(self.target_function_medium, 1000, 10)
        self._record_result("stress_long_running", baseline, resilience)

    async def benchmark_async(self):
        """å¼‚æ­¥æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ”§ Running async benchmarks...")

        # æš‚æ—¶è·³è¿‡å¼‚æ­¥æµ‹è¯•ï¼ˆéœ€è¦å¼‚æ­¥ç‰ˆæœ¬çš„ Resilience ç»„ä»¶ï¼‰
        print("  Async benchmarks skipped (requires async resilience components)")

    def _record_result(
        self,
        test_name: str,
        baseline: PerformanceMetrics,
        with_resilience: PerformanceMetrics
    ):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        # è®¡ç®—å¼€é”€
        overhead_p95 = (
            (with_resilience.p95_latency - baseline.p95_latency) / baseline.p95_latency
            if baseline.p95_latency > 0 else 0
        )
        overhead_avg = (
            (with_resilience.avg_latency - baseline.avg_latency) / baseline.avg_latency
            if baseline.avg_latency > 0 else 0
        )
        memory_overhead = with_resilience.memory_usage_mb - baseline.memory_usage_mb

        # åˆ¤å®šç»“æœ
        verdict = "PASS"
        if overhead_p95 > self.thresholds["max_overhead_p95"]:
            verdict = "FAIL"
        elif overhead_avg > self.thresholds["max_overhead_avg"]:
            verdict = "WARNING"
        elif memory_overhead > self.thresholds["max_memory_overhead_mb"]:
            verdict = "WARNING"
        elif with_resilience.lock_wait_time > self.thresholds["max_lock_wait_ms"]:
            verdict = "WARNING"

        result = BenchmarkResult(
            test_name=test_name,
            config={
                "iterations": 1000,  # ç®€åŒ–ï¼Œå®é™…åº”ä¼ å…¥
                "concurrency": 1 if "single" in test_name else 10
            },
            metrics={
                "baseline_p95": baseline.p95_latency,
                "resilience_p95": with_resilience.p95_latency,
                "overhead_p95_percent": overhead_p95 * 100,
                "baseline_avg": baseline.avg_latency,
                "resilience_avg": with_resilience.avg_latency,
                "overhead_avg_percent": overhead_avg * 100,
                "memory_overhead_mb": memory_overhead,
                "lock_wait_ms": with_resilience.lock_wait_time,
                "baseline_throughput": baseline.throughput,
                "resilience_throughput": with_resilience.throughput
            },
            overhead=overhead_p95,
            verdict=verdict
        )

        self.results.append(result)
        self.resilience_metrics[test_name] = with_resilience

    def generate_report(self) -> Dict[str, Any]:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        passed = sum(1 for r in self.results if r.verdict == "PASS")
        warned = sum(1 for r in self.results if r.verdict == "WARNING")
        failed = sum(1 for r in self.results if r.verdict == "FAIL")

        # è®¡ç®—å¹³å‡å¼€é”€
        avg_overhead_p95 = statistics.mean(
            r.overhead for r in self.results
        ) if self.results else 0

        # æ‰¾å‡ºæœ€å·®æƒ…å†µ
        worst_case = max(
            self.results,
            key=lambda r: r.overhead
        ) if self.results else None

        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(self.results),
                "passed": passed,
                "warnings": warned,
                "failures": failed,
                "avg_overhead_p95": f"{avg_overhead_p95 * 100:.2f}%",
                "verdict": "PASS" if failed == 0 else "FAIL"
            },
            "thresholds": self.thresholds,
            "worst_case": {
                "test": worst_case.test_name if worst_case else "N/A",
                "overhead": f"{worst_case.overhead * 100:.2f}%" if worst_case else "N/A"
            },
            "results": [
                {
                    "test": r.test_name,
                    "verdict": r.verdict,
                    "p95_overhead": f"{r.metrics['overhead_p95_percent']:.2f}%",
                    "avg_overhead": f"{r.metrics['overhead_avg_percent']:.2f}%",
                    "memory_overhead": f"{r.metrics['memory_overhead_mb']:.1f} MB",
                    "lock_wait": f"{r.metrics['lock_wait_ms']:.2f} ms"
                }
                for r in self.results
            ]
        }

        return report

    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        report = self.generate_report()

        print("\n" + "=" * 60)
        print("ğŸ“Š Resilience Performance Benchmark Summary")
        print("=" * 60)

        summary = report["summary"]
        print(f"\nâœ… Passed: {summary['passed']}")
        print(f"âš ï¸  Warnings: {summary['warnings']}")
        print(f"âŒ Failed: {summary['failures']}")
        print(f"\nğŸ“ˆ Average P95 Overhead: {summary['avg_overhead_p95']}")

        if report["worst_case"]["test"] != "N/A":
            print(f"\nğŸ”´ Worst Case:")
            print(f"   Test: {report['worst_case']['test']}")
            print(f"   Overhead: {report['worst_case']['overhead']}")

        print(f"\nğŸ¯ Overall Verdict: {summary['verdict']}")

        if summary["verdict"] == "FAIL":
            print("\nâš ï¸  Performance regression detected!")
            print("   Resilience overhead exceeds acceptable thresholds.")

        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Benchmark resilience components performance"
    )
    parser.add_argument(
        "--quick",
        action="store_true",
        help="Run quick benchmark (reduced iterations)"
    )
    parser.add_argument(
        "--output",
        "-o",
        help="Output file for JSON report"
    )
    parser.add_argument(
        "--skip-stress",
        action="store_true",
        help="Skip stress tests"
    )

    args = parser.parse_args()

    # åˆ›å»ºåŸºå‡†æµ‹è¯•
    benchmark = ResilienceBenchmark()

    print("ğŸš€ Starting Resilience Performance Benchmark...")
    print(f"   Available: {RESILIENCE_AVAILABLE}")

    # è¿è¡Œæµ‹è¯•å¥—ä»¶
    benchmark.benchmark_single_thread()
    benchmark.benchmark_multi_thread()

    if not args.skip_stress and not args.quick:
        benchmark.benchmark_stress_test()

    # ç”ŸæˆæŠ¥å‘Š
    report = benchmark.generate_report()

    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(report, f, indent=2)
        print(f"\nğŸ“ Report saved to: {args.output}")

    # æ‰“å°æ‘˜è¦
    benchmark.print_summary()

    # è¿”å›çŠ¶æ€ç 
    return 0 if report["summary"]["verdict"] == "PASS" else 1


if __name__ == "__main__":
    sys.exit(main())
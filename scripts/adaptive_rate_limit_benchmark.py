#!/usr/bin/env python3
"""
Adaptive Rate Limiter Performance Benchmark
è‡ªé€‚åº”é™æµå™¨æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import time
import json
import statistics
import threading
import concurrent.futures
from pathlib import Path
from typing import List, Dict, Any, Callable
from dataclasses import dataclass, asdict
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent.parent))

from src.core.resilience.adaptive_rate_limiter import (
    AdaptiveRateLimiter,
    AdaptiveConfig,
    AdaptivePhase
)
from src.core.resilience.adaptive_decorator import adaptive_rate_limit


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    scenario: str
    enabled: bool
    requests: int
    duration: float
    throughput: float
    latency_p50: float
    latency_p90: float
    latency_p95: float
    latency_p99: float
    overhead_percentage: float
    adjustments_made: int
    final_phase: str
    final_rate: float


class AdaptiveRateLimiterBenchmark:
    """è‡ªé€‚åº”é™æµå™¨åŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.results: List[BenchmarkResult] = []
        self.baseline_latencies: List[float] = []

    def benchmark_function(self, delay_ms: float = 10.0):
        """åŸºå‡†æµ‹è¯•å‡½æ•°ï¼ˆæ¨¡æ‹Ÿä¸šåŠ¡é€»è¾‘ï¼‰"""
        time.sleep(delay_ms / 1000)
        return "success"

    def run_scenario(
        self,
        scenario_name: str,
        num_requests: int,
        num_threads: int,
        error_rate: float,
        use_adaptive: bool,
        config: AdaptiveConfig = None
    ) -> BenchmarkResult:
        """è¿è¡Œæµ‹è¯•åœºæ™¯"""
        print(f"\nğŸ” Running scenario: {scenario_name}")
        print(f"   Requests: {num_requests}, Threads: {num_threads}, Error rate: {error_rate:.2%}")
        print(f"   Adaptive: {'Enabled' if use_adaptive else 'Disabled'}")

        if use_adaptive and config:
            # åˆ›å»ºè‡ªé€‚åº”é™æµå™¨
            limiter = AdaptiveRateLimiter("benchmark", scenario_name, config)
            limiter.set_baseline(15.0)  # åŸºçº¿15ms
        else:
            limiter = None

        latencies = []
        errors = 0
        success = 0
        start_time = time.time()

        def worker(request_id: int):
            nonlocal errors, success

            request_start = time.time()

            try:
                # å¦‚æœå¯ç”¨è‡ªé€‚åº”é™æµ
                if limiter:
                    if not limiter.acquire():
                        errors += 1
                        return None

                # æ¨¡æ‹Ÿé”™è¯¯
                if np.random.random() < error_rate:
                    if limiter:
                        limiter.record_error()
                    errors += 1
                    raise Exception("Simulated error")

                # æ‰§è¡Œå‡½æ•°
                result = self.benchmark_function()

                if limiter:
                    limiter.record_success()
                success += 1

                return result

            finally:
                latency_ms = (time.time() - request_start) * 1000
                latencies.append(latency_ms)

                if limiter:
                    limiter.record_latency(latency_ms)

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = []
            for i in range(num_requests):
                future = executor.submit(worker, i)
                futures.append(future)

                # å®šæœŸè¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if limiter and i % 100 == 0:
                    limiter.evaluate_and_adjust()

            # ç­‰å¾…å®Œæˆ
            concurrent.futures.wait(futures)

        duration = time.time() - start_time
        throughput = num_requests / duration

        # è®¡ç®—å»¶è¿Ÿç™¾åˆ†ä½
        sorted_latencies = sorted(latencies)
        p50 = np.percentile(sorted_latencies, 50)
        p90 = np.percentile(sorted_latencies, 90)
        p95 = np.percentile(sorted_latencies, 95)
        p99 = np.percentile(sorted_latencies, 99)

        # è®¡ç®—å¼€é”€
        if not use_adaptive:
            self.baseline_latencies = latencies.copy()
            overhead = 0.0
        else:
            if self.baseline_latencies:
                baseline_p95 = np.percentile(self.baseline_latencies, 95)
                overhead = ((p95 - baseline_p95) / baseline_p95) * 100
            else:
                overhead = 0.0

        # è·å–æœ€ç»ˆçŠ¶æ€
        if limiter:
            status = limiter.get_status()
            final_phase = status["phase"]
            final_rate = status["current_rate"]
            adjustments = len(status.get("recent_adjustments", []))
        else:
            final_phase = "N/A"
            final_rate = 0.0
            adjustments = 0

        result = BenchmarkResult(
            scenario=scenario_name,
            enabled=use_adaptive,
            requests=num_requests,
            duration=duration,
            throughput=throughput,
            latency_p50=p50,
            latency_p90=p90,
            latency_p95=p95,
            latency_p99=p99,
            overhead_percentage=overhead,
            adjustments_made=adjustments,
            final_phase=final_phase,
            final_rate=final_rate
        )

        self.results.append(result)

        # æ‰“å°ç»“æœ
        print(f"   âœ… Completed in {duration:.2f}s")
        print(f"   Throughput: {throughput:.2f} req/s")
        print(f"   P95 Latency: {p95:.2f}ms (overhead: {overhead:+.2f}%)")
        if limiter:
            print(f"   Final phase: {final_phase}, Rate: {final_rate:.2f}")

        return result

    def run_all_scenarios(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•åœºæ™¯"""
        print("=" * 60)
        print("ğŸš€ Adaptive Rate Limiter Performance Benchmark")
        print("=" * 60)

        # é…ç½®
        config = AdaptiveConfig(
            base_rate=1000.0,
            error_threshold=0.02,
            recover_threshold=0.008,
            latency_p95_threshold_multiplier=1.5,
            min_rate_ratio=0.15,
            adjust_min_interval_ms=1000,
            recover_step=0.1,
            error_alpha=0.25
        )

        scenarios = [
            # åœºæ™¯1ï¼šæ­£å¸¸è´Ÿè½½åŸºçº¿ï¼ˆæ— è‡ªé€‚åº”ï¼‰
            ("Normal Load Baseline", 1000, 10, 0.01, False, None),

            # åœºæ™¯2ï¼šæ­£å¸¸è´Ÿè½½ï¼ˆæœ‰è‡ªé€‚åº”ï¼‰
            ("Normal Load Adaptive", 1000, 10, 0.01, True, config),

            # åœºæ™¯3ï¼šé”™è¯¯æ¿€å¢åŸºçº¿
            ("Error Spike Baseline", 1000, 10, 0.10, False, None),

            # åœºæ™¯4ï¼šé”™è¯¯æ¿€å¢è‡ªé€‚åº”
            ("Error Spike Adaptive", 1000, 10, 0.10, True, config),

            # åœºæ™¯5ï¼šé«˜å¹¶å‘åŸºçº¿
            ("High Concurrency Baseline", 2000, 50, 0.02, False, None),

            # åœºæ™¯6ï¼šé«˜å¹¶å‘è‡ªé€‚åº”
            ("High Concurrency Adaptive", 2000, 50, 0.02, True, config),

            # åœºæ™¯7ï¼šæ¸è¿›é”™è¯¯ï¼ˆæ¨¡æ‹Ÿç³»ç»Ÿé€€åŒ–ï¼‰
            ("Gradual Degradation", 2000, 20, 0.05, True, config),
        ]

        for scenario in scenarios:
            self.run_scenario(*scenario)
            time.sleep(1)  # åœºæ™¯é—´éš”

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()

    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Performance Benchmark Report")
        print("=" * 60)

        # æŒ‰åœºæ™¯é…å¯¹åˆ†æ
        paired_results = {}
        for result in self.results:
            base_name = result.scenario.replace(" Baseline", "").replace(" Adaptive", "")
            if base_name not in paired_results:
                paired_results[base_name] = {}

            if "Baseline" in result.scenario:
                paired_results[base_name]["baseline"] = result
            else:
                paired_results[base_name]["adaptive"] = result

        print("\n### Overhead Analysis")
        print("-" * 40)

        overhead_list = []
        for scenario_name, pair in paired_results.items():
            if "baseline" in pair and "adaptive" in pair:
                baseline = pair["baseline"]
                adaptive = pair["adaptive"]

                overhead_p95 = ((adaptive.latency_p95 - baseline.latency_p95) / baseline.latency_p95) * 100
                overhead_p99 = ((adaptive.latency_p99 - baseline.latency_p99) / baseline.latency_p99) * 100

                overhead_list.append(overhead_p95)

                print(f"\n{scenario_name}:")
                print(f"  P95 Overhead: {overhead_p95:+.2f}%")
                print(f"  P99 Overhead: {overhead_p99:+.2f}%")
                print(f"  Throughput Impact: {(adaptive.throughput - baseline.throughput):+.2f} req/s")

                if adaptive.adjustments_made > 0:
                    print(f"  Adjustments Made: {adaptive.adjustments_made}")
                    print(f"  Final Phase: {adaptive.final_phase}")
                    print(f"  Final Rate: {adaptive.final_rate:.2f}")

        # éªŒè¯æ ‡å‡†
        print("\n### Validation Against Requirements")
        print("-" * 40)

        avg_overhead = statistics.mean(overhead_list) if overhead_list else 0
        max_overhead = max(overhead_list) if overhead_list else 0

        print(f"Average P95 Overhead: {avg_overhead:.2f}%")
        print(f"Maximum P95 Overhead: {max_overhead:.2f}%")
        print(f"Target: â‰¤5%")

        if max_overhead <= 5.0:
            print("âœ… PASSED: Overhead within acceptable range")
        else:
            print("âŒ FAILED: Overhead exceeds 5% threshold")

        # ä¿å­˜JSONæŠ¥å‘Š
        self.save_json_report()

    def save_json_report(self):
        """ä¿å­˜JSONæ ¼å¼æŠ¥å‘Š"""
        report_dir = Path(__file__).parent.parent / "reports" / "perf"
        report_dir.mkdir(parents=True, exist_ok=True)

        report_file = report_dir / "adaptive_rate_limit_benchmark.json"

        report_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "results": [asdict(r) for r in self.results],
            "summary": self._calculate_summary()
        }

        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)

        print(f"\nğŸ“ Report saved to: {report_file}")

    def _calculate_summary(self) -> Dict[str, Any]:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        overhead_values = [r.overhead_percentage for r in self.results if r.enabled]

        return {
            "total_scenarios": len(self.results),
            "adaptive_scenarios": sum(1 for r in self.results if r.enabled),
            "avg_overhead_p95": statistics.mean(overhead_values) if overhead_values else 0,
            "max_overhead_p95": max(overhead_values) if overhead_values else 0,
            "min_overhead_p95": min(overhead_values) if overhead_values else 0,
            "meets_requirement": max(overhead_values) <= 5.0 if overhead_values else True
        }


def run_integration_test():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("\n" + "=" * 60)
    print("ğŸ§ª Integration Test with Decorator")
    print("=" * 60)

    # ä½¿ç”¨è£…é¥°å™¨çš„å‡½æ•°
    @adaptive_rate_limit(service="test", endpoint="process", base_rate=100.0)
    def process_with_adaptive(data):
        time.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†
        if np.random.random() < 0.05:  # 5%é”™è¯¯ç‡
            raise Exception("Processing error")
        return f"Processed: {data}"

    def process_without_adaptive(data):
        time.sleep(0.01)
        if np.random.random() < 0.05:
            raise Exception("Processing error")
        return f"Processed: {data}"

    # æµ‹è¯•æœ‰æ— è‡ªé€‚åº”çš„å·®å¼‚
    print("\nTesting with adaptive rate limiting...")
    adaptive_times = []
    for i in range(100):
        start = time.time()
        try:
            process_with_adaptive(f"data_{i}")
        except:
            pass
        adaptive_times.append(time.time() - start)

    print("\nTesting without adaptive rate limiting...")
    normal_times = []
    for i in range(100):
        start = time.time()
        try:
            process_without_adaptive(f"data_{i}")
        except:
            pass
        normal_times.append(time.time() - start)

    # åˆ†æç»“æœ
    adaptive_p95 = np.percentile(adaptive_times, 95) * 1000
    normal_p95 = np.percentile(normal_times, 95) * 1000
    overhead = ((adaptive_p95 - normal_p95) / normal_p95) * 100

    print(f"\nğŸ“Š Integration Test Results:")
    print(f"  Normal P95: {normal_p95:.2f}ms")
    print(f"  Adaptive P95: {adaptive_p95:.2f}ms")
    print(f"  Overhead: {overhead:+.2f}%")

    if overhead <= 5.0:
        print("  âœ… Integration test PASSED")
    else:
        print("  âŒ Integration test FAILED")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Adaptive rate limiter performance benchmark"
    )
    parser.add_argument(
        "--quick",
        action="store_true",
        help="Run quick benchmark (fewer requests)"
    )
    parser.add_argument(
        "--integration",
        action="store_true",
        help="Run integration test only"
    )

    args = parser.parse_args()

    if args.integration:
        run_integration_test()
    else:
        benchmark = AdaptiveRateLimiterBenchmark()

        if args.quick:
            # å¿«é€Ÿæµ‹è¯•
            config = AdaptiveConfig(base_rate=100.0)
            benchmark.run_scenario("Quick Test Baseline", 100, 5, 0.01, False, None)
            benchmark.run_scenario("Quick Test Adaptive", 100, 5, 0.01, True, config)
            benchmark.generate_report()
        else:
            # å®Œæ•´åŸºå‡†æµ‹è¯•
            benchmark.run_all_scenarios()

    return 0


if __name__ == "__main__":
    sys.exit(main())
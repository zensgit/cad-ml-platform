#!/usr/bin/env python3
"""
Rate Limit Analyzer
åˆ†ææµé‡æ¨¡å¼ï¼Œè¯†åˆ«å¼‚å¸¸è¡Œä¸ºï¼Œç”Ÿæˆç”¨æˆ·ç”»åƒ
"""

import json
import logging
import statistics
import numpy as np
from collections import defaultdict, Counter
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Tuple, Set, Any
from pathlib import Path
import math
import hashlib

# å°è¯•å¯¼å…¥ç›¸å…³æ¨¡å—
try:
    from .adaptive_rate_limiter import Request, SystemMetrics, TrafficPattern
except ImportError:
    from adaptive_rate_limiter import Request, SystemMetrics, TrafficPattern  # type: ignore

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PatternType(Enum):
    """æµé‡æ¨¡å¼ç±»å‹"""
    NORMAL = "normal"  # æ­£å¸¸ä¸šåŠ¡
    SPIKE = "spike"  # æµé‡å°–å³°
    GRADUAL_INCREASE = "gradual_increase"  # æ¸è¿›å¢é•¿
    PERIODIC = "periodic"  # å‘¨æœŸæ€§æ¨¡å¼
    RANDOM = "random"  # éšæœºæ¨¡å¼
    CRAWLER = "crawler"  # çˆ¬è™«
    DDOS = "ddos"  # DDoSæ”»å‡»
    BRUTE_FORCE = "brute_force"  # æš´åŠ›ç ´è§£


class AnomalyType(Enum):
    """å¼‚å¸¸ç±»å‹"""
    RATE_ANOMALY = "rate_anomaly"  # é€Ÿç‡å¼‚å¸¸
    PATTERN_ANOMALY = "pattern_anomaly"  # æ¨¡å¼å¼‚å¸¸
    VOLUME_ANOMALY = "volume_anomaly"  # æµé‡å¼‚å¸¸
    BEHAVIOR_ANOMALY = "behavior_anomaly"  # è¡Œä¸ºå¼‚å¸¸
    SOURCE_ANOMALY = "source_anomaly"  # æ¥æºå¼‚å¸¸


@dataclass
class TimeWindow:
    """æ—¶é—´çª—å£"""
    start: float
    end: float
    duration: float = field(init=False)

    def __post_init__(self):
        self.duration = self.end - self.start

    def contains(self, timestamp: float) -> bool:
        return self.start <= timestamp <= self.end


@dataclass
class TrafficMetrics:
    """æµé‡æŒ‡æ ‡"""
    request_count: int = 0
    unique_users: int = 0
    unique_ips: int = 0
    avg_request_rate: float = 0.0
    peak_request_rate: float = 0.0
    error_count: int = 0
    error_rate: float = 0.0
    avg_latency: float = 0.0
    p95_latency: float = 0.0
    bandwidth_usage: float = 0.0
    api_distribution: Dict[str, int] = field(default_factory=dict)
    method_distribution: Dict[str, int] = field(default_factory=dict)
    status_distribution: Dict[int, int] = field(default_factory=dict)


@dataclass
class TrafficAnalysis:
    """æµé‡åˆ†æç»“æœ"""
    time_window: TimeWindow
    metrics: TrafficMetrics
    pattern_type: PatternType
    anomalies: List['Anomaly']
    confidence: float  # åˆ†æç½®ä¿¡åº¦ 0-1
    recommendations: List[str]


@dataclass
class Anomaly:
    """å¼‚å¸¸æ£€æµ‹ç»“æœ"""
    type: AnomalyType
    severity: str  # low, medium, high, critical
    timestamp: float
    description: str
    affected_users: List[str] = field(default_factory=list)
    affected_ips: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)


@dataclass
class UserProfile:
    """ç”¨æˆ·è¡Œä¸ºç”»åƒ"""
    user_id: str
    total_requests: int = 0
    avg_request_rate: float = 0.0  # å¹³å‡è¯·æ±‚é€Ÿç‡
    peak_request_rate: float = 0.0  # å³°å€¼è¯·æ±‚é€Ÿç‡
    error_rate: float = 0.0  # é”™è¯¯ç‡
    avg_latency: float = 0.0  # å¹³å‡å»¶è¿Ÿ
    api_usage: Dict[str, int] = field(default_factory=dict)  # APIä½¿ç”¨åˆ†å¸ƒ
    access_pattern: str = "normal"  # è®¿é—®æ¨¡å¼
    reputation_score: float = 100.0  # ä¿¡èª‰åˆ†æ•° 0-100
    is_suspicious: bool = False
    last_activity: float = 0.0
    created_at: float = field(default_factory=lambda: datetime.now().timestamp())


class PatternDetector:
    """æµé‡æ¨¡å¼æ£€æµ‹å™¨"""

    def __init__(self):
        self.patterns = {
            PatternType.NORMAL: self._detect_normal,
            PatternType.SPIKE: self._detect_spike,
            PatternType.GRADUAL_INCREASE: self._detect_gradual_increase,
            PatternType.PERIODIC: self._detect_periodic,
            PatternType.CRAWLER: self._detect_crawler,
            PatternType.DDOS: self._detect_ddos,
            PatternType.BRUTE_FORCE: self._detect_brute_force
        }

    def detect(self, requests: List[Request], metrics: TrafficMetrics) -> PatternType:
        """æ£€æµ‹æµé‡æ¨¡å¼"""
        if not requests:
            return PatternType.NORMAL

        # è®¡ç®—æ—¶é—´åºåˆ—
        timestamps = [r.timestamp for r in requests]
        timestamps.sort()

        # æ£€æµ‹å„ç§æ¨¡å¼
        scores = {}
        for pattern_type, detector in self.patterns.items():
            score = detector(requests, timestamps, metrics)
            scores[pattern_type] = score

        # è¿”å›å¾—åˆ†æœ€é«˜çš„æ¨¡å¼
        return max(scores, key=scores.get)

    def _detect_normal(self, requests: List[Request], timestamps: List[float],
                      metrics: TrafficMetrics) -> float:
        """æ£€æµ‹æ­£å¸¸æ¨¡å¼"""
        if not timestamps:
            return 1.0

        # è®¡ç®—è¯·æ±‚é—´éš”
        intervals = [timestamps[i] - timestamps[i-1]
                    for i in range(1, len(timestamps))]

        if not intervals:
            return 1.0

        # æ­£å¸¸æµé‡çš„ç‰¹å¾ï¼šé—´éš”ç›¸å¯¹å‡åŒ€
        mean_interval = statistics.mean(intervals)
        if mean_interval == 0:
            return 0.0

        cv = statistics.stdev(intervals) / mean_interval if len(intervals) > 1 else 0
        return max(0, 1 - cv)

    def _detect_spike(self, requests: List[Request], timestamps: List[float],
                     metrics: TrafficMetrics) -> float:
        """æ£€æµ‹æµé‡å°–å³°"""
        if len(timestamps) < 10:
            return 0.0

        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window_size = min(10, len(timestamps) // 3)
        rates = []

        for i in range(window_size, len(timestamps)):
            duration = timestamps[i] - timestamps[i - window_size]
            if duration > 0:
                rate = window_size / duration
                rates.append(rate)

        if not rates:
            return 0.0

        # å°–å³°ç‰¹å¾ï¼šå­˜åœ¨æ˜¾è‘—é«˜äºå¹³å‡çš„ç‚¹
        mean_rate = statistics.mean(rates)
        max_rate = max(rates)

        if mean_rate == 0:
            return 0.0

        spike_factor = max_rate / mean_rate
        return min(1.0, (spike_factor - 1) / 5)  # 5å€ä»¥ä¸Šç®—å®Œå…¨å°–å³°

    def _detect_gradual_increase(self, requests: List[Request], timestamps: List[float],
                                 metrics: TrafficMetrics) -> float:
        """æ£€æµ‹æ¸è¿›å¢é•¿"""
        if len(timestamps) < 20:
            return 0.0

        # å°†æ—¶é—´åºåˆ—åˆ†æˆè‹¥å¹²æ®µï¼Œè®¡ç®—æ¯æ®µçš„é€Ÿç‡
        segments = 5
        segment_size = len(timestamps) // segments
        segment_rates = []

        for i in range(segments):
            start_idx = i * segment_size
            end_idx = min((i + 1) * segment_size, len(timestamps))

            if end_idx - start_idx > 1:
                duration = timestamps[end_idx - 1] - timestamps[start_idx]
                if duration > 0:
                    rate = (end_idx - start_idx) / duration
                    segment_rates.append(rate)

        if len(segment_rates) < 2:
            return 0.0

        # æ¸è¿›å¢é•¿ç‰¹å¾ï¼šé€Ÿç‡å•è°ƒé€’å¢
        increases = sum(1 for i in range(1, len(segment_rates))
                       if segment_rates[i] > segment_rates[i-1])
        return increases / (len(segment_rates) - 1)

    def _detect_periodic(self, requests: List[Request], timestamps: List[float],
                        metrics: TrafficMetrics) -> float:
        """æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼"""
        if len(timestamps) < 50:
            return 0.0

        # è®¡ç®—è‡ªç›¸å…³æ€§
        intervals = [timestamps[i] - timestamps[i-1]
                    for i in range(1, len(timestamps))]

        if len(intervals) < 20:
            return 0.0

        # ç®€åŒ–çš„å‘¨æœŸæ€§æ£€æµ‹ï¼šæ£€æŸ¥é—´éš”çš„è§„å¾‹æ€§
        mean_interval = statistics.mean(intervals)
        if mean_interval == 0:
            return 0.0

        # è®¡ç®—é—´éš”çš„åˆ†å¸ƒ
        interval_counts = Counter([round(i / mean_interval) for i in intervals])

        # å‘¨æœŸæ€§ç‰¹å¾ï¼šé—´éš”åˆ†å¸ƒé›†ä¸­
        total_counts = sum(interval_counts.values())
        max_count = max(interval_counts.values())

        return max_count / total_counts if total_counts > 0 else 0.0

    def _detect_crawler(self, requests: List[Request], timestamps: List[float],
                       metrics: TrafficMetrics) -> float:
        """æ£€æµ‹çˆ¬è™«è¡Œä¸º"""
        if not requests:
            return 0.0

        # çˆ¬è™«ç‰¹å¾
        crawler_score = 0.0

        # ç‰¹å¾1ï¼šè®¿é—®å¤§é‡ä¸åŒçš„APIè·¯å¾„
        unique_apis = len(set(r.api_path for r in requests))
        if unique_apis > len(requests) * 0.8:
            crawler_score += 0.3

        # ç‰¹å¾2ï¼šè¯·æ±‚é—´éš”éå¸¸å‡åŒ€
        if len(timestamps) > 10:
            intervals = [timestamps[i] - timestamps[i-1]
                        for i in range(1, len(timestamps))]
            if intervals:
                mean_interval = statistics.mean(intervals)
                if mean_interval > 0 and len(intervals) > 1:
                    cv = statistics.stdev(intervals) / mean_interval
                    if cv < 0.1:  # é—´éš”éå¸¸å‡åŒ€
                        crawler_score += 0.4

        # ç‰¹å¾3ï¼šå•ä¸ªIPæˆ–ç”¨æˆ·å‘èµ·å¤§é‡è¯·æ±‚
        user_counts = Counter(r.user_id for r in requests)
        if user_counts:
            max_user_requests = max(user_counts.values())
            if max_user_requests > len(requests) * 0.5:
                crawler_score += 0.3

        return min(1.0, crawler_score)

    def _detect_ddos(self, requests: List[Request], timestamps: List[float],
                    metrics: TrafficMetrics) -> float:
        """æ£€æµ‹DDoSæ”»å‡»"""
        if not requests or not timestamps:
            return 0.0

        ddos_score = 0.0

        # ç‰¹å¾1ï¼šå¤§é‡ä¸åŒIPçš„è¯·æ±‚
        unique_ips = len(set(r.source_ip for r in requests))
        if unique_ips > len(requests) * 0.7:
            ddos_score += 0.3

        # ç‰¹å¾2ï¼šè¯·æ±‚é€Ÿç‡å¼‚å¸¸é«˜
        duration = timestamps[-1] - timestamps[0] if len(timestamps) > 1 else 1
        if duration > 0:
            rate = len(requests) / duration
            if rate > 1000:  # å‡è®¾æ­£å¸¸é€Ÿç‡ä¸Šé™æ˜¯1000 req/s
                ddos_score += 0.4

        # ç‰¹å¾3ï¼šé”™è¯¯ç‡é«˜
        if metrics.error_rate > 0.3:
            ddos_score += 0.3

        return min(1.0, ddos_score)

    def _detect_brute_force(self, requests: List[Request], timestamps: List[float],
                           metrics: TrafficMetrics) -> float:
        """æ£€æµ‹æš´åŠ›ç ´è§£"""
        if not requests:
            return 0.0

        brute_force_score = 0.0

        # ç‰¹å¾1ï¼šé’ˆå¯¹ç‰¹å®šAPIçš„é‡å¤è¯·æ±‚
        api_counts = Counter(r.api_path for r in requests)
        if api_counts:
            # æ£€æŸ¥æ˜¯å¦æœ‰è®¤è¯ç›¸å…³çš„APIè¢«é¢‘ç¹è°ƒç”¨
            auth_apis = [api for api in api_counts
                        if 'login' in api.lower() or 'auth' in api.lower()]
            if auth_apis:
                auth_requests = sum(api_counts[api] for api in auth_apis)
                if auth_requests > len(requests) * 0.5:
                    brute_force_score += 0.5

        # ç‰¹å¾2ï¼šé«˜é”™è¯¯ç‡
        if metrics.error_rate > 0.5:
            brute_force_score += 0.3

        # ç‰¹å¾3ï¼šæ¥è‡ªå°‘é‡IPçš„å¤§é‡è¯·æ±‚
        ip_counts = Counter(r.source_ip for r in requests)
        if ip_counts and len(ip_counts) <= 5:
            concentrated_requests = sum(ip_counts.values())
            if concentrated_requests > len(requests) * 0.8:
                brute_force_score += 0.2

        return min(1.0, brute_force_score)


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, sensitivity: float = 2.0):
        self.sensitivity = sensitivity  # Z-scoreé˜ˆå€¼

    def detect(self, current_metrics: TrafficMetrics,
              historical_metrics: List[TrafficMetrics]) -> List[Anomaly]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []

        if not historical_metrics:
            return anomalies

        # æ£€æµ‹é€Ÿç‡å¼‚å¸¸
        rate_anomaly = self._detect_rate_anomaly(current_metrics, historical_metrics)
        if rate_anomaly:
            anomalies.append(rate_anomaly)

        # æ£€æµ‹é”™è¯¯ç‡å¼‚å¸¸
        error_anomaly = self._detect_error_anomaly(current_metrics, historical_metrics)
        if error_anomaly:
            anomalies.append(error_anomaly)

        # æ£€æµ‹å»¶è¿Ÿå¼‚å¸¸
        latency_anomaly = self._detect_latency_anomaly(current_metrics, historical_metrics)
        if latency_anomaly:
            anomalies.append(latency_anomaly)

        # æ£€æµ‹æµé‡åˆ†å¸ƒå¼‚å¸¸
        distribution_anomaly = self._detect_distribution_anomaly(current_metrics, historical_metrics)
        if distribution_anomaly:
            anomalies.append(distribution_anomaly)

        return anomalies

    def _detect_rate_anomaly(self, current: TrafficMetrics,
                            historical: List[TrafficMetrics]) -> Optional[Anomaly]:
        """æ£€æµ‹é€Ÿç‡å¼‚å¸¸"""
        historical_rates = [m.avg_request_rate for m in historical]
        if not historical_rates:
            return None

        mean_rate = statistics.mean(historical_rates)
        std_rate = statistics.stdev(historical_rates) if len(historical_rates) > 1 else 0

        if std_rate == 0:
            return None

        z_score = (current.avg_request_rate - mean_rate) / std_rate

        if abs(z_score) > self.sensitivity:
            severity = self._calculate_severity(abs(z_score))
            return Anomaly(
                type=AnomalyType.RATE_ANOMALY,
                severity=severity,
                timestamp=datetime.now().timestamp(),
                description=f"Request rate anomaly: {current.avg_request_rate:.2f} req/s "
                          f"(normal: {mean_rate:.2f} Â± {std_rate:.2f})",
                metrics={
                    'current_rate': current.avg_request_rate,
                    'mean_rate': mean_rate,
                    'std_rate': std_rate,
                    'z_score': z_score
                }
            )

        return None

    def _detect_error_anomaly(self, current: TrafficMetrics,
                             historical: List[TrafficMetrics]) -> Optional[Anomaly]:
        """æ£€æµ‹é”™è¯¯ç‡å¼‚å¸¸"""
        historical_errors = [m.error_rate for m in historical]
        if not historical_errors:
            return None

        mean_error = statistics.mean(historical_errors)
        std_error = statistics.stdev(historical_errors) if len(historical_errors) > 1 else 0

        # é”™è¯¯ç‡å¢åŠ æ›´æ•æ„Ÿ
        if current.error_rate > mean_error + std_error * self.sensitivity:
            severity = self._calculate_severity(
                (current.error_rate - mean_error) / max(std_error, 0.001)
            )
            return Anomaly(
                type=AnomalyType.RATE_ANOMALY,
                severity=severity,
                timestamp=datetime.now().timestamp(),
                description=f"Error rate anomaly: {current.error_rate:.2%} "
                          f"(normal: {mean_error:.2%})",
                metrics={
                    'current_error_rate': current.error_rate,
                    'mean_error_rate': mean_error,
                    'std_error_rate': std_error
                }
            )

        return None

    def _detect_latency_anomaly(self, current: TrafficMetrics,
                               historical: List[TrafficMetrics]) -> Optional[Anomaly]:
        """æ£€æµ‹å»¶è¿Ÿå¼‚å¸¸"""
        historical_latencies = [m.p95_latency for m in historical]
        if not historical_latencies:
            return None

        mean_latency = statistics.mean(historical_latencies)
        std_latency = statistics.stdev(historical_latencies) if len(historical_latencies) > 1 else 0

        if std_latency == 0:
            return None

        z_score = (current.p95_latency - mean_latency) / std_latency

        if z_score > self.sensitivity:  # åªå…³æ³¨å»¶è¿Ÿå¢åŠ 
            severity = self._calculate_severity(z_score)
            return Anomaly(
                type=AnomalyType.PATTERN_ANOMALY,
                severity=severity,
                timestamp=datetime.now().timestamp(),
                description=f"Latency anomaly: P95={current.p95_latency:.2f}ms "
                          f"(normal: {mean_latency:.2f}ms)",
                metrics={
                    'current_p95': current.p95_latency,
                    'mean_p95': mean_latency,
                    'z_score': z_score
                }
            )

        return None

    def _detect_distribution_anomaly(self, current: TrafficMetrics,
                                    historical: List[TrafficMetrics]) -> Optional[Anomaly]:
        """æ£€æµ‹æµé‡åˆ†å¸ƒå¼‚å¸¸"""
        # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥APIåˆ†å¸ƒçš„å˜åŒ–
        if not current.api_distribution or not historical:
            return None

        # è®¡ç®—å½“å‰åˆ†å¸ƒçš„ç†µ
        total_requests = sum(current.api_distribution.values())
        if total_requests == 0:
            return None

        current_entropy = -sum(
            (count / total_requests) * math.log(count / total_requests)
            for count in current.api_distribution.values() if count > 0
        )

        # è®¡ç®—å†å²å¹³å‡ç†µ
        historical_entropies = []
        for h in historical:
            if h.api_distribution:
                h_total = sum(h.api_distribution.values())
                if h_total > 0:
                    h_entropy = -sum(
                        (count / h_total) * math.log(count / h_total)
                        for count in h.api_distribution.values() if count > 0
                    )
                    historical_entropies.append(h_entropy)

        if not historical_entropies:
            return None

        mean_entropy = statistics.mean(historical_entropies)

        # ç†µçš„æ˜¾è‘—å˜åŒ–å¯èƒ½è¡¨ç¤ºå¼‚å¸¸
        entropy_change = abs(current_entropy - mean_entropy)
        if entropy_change > mean_entropy * 0.5:  # 50%çš„å˜åŒ–
            return Anomaly(
                type=AnomalyType.PATTERN_ANOMALY,
                severity="medium",
                timestamp=datetime.now().timestamp(),
                description=f"Traffic distribution anomaly detected",
                metrics={
                    'current_entropy': current_entropy,
                    'mean_entropy': mean_entropy,
                    'change_ratio': entropy_change / mean_entropy
                }
            )

        return None

    def _calculate_severity(self, z_score: float) -> str:
        """æ ¹æ®Z-scoreè®¡ç®—ä¸¥é‡ç¨‹åº¦"""
        abs_z = abs(z_score)
        if abs_z < 2:
            return "low"
        elif abs_z < 3:
            return "medium"
        elif abs_z < 4:
            return "high"
        else:
            return "critical"


class RateLimitAnalyzer:
    """é™æµåˆ†æå™¨"""

    def __init__(self):
        self.pattern_detector = PatternDetector()
        self.anomaly_detector = AnomalyDetector()
        self.user_profiles: Dict[str, UserProfile] = {}
        self.historical_metrics: List[TrafficMetrics] = []
        self.max_history = 100

    def analyze_traffic(self, requests: List[Request],
                       time_window: TimeWindow) -> TrafficAnalysis:
        """
        åˆ†ææ—¶é—´çª—å£å†…çš„æµé‡

        Args:
            requests: è¯·æ±‚åˆ—è¡¨
            time_window: æ—¶é—´çª—å£

        Returns:
            æµé‡åˆ†æç»“æœ
        """
        # è®¡ç®—æµé‡æŒ‡æ ‡
        metrics = self._calculate_metrics(requests, time_window)

        # æ£€æµ‹æµé‡æ¨¡å¼
        pattern = self.pattern_detector.detect(requests, metrics)

        # æ£€æµ‹å¼‚å¸¸
        anomalies = self.anomaly_detector.detect(metrics, self.historical_metrics)

        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(requests, metrics, pattern)

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(pattern, anomalies, metrics)

        # æ›´æ–°å†å²
        self.historical_metrics.append(metrics)
        if len(self.historical_metrics) > self.max_history:
            self.historical_metrics.pop(0)

        return TrafficAnalysis(
            time_window=time_window,
            metrics=metrics,
            pattern_type=pattern,
            anomalies=anomalies,
            confidence=confidence,
            recommendations=recommendations
        )

    def _calculate_metrics(self, requests: List[Request],
                         time_window: TimeWindow) -> TrafficMetrics:
        """è®¡ç®—æµé‡æŒ‡æ ‡"""
        metrics = TrafficMetrics()

        if not requests:
            return metrics

        metrics.request_count = len(requests)
        metrics.unique_users = len(set(r.user_id for r in requests))
        metrics.unique_ips = len(set(r.source_ip for r in requests))

        # è®¡ç®—é€Ÿç‡
        if time_window.duration > 0:
            metrics.avg_request_rate = len(requests) / time_window.duration

            # è®¡ç®—å³°å€¼é€Ÿç‡ï¼ˆç®€åŒ–ï¼šä½¿ç”¨1ç§’çª—å£ï¼‰
            timestamps = [r.timestamp for r in requests]
            if timestamps:
                timestamps.sort()
                max_count_per_second = 0
                for t in range(int(timestamps[0]), int(timestamps[-1]) + 1):
                    count = sum(1 for ts in timestamps if t <= ts < t + 1)
                    max_count_per_second = max(max_count_per_second, count)
                metrics.peak_request_rate = max_count_per_second

        # APIåˆ†å¸ƒ
        for r in requests:
            metrics.api_distribution[r.api_path] = \
                metrics.api_distribution.get(r.api_path, 0) + 1
            metrics.method_distribution[r.method] = \
                metrics.method_distribution.get(r.method, 0) + 1

        return metrics

    def _calculate_confidence(self, requests: List[Request],
                            metrics: TrafficMetrics,
                            pattern: PatternType) -> float:
        """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
        confidence = 1.0

        # æ ·æœ¬é‡å½±å“
        if len(requests) < 10:
            confidence *= 0.5
        elif len(requests) < 100:
            confidence *= 0.8
        elif len(requests) < 1000:
            confidence *= 0.9

        # å†å²æ•°æ®å½±å“
        if len(self.historical_metrics) < 10:
            confidence *= 0.8

        # æ¨¡å¼æ¸…æ™°åº¦å½±å“
        if pattern == PatternType.RANDOM:
            confidence *= 0.7

        return min(1.0, confidence)

    def _generate_recommendations(self, pattern: PatternType,
                                 anomalies: List[Anomaly],
                                 metrics: TrafficMetrics) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæ¨¡å¼çš„å»ºè®®
        pattern_recommendations = {
            PatternType.SPIKE: "å¯ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•å¤„ç†çªå‘æµé‡",
            PatternType.SUSTAINED: "ä½¿ç”¨æ¼æ¡¶ç®—æ³•å¹³æ»‘æµé‡",
            PatternType.CRAWLER: "å¯¹ç–‘ä¼¼çˆ¬è™«IPè¿›è¡Œé¢å¤–é™æµ",
            PatternType.DDOS: "å¯ç”¨DDoSé˜²æŠ¤æ¨¡å¼ï¼Œè€ƒè™‘ä½¿ç”¨CDN",
            PatternType.BRUTE_FORCE: "å¯¹è®¤è¯APIå¢åŠ é¢å¤–ä¿æŠ¤ï¼Œå¦‚éªŒè¯ç "
        }

        if pattern in pattern_recommendations:
            recommendations.append(pattern_recommendations[pattern])

        # åŸºäºå¼‚å¸¸çš„å»ºè®®
        for anomaly in anomalies:
            if anomaly.type == AnomalyType.RATE_ANOMALY:
                recommendations.append("åŠ¨æ€è°ƒæ•´é™æµé˜ˆå€¼ä»¥é€‚åº”æµé‡å˜åŒ–")
            elif anomaly.type == AnomalyType.PATTERN_ANOMALY:
                if 'latency' in anomaly.description.lower():
                    recommendations.append("è€ƒè™‘é™ä½é™æµé˜ˆå€¼ä»¥å‡è½»ç³»ç»Ÿå‹åŠ›")

        # åŸºäºæŒ‡æ ‡çš„å»ºè®®
        if metrics.error_rate > 0.05:
            recommendations.append("é”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€")

        if metrics.unique_ips > metrics.unique_users * 10:
            recommendations.append("æ£€æµ‹åˆ°å¯èƒ½çš„IPä¼ªé€ ï¼Œå»ºè®®åŠ å¼ºèº«ä»½éªŒè¯")

        return recommendations

    def classify_traffic_pattern(self, metrics: TrafficMetrics) -> PatternType:
        """
        åˆ†ç±»æµé‡æ¨¡å¼

        Args:
            metrics: æµé‡æŒ‡æ ‡

        Returns:
            æµé‡æ¨¡å¼ç±»å‹
        """
        # ç®€åŒ–çš„åˆ†ç±»é€»è¾‘
        if metrics.error_rate > 0.3:
            if metrics.unique_ips > metrics.unique_users * 5:
                return PatternType.DDOS
            else:
                return PatternType.BRUTE_FORCE

        if metrics.peak_request_rate > metrics.avg_request_rate * 3:
            return PatternType.SPIKE

        return PatternType.NORMAL

    def generate_user_profile(self, user_id: str, requests: List[Request]) -> UserProfile:
        """
        ç”Ÿæˆæˆ–æ›´æ–°ç”¨æˆ·è¡Œä¸ºç”»åƒ

        Args:
            user_id: ç”¨æˆ·ID
            requests: è¯¥ç”¨æˆ·çš„è¯·æ±‚åˆ—è¡¨

        Returns:
            ç”¨æˆ·ç”»åƒ
        """
        # è·å–æˆ–åˆ›å»ºç”¨æˆ·ç”»åƒ
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserProfile(user_id=user_id)

        profile = self.user_profiles[user_id]

        # æ›´æ–°åŸºç¡€ç»Ÿè®¡
        profile.total_requests += len(requests)

        if requests:
            # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
            profile.last_activity = max(r.timestamp for r in requests)

            # è®¡ç®—è¯·æ±‚é€Ÿç‡
            timestamps = [r.timestamp for r in requests]
            if len(timestamps) > 1:
                duration = max(timestamps) - min(timestamps)
                if duration > 0:
                    current_rate = len(requests) / duration
                    # æŒ‡æ•°ç§»åŠ¨å¹³å‡
                    alpha = 0.3
                    profile.avg_request_rate = (1 - alpha) * profile.avg_request_rate + alpha * current_rate
                    profile.peak_request_rate = max(profile.peak_request_rate, current_rate)

            # æ›´æ–°APIä½¿ç”¨åˆ†å¸ƒ
            for r in requests:
                profile.api_usage[r.api_path] = profile.api_usage.get(r.api_path, 0) + 1

        # è¯„ä¼°ç”¨æˆ·è¡Œä¸º
        profile = self._evaluate_user_behavior(profile, requests)

        return profile

    def _evaluate_user_behavior(self, profile: UserProfile,
                               recent_requests: List[Request]) -> UserProfile:
        """è¯„ä¼°ç”¨æˆ·è¡Œä¸º"""
        # æ£€æŸ¥æ˜¯å¦å¯ç–‘
        suspicious_indicators = 0

        # æŒ‡æ ‡1ï¼šè¯·æ±‚é€Ÿç‡å¼‚å¸¸é«˜
        if profile.avg_request_rate > 100:  # 100 req/s
            suspicious_indicators += 1

        # æŒ‡æ ‡2ï¼šé”™è¯¯ç‡é«˜
        if profile.error_rate > 0.3:
            suspicious_indicators += 1

        # æŒ‡æ ‡3ï¼šè®¿é—®æ¨¡å¼å¼‚å¸¸
        if len(profile.api_usage) > 100:  # è®¿é—®å¤ªå¤šä¸åŒçš„API
            suspicious_indicators += 1

        profile.is_suspicious = suspicious_indicators >= 2

        # è®¡ç®—ä¿¡èª‰åˆ†æ•°
        base_score = 100.0
        base_score -= profile.error_rate * 50  # é”™è¯¯ç‡æ‰£åˆ†
        base_score -= min(20, profile.avg_request_rate / 10)  # é«˜é¢‘ç‡æ‰£åˆ†

        if profile.is_suspicious:
            base_score -= 30

        profile.reputation_score = max(0, min(100, base_score))

        # ç¡®å®šè®¿é—®æ¨¡å¼
        if profile.is_suspicious:
            if profile.error_rate > 0.5:
                profile.access_pattern = "brute_force"
            elif len(profile.api_usage) > 100:
                profile.access_pattern = "crawler"
            else:
                profile.access_pattern = "suspicious"
        elif profile.avg_request_rate > 50:
            profile.access_pattern = "heavy"
        else:
            profile.access_pattern = "normal"

        return profile

    def export_analysis(self, analysis: TrafficAnalysis, output_file: str):
        """å¯¼å‡ºåˆ†æç»“æœ"""
        data = {
            'timestamp': datetime.now().isoformat(),
            'time_window': {
                'start': analysis.time_window.start,
                'end': analysis.time_window.end,
                'duration': analysis.time_window.duration
            },
            'metrics': asdict(analysis.metrics),
            'pattern': analysis.pattern_type.value,
            'anomalies': [asdict(a) for a in analysis.anomalies],
            'confidence': analysis.confidence,
            'recommendations': analysis.recommendations
        }

        with open(output_file, 'w') as f:
            json.dump(data, f, indent=2)

    def get_user_profiles_summary(self) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·ç”»åƒæ±‡æ€»"""
        total_users = len(self.user_profiles)
        suspicious_users = sum(1 for p in self.user_profiles.values() if p.is_suspicious)

        pattern_distribution = Counter(
            p.access_pattern for p in self.user_profiles.values()
        )

        avg_reputation = statistics.mean(
            [p.reputation_score for p in self.user_profiles.values()]
        ) if self.user_profiles else 0

        return {
            'total_users': total_users,
            'suspicious_users': suspicious_users,
            'suspicious_rate': suspicious_users / total_users if total_users > 0 else 0,
            'pattern_distribution': dict(pattern_distribution),
            'avg_reputation_score': avg_reputation,
            'top_requesters': sorted(
                self.user_profiles.values(),
                key=lambda p: p.total_requests,
                reverse=True
            )[:10]
        }


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse
    import random

    parser = argparse.ArgumentParser(description='Rate Limit Analyzer')
    parser.add_argument('--analyze', help='Analyze request log file')
    parser.add_argument('--output', help='Output analysis results')
    parser.add_argument('--simulate', action='store_true', help='Simulate and analyze traffic')

    args = parser.parse_args()

    analyzer = RateLimitAnalyzer()

    if args.simulate:
        print("ğŸ” Simulating and analyzing traffic...")

        # ç”Ÿæˆæ¨¡æ‹Ÿè¯·æ±‚
        requests = []
        start_time = time.time()

        # æ¨¡æ‹Ÿä¸åŒçš„æµé‡æ¨¡å¼
        for i in range(1000):
            # åˆ›å»ºè¯·æ±‚
            request = Request(
                id=f"req_{i}",
                user_id=f"user_{random.randint(1, 20)}",
                api_path=random.choice([
                    "/api/v1/users",
                    "/api/v1/products",
                    "/api/v1/orders",
                    "/api/v1/login",
                    "/api/v1/search"
                ]),
                method=random.choice(["GET", "POST", "PUT", "DELETE"]),
                source_ip=f"192.168.1.{random.randint(1, 50)}",
                timestamp=start_time + i * random.uniform(0.01, 0.5)
            )
            requests.append(request)

        # åˆ†ææµé‡
        end_time = start_time + 60  # 60ç§’çª—å£
        time_window = TimeWindow(start=start_time, end=end_time)
        analysis = analyzer.analyze_traffic(requests, time_window)

        print(f"\nğŸ“Š Analysis Results:")
        print(f"   Pattern: {analysis.pattern_type.value}")
        print(f"   Confidence: {analysis.confidence:.2%}")
        print(f"   Request Count: {analysis.metrics.request_count}")
        print(f"   Unique Users: {analysis.metrics.unique_users}")
        print(f"   Avg Rate: {analysis.metrics.avg_request_rate:.2f} req/s")

        if analysis.anomalies:
            print(f"\nâš ï¸ Anomalies Detected:")
            for anomaly in analysis.anomalies:
                print(f"   - {anomaly.type.value}: {anomaly.description}")

        if analysis.recommendations:
            print(f"\nğŸ’¡ Recommendations:")
            for rec in analysis.recommendations:
                print(f"   - {rec}")

        # ç”¨æˆ·ç”»åƒæ±‡æ€»
        for req in requests:
            user_requests = [r for r in requests if r.user_id == req.user_id]
            analyzer.generate_user_profile(req.user_id, user_requests)

        summary = analyzer.get_user_profiles_summary()
        print(f"\nğŸ‘¥ User Profiles:")
        print(f"   Total Users: {summary['total_users']}")
        print(f"   Suspicious Users: {summary['suspicious_users']}")
        print(f"   Avg Reputation: {summary['avg_reputation_score']:.1f}")

        # å¯¼å‡ºç»“æœ
        if args.output:
            analyzer.export_analysis(analysis, args.output)
            print(f"\nâœ… Analysis exported to {args.output}")


if __name__ == "__main__":
    main()
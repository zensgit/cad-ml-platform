#!/usr/bin/env python3
"""
åˆ†æV16è¶…çº§é›†æˆçš„æœ€ç»ˆé”™è¯¯æ ·æœ¬
ç¡®å®šæ˜¯å¦è¿˜æœ‰æå‡ç©ºé—´
"""

import json
import logging
import sys
from pathlib import Path
from collections import Counter

import numpy as np
import torch
import torch.nn as nn

sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

REAL_DATA_DIR = Path("data/training_v7")
MODEL_DIR = Path("models")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")

CATEGORIES = ["è½´ç±»", "ä¼ åŠ¨ä»¶", "å£³ä½“ç±»", "è¿æ¥ä»¶", "å…¶ä»–"]
CAT_TO_IDX = {cat: i for i, cat in enumerate(CATEGORIES)}
IDX_TO_CAT = {i: cat for i, cat in enumerate(CATEGORIES)}

SUSPICIOUS_SAMPLES = [
    "å…¶ä»–/old_0033.dxf",
    "å…¶ä»–/old_0085.dxf",
    "è¿æ¥ä»¶/old_0008.dxf",
    "å…¶ä»–/new_0208.dxf",
]


def extract_geometric_features(dxf_path: str) -> np.ndarray:
    """æå–48ç»´å‡ ä½•ç‰¹å¾"""
    try:
        import ezdxf
        doc = ezdxf.readfile(dxf_path)
        msp = doc.modelspace()

        entity_counts = Counter()
        all_x, all_y = [], []
        line_lengths = []
        circle_radii = []
        arc_angles = []
        layer_names = set()

        for entity in msp:
            etype = entity.dxftype()
            entity_counts[etype] += 1

            if hasattr(entity.dxf, 'layer'):
                layer_names.add(entity.dxf.layer)

            try:
                if etype == "LINE":
                    x1, y1 = entity.dxf.start.x, entity.dxf.start.y
                    x2, y2 = entity.dxf.end.x, entity.dxf.end.y
                    length = np.sqrt((x2-x1)**2 + (y2-y1)**2)
                    line_lengths.append(length)
                    all_x.extend([x1, x2])
                    all_y.extend([y1, y2])
                elif etype == "CIRCLE":
                    cx, cy = entity.dxf.center.x, entity.dxf.center.y
                    r = entity.dxf.radius
                    circle_radii.append(r)
                    all_x.extend([cx-r, cx+r])
                    all_y.extend([cy-r, cy+r])
                elif etype == "ARC":
                    cx, cy = entity.dxf.center.x, entity.dxf.center.y
                    r = entity.dxf.radius
                    angle = abs(entity.dxf.end_angle - entity.dxf.start_angle)
                    arc_angles.append(angle)
                    all_x.extend([cx-r, cx+r])
                    all_y.extend([cy-r, cy+r])
                elif etype in ["LWPOLYLINE", "POLYLINE"]:
                    if hasattr(entity, 'get_points'):
                        pts = list(entity.get_points())
                        for p in pts:
                            all_x.append(p[0])
                            all_y.append(p[1])
            except:
                pass

        total = sum(entity_counts.values())
        if total == 0:
            return np.zeros(48)

        # åŸºç¡€å®ä½“ç»Ÿè®¡ (10ç»´)
        features = [
            total,
            entity_counts.get("LINE", 0),
            entity_counts.get("CIRCLE", 0),
            entity_counts.get("ARC", 0),
            entity_counts.get("LWPOLYLINE", 0) + entity_counts.get("POLYLINE", 0),
            entity_counts.get("SPLINE", 0),
            entity_counts.get("ELLIPSE", 0),
            entity_counts.get("POINT", 0),
            entity_counts.get("TEXT", 0) + entity_counts.get("MTEXT", 0),
            len(layer_names),
        ]

        # æ¯”ä¾‹ç‰¹å¾ (8ç»´)
        features.extend([
            entity_counts.get("LINE", 0) / total,
            entity_counts.get("CIRCLE", 0) / total,
            entity_counts.get("ARC", 0) / total,
            (entity_counts.get("CIRCLE", 0) + entity_counts.get("ARC", 0)) / total,
            (entity_counts.get("LWPOLYLINE", 0) + entity_counts.get("POLYLINE", 0)) / total,
            entity_counts.get("SPLINE", 0) / total,
            (entity_counts.get("TEXT", 0) + entity_counts.get("MTEXT", 0)) / total,
            len(entity_counts) / 20,
        ])

        # è¾¹ç•Œæ¡†ç‰¹å¾ (8ç»´)
        if all_x and all_y:
            x_min, x_max = min(all_x), max(all_x)
            y_min, y_max = min(all_y), max(all_y)
            width = x_max - x_min
            height = y_max - y_min
            aspect_ratio = width / max(height, 1e-6)
            area = width * height
            features.extend([
                width, height, aspect_ratio, area,
                np.log1p(width), np.log1p(height),
                np.log1p(area), 1.0 if aspect_ratio > 2 else 0.0,
            ])
        else:
            features.extend([0]*8)

        # çº¿æ®µç‰¹å¾ (6ç»´)
        if line_lengths:
            features.extend([
                np.mean(line_lengths), np.std(line_lengths),
                np.min(line_lengths), np.max(line_lengths),
                len(line_lengths), np.log1p(np.sum(line_lengths)),
            ])
        else:
            features.extend([0]*6)

        # åœ†ç‰¹å¾ (6ç»´)
        if circle_radii:
            features.extend([
                np.mean(circle_radii), np.std(circle_radii),
                np.min(circle_radii), np.max(circle_radii),
                len(circle_radii), np.log1p(np.sum(circle_radii)),
            ])
        else:
            features.extend([0]*6)

        # å¼§ç‰¹å¾ (6ç»´)
        if arc_angles:
            features.extend([
                np.mean(arc_angles), np.std(arc_angles),
                np.min(arc_angles), np.max(arc_angles),
                len(arc_angles), np.sum(arc_angles) / 360,
            ])
        else:
            features.extend([0]*6)

        # å¤æ‚åº¦ç‰¹å¾ (4ç»´)
        features.extend([
            total / max(len(layer_names), 1),
            np.log1p(total),
            len(entity_counts),
            (entity_counts.get("SPLINE", 0) + entity_counts.get("ELLIPSE", 0)) / max(total, 1),
        ])

        return np.array(features[:48], dtype=np.float32)

    except Exception as e:
        logger.error(f"æå–ç‰¹å¾å¤±è´¥ {dxf_path}: {e}")
        return np.zeros(48, dtype=np.float32)


class ImprovedClassifierV6(nn.Module):
    """V6åˆ†ç±»å™¨ç»“æ„"""

    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int, dropout: float = 0.5):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.1),
            nn.Dropout(dropout),

            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.1),
            nn.Dropout(dropout),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.LeakyReLU(0.1),
            nn.Dropout(dropout * 0.6),

            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.BatchNorm1d(hidden_dim // 4),
            nn.LeakyReLU(0.1),
            nn.Dropout(dropout * 0.4),

            nn.Linear(hidden_dim // 4, num_classes)
        )

    def forward(self, x):
        return self.net(x)


def load_data():
    """åŠ è½½æ•°æ®é›†"""
    manifest_path = REAL_DATA_DIR / "manifest.json"
    with open(manifest_path, 'r', encoding='utf-8') as f:
        manifest = json.load(f)
    return manifest


def main():
    logger.info("=" * 60)
    logger.info("åˆ†æV16æœ€ç»ˆé”™è¯¯æ ·æœ¬")
    logger.info("=" * 60)

    # åŠ è½½V6æ¨¡å‹
    logger.info("\nåŠ è½½V6å‡ ä½•åˆ†ç±»å™¨...")
    v6_path = MODEL_DIR / "cad_classifier_v6.pt"
    v6_checkpoint = torch.load(v6_path, map_location=DEVICE, weights_only=False)

    v6_model = ImprovedClassifierV6(input_dim=48, hidden_dim=256, num_classes=5, dropout=0.5)
    v6_model.load_state_dict(v6_checkpoint['model_state_dict'])
    v6_model.to(DEVICE)
    v6_model.eval()

    # ç‰¹å¾æ ‡å‡†åŒ–å‚æ•°
    feature_mean = v6_checkpoint.get('feature_mean', np.zeros(48))
    feature_std = v6_checkpoint.get('feature_std', np.ones(48))

    # åŠ è½½æ•°æ®
    manifest = load_data()
    logger.info(f"æ€»æ ·æœ¬æ•°: {len(manifest)}")

    # è®¡ç®—å„ç±»åˆ«çš„ç‰¹å¾ç»Ÿè®¡
    logger.info("\nè®¡ç®—å„ç±»åˆ«ç‰¹å¾ç»Ÿè®¡...")
    class_features = {cat: [] for cat in CATEGORIES}

    for item in manifest:
        file_path = REAL_DATA_DIR / item["file"]
        category = item["category"]
        features = extract_geometric_features(str(file_path))
        class_features[category].append(features)

    # è®¡ç®—ç±»ä¸­å¿ƒ
    class_centers = {}
    for cat in CATEGORIES:
        if class_features[cat]:
            class_centers[cat] = np.mean(class_features[cat], axis=0)

    # V6é¢„æµ‹æ‰€æœ‰æ ·æœ¬
    logger.info("\nV6é¢„æµ‹ç»“æœåˆ†æ...")
    errors = []
    correct = 0
    total = 0

    for item in manifest:
        file_path = REAL_DATA_DIR / item["file"]
        true_cat = item["category"]

        features = extract_geometric_features(str(file_path))
        features_norm = (features - feature_mean) / (feature_std + 1e-8)

        with torch.no_grad():
            x = torch.FloatTensor(features_norm).unsqueeze(0).to(DEVICE)
            logits = v6_model(x)
            probs = torch.softmax(logits, dim=1)
            pred_idx = torch.argmax(probs, dim=1).item()
            pred_cat = IDX_TO_CAT[pred_idx]
            confidence = probs[0, pred_idx].item()

        total += 1
        if pred_cat == true_cat:
            correct += 1
        else:
            # è®¡ç®—åˆ°å„ç±»ä¸­å¿ƒçš„è·ç¦»
            distances = {}
            for cat, center in class_centers.items():
                dist = np.linalg.norm(features - center)
                distances[cat] = dist

            errors.append({
                "file": item["file"],
                "true": true_cat,
                "pred": pred_cat,
                "confidence": confidence,
                "is_suspicious": item["file"] in SUSPICIOUS_SAMPLES,
                "distances": distances,
            })

    logger.info(f"\nV6å‡†ç¡®ç‡: {correct}/{total} = {100*correct/total:.2f}%")
    logger.info(f"é”™è¯¯æ•°: {len(errors)}")

    # åˆ†ææ¯ä¸ªé”™è¯¯æ ·æœ¬
    logger.info("\n" + "=" * 60)
    logger.info("é”™è¯¯æ ·æœ¬è¯¦ç»†åˆ†æ")
    logger.info("=" * 60)

    for i, err in enumerate(errors):
        logger.info(f"\n{i+1}. {err['file']}")
        logger.info(f"   æ ‡æ³¨: {err['true']} â†’ é¢„æµ‹: {err['pred']} (ç½®ä¿¡åº¦: {err['confidence']:.2%})")
        logger.info(f"   å¯ç–‘æ ·æœ¬: {'æ˜¯' if err['is_suspicious'] else 'å¦'}")

        # è·ç¦»åˆ†æ
        sorted_dists = sorted(err['distances'].items(), key=lambda x: x[1])
        logger.info(f"   åˆ°å„ç±»ä¸­å¿ƒè·ç¦»:")
        for cat, dist in sorted_dists:
            marker = "â† æ ‡æ³¨" if cat == err['true'] else ("â† é¢„æµ‹" if cat == err['pred'] else "")
            logger.info(f"      {cat}: {dist:.2f} {marker}")

        # åˆ¤æ–­æ›´æ¥è¿‘å“ªä¸ªç±»
        nearest_cat = sorted_dists[0][0]
        if nearest_cat == err['pred']:
            logger.info(f"   ğŸ“Œ ç‰¹å¾æœ€æ¥è¿‘é¢„æµ‹ç±» '{err['pred']}'ï¼Œæ ‡æ³¨å¯èƒ½æœ‰è¯¯")
        elif nearest_cat == err['true']:
            logger.info(f"   âœ“ ç‰¹å¾æœ€æ¥è¿‘æ ‡æ³¨ç±» '{err['true']}'ï¼Œæ˜¯å›°éš¾æ ·æœ¬")
        else:
            logger.info(f"   âš ï¸ ç‰¹å¾æœ€æ¥è¿‘ '{nearest_cat}'ï¼Œä½†é¢„æµ‹ä¸º '{err['pred']}'")

    # ç»Ÿè®¡åˆ†æ
    suspicious_count = sum(1 for e in errors if e['is_suspicious'])
    label_issue_count = sum(1 for e in errors if sorted(e['distances'].items(), key=lambda x: x[1])[0][0] == e['pred'])

    logger.info("\n" + "=" * 60)
    logger.info("é”™è¯¯åˆ†ç±»æ±‡æ€»")
    logger.info("=" * 60)
    logger.info(f"æ€»é”™è¯¯æ•°: {len(errors)}")
    logger.info(f"å·²æ ‡è®°å¯ç–‘æ ·æœ¬: {suspicious_count}")
    logger.info(f"ç‰¹å¾æ¥è¿‘é¢„æµ‹ç±»(å¯èƒ½æ ‡æ³¨é”™è¯¯): {label_issue_count}")
    logger.info(f"ç‰¹å¾æ¥è¿‘æ ‡æ³¨ç±»(çœŸæ­£å›°éš¾æ ·æœ¬): {len(errors) - label_issue_count}")

    # å¦‚æœä¿®æ­£æ‰€æœ‰å¯èƒ½çš„æ ‡æ³¨é”™è¯¯
    potential_accuracy = (total - (len(errors) - label_issue_count)) / total
    logger.info(f"\nè‹¥ä¿®æ­£æ‰€æœ‰ç–‘ä¼¼æ ‡æ³¨é”™è¯¯ï¼Œç†è®ºå‡†ç¡®ç‡: {100*potential_accuracy:.2f}%")

    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªæ ‡è®°çš„å¯ç–‘æ ·æœ¬
    logger.info("\n" + "=" * 60)
    logger.info("å»ºè®®æ·»åŠ åˆ°å¯ç–‘æ ·æœ¬åˆ—è¡¨")
    logger.info("=" * 60)

    new_suspicious = []
    for err in errors:
        if not err['is_suspicious']:
            sorted_dists = sorted(err['distances'].items(), key=lambda x: x[1])
            if sorted_dists[0][0] == err['pred']:
                new_suspicious.append(err['file'])
                logger.info(f"  {err['file']}: {err['true']} â†’ {err['pred']}")

    if not new_suspicious:
        logger.info("  æ— æ–°å¢å¯ç–‘æ ·æœ¬")

    # æœ€ç»ˆç»“è®º
    logger.info("\n" + "=" * 60)
    logger.info("ç»“è®º")
    logger.info("=" * 60)

    clean_errors = len(errors) - label_issue_count
    clean_total = total - label_issue_count
    clean_accuracy = (clean_total - clean_errors) / clean_total if clean_total > 0 else 1.0

    logger.info(f"1. å½“å‰V6å‡†ç¡®ç‡: {100*correct/total:.2f}% ({correct}/{total})")
    logger.info(f"2. æ’é™¤æ ‡æ³¨é—®é¢˜å: {100*(total-len(errors))/(total-label_issue_count):.2f}%")
    logger.info(f"3. çœŸæ­£å›°éš¾æ ·æœ¬æ•°: {clean_errors}")

    if clean_errors == 0:
        logger.info("\nğŸ‰ æ‰€æœ‰é”™è¯¯å‡ä¸ºç–‘ä¼¼æ ‡æ³¨é—®é¢˜ï¼Œæ¨¡å‹å·²è¾¾ç†è®ºæœ€ä¼˜!")
    else:
        logger.info(f"\nè¿˜æœ‰ {clean_errors} ä¸ªçœŸæ­£å›°éš¾æ ·æœ¬éœ€è¦è¿›ä¸€æ­¥åˆ†æ")


if __name__ == "__main__":
    main()

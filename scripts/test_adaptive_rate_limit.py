#!/usr/bin/env python3
"""
è‡ªé€‚åº”é™æµç³»ç»Ÿæµ‹è¯•å¥—ä»¶
æµ‹è¯•æ‰€æœ‰é™æµç»„ä»¶çš„åŠŸèƒ½å’Œé›†æˆ
"""

import os
import sys
import json
import time
import random
import tempfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any
import subprocess
import threading
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥è¦æµ‹è¯•çš„æ¨¡å—
try:
    from scripts.adaptive_rate_limiter import (
        AdaptiveRateLimiter,
        RateLimitConfig,
        SystemMetrics,
        Decision,
        Request,
        TokenBucketAlgorithm,
        LeakyBucketAlgorithm,
        SlidingWindowAlgorithm,
    )
    from scripts.rate_limit_analyzer import (
        RateLimitAnalyzer,
        PatternType,
        TrafficMetrics,
        UserProfile,
        AnomalyDetector,
        PatternDetector,
    )
    from scripts.auto_calibrator import (
        AutoCalibrator,
        OptimizationGoal,
        Parameters,
        PerformanceScore,
        TestStatus,
    )
    from scripts.performance_monitor import (
        PerformanceMonitor,
        SLAConfig,
        ComplianceStatus,
        ImpactLevel,
        AlertSeverity,
        MetricSnapshot,
    )
except ImportError:
    from adaptive_rate_limiter import (
        AdaptiveRateLimiter,
        RateLimitConfig,
        SystemMetrics,
        Decision,
        Request,
        TokenBucketAlgorithm,
        LeakyBucketAlgorithm,
        SlidingWindowAlgorithm,
    )
    from rate_limit_analyzer import (
        RateLimitAnalyzer,
        PatternType,
        TrafficMetrics,
        UserProfile,
        AnomalyDetector,
        PatternDetector,
    )
    from auto_calibrator import (
        AutoCalibrator,
        OptimizationGoal,
        Parameters,
        PerformanceScore,
        TestStatus,
    )
    from performance_monitor import (
        PerformanceMonitor,
        SLAConfig,
        ComplianceStatus,
        ImpactLevel,
        AlertSeverity,
        MetricSnapshot,
    )


def run_command(cmd: str, check: bool = False) -> tuple:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            capture_output=True,
            text=True,
            check=check
        )
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.CalledProcessError as e:
        return False, e.stdout, e.stderr


class TestAdaptiveRateLimiter:
    """æµ‹è¯•è‡ªé€‚åº”é™æµå™¨"""

    def __init__(self):
        self.passed = 0
        self.failed = 0
        self.results = []

    def test_token_bucket(self):
        """æµ‹è¯•ä»¤ç‰Œæ¡¶ç®—æ³•"""
        print("\nğŸ” æµ‹è¯•ä»¤ç‰Œæ¡¶ç®—æ³•...")

        config = RateLimitConfig(
            algorithm="token_bucket",
            rate_limit=10,  # 10 req/s
            burst_size=5
        )
        limiter = AdaptiveRateLimiter(config)

        # æµ‹è¯•çªå‘è¯·æ±‚
        allowed = 0
        for i in range(10):
            request = Request(
                id=f"req_{i}",
                user_id="test_user",
                ip="127.0.0.1",
                path="/api/test",
                timestamp=datetime.now()
            )
            metrics = SystemMetrics(
                cpu_usage=0.5,
                memory_usage=0.6,
                latency_p50=50,
                latency_p95=95,
                latency_p99=150,
                error_rate=0.01,
                request_rate=8,
                active_connections=100
            )
            decision = limiter.should_allow_request(request, metrics)
            if decision.allowed:
                allowed += 1

        # å‰5ä¸ªåº”è¯¥è¢«å…è®¸ï¼ˆburst sizeï¼‰
        if allowed >= 5:
            self.passed += 1
            print(f"  âœ… ä»¤ç‰Œæ¡¶çªå‘å¤„ç†æ­£ç¡®: {allowed}/10 è¯·æ±‚è¢«å…è®¸")
            return True
        else:
            self.failed += 1
            print(f"  âŒ ä»¤ç‰Œæ¡¶çªå‘å¤„ç†å¤±è´¥: åªæœ‰ {allowed}/10 è¯·æ±‚è¢«å…è®¸")
            return False

    def test_sliding_window(self):
        """æµ‹è¯•æ»‘åŠ¨çª—å£ç®—æ³•"""
        print("\nğŸ” æµ‹è¯•æ»‘åŠ¨çª—å£ç®—æ³•...")

        config = RateLimitConfig(
            algorithm="sliding_window",
            rate_limit=100,
            window_size=1.0  # 1ç§’çª—å£
        )
        limiter = AdaptiveRateLimiter(config)

        # æµ‹è¯•çª—å£å†…è¯·æ±‚è®¡æ•°
        metrics = SystemMetrics(
            cpu_usage=0.5, memory_usage=0.6,
            latency_p50=50, latency_p95=95, latency_p99=150,
            error_rate=0.01, request_rate=80, active_connections=100
        )

        # å‘é€è¯·æ±‚ç›´åˆ°è¢«é™æµ
        allowed_count = 0
        for i in range(150):
            request = Request(
                id=f"req_{i}",
                user_id="test_user",
                ip="127.0.0.1",
                path="/api/test",
                timestamp=datetime.now()
            )
            decision = limiter.should_allow_request(request, metrics)
            if decision.allowed:
                allowed_count += 1
            time.sleep(0.001)  # å°å»¶è¿Ÿ

        # åº”è¯¥æ¥è¿‘ä½†ä¸è¶…è¿‡rate_limit
        if 90 <= allowed_count <= 110:
            self.passed += 1
            print(f"  âœ… æ»‘åŠ¨çª—å£é™æµæ­£ç¡®: {allowed_count} è¯·æ±‚è¢«å…è®¸")
            return True
        else:
            self.failed += 1
            print(f"  âŒ æ»‘åŠ¨çª—å£é™æµå¤±è´¥: {allowed_count} è¯·æ±‚è¢«å…è®¸ï¼ˆæœŸæœ›çº¦100ï¼‰")
            return False

    def test_adaptation(self):
        """æµ‹è¯•è‡ªé€‚åº”è°ƒæ•´"""
        print("\nğŸ” æµ‹è¯•è‡ªé€‚åº”è°ƒæ•´...")

        config = RateLimitConfig(
            algorithm="adaptive_window",
            rate_limit=100,
            enable_adaptation=True
        )
        limiter = AdaptiveRateLimiter(config)

        # æ¨¡æ‹Ÿé«˜è´Ÿè½½
        high_load_metrics = SystemMetrics(
            cpu_usage=0.9,
            memory_usage=0.85,
            latency_p50=100,
            latency_p95=200,
            latency_p99=500,
            error_rate=0.05,
            request_rate=150,
            active_connections=500
        )

        # è§¦å‘è‡ªé€‚åº”
        limiter.adapt_thresholds(high_load_metrics)
        new_threshold = limiter.get_effective_rate_limit()

        # é«˜è´Ÿè½½æ—¶åº”è¯¥é™ä½é˜ˆå€¼
        if new_threshold < 100:
            self.passed += 1
            print(f"  âœ… é«˜è´Ÿè½½è‡ªé€‚åº”æ­£ç¡®: é˜ˆå€¼é™è‡³ {new_threshold}")

            # æµ‹è¯•ä½è´Ÿè½½æ¢å¤
            low_load_metrics = SystemMetrics(
                cpu_usage=0.3,
                memory_usage=0.4,
                latency_p50=30,
                latency_p95=60,
                latency_p99=90,
                error_rate=0.001,
                request_rate=50,
                active_connections=100
            )
            limiter.adapt_thresholds(low_load_metrics)
            recovered_threshold = limiter.get_effective_rate_limit()

            if recovered_threshold > new_threshold:
                print(f"  âœ… ä½è´Ÿè½½æ¢å¤æ­£ç¡®: é˜ˆå€¼æ¢å¤è‡³ {recovered_threshold}")
                return True

        self.failed += 1
        print(f"  âŒ è‡ªé€‚åº”è°ƒæ•´å¤±è´¥")
        return False

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•è‡ªé€‚åº”é™æµå™¨")
        print("="*60)

        test_methods = [
            self.test_token_bucket,
            self.test_sliding_window,
            self.test_adaptation,
        ]

        for test in test_methods:
            try:
                test()
            except Exception as e:
                self.failed += 1
                print(f"  âŒ æµ‹è¯•å¼‚å¸¸: {e}")

        return {
            'component': 'AdaptiveRateLimiter',
            'passed': self.passed,
            'failed': self.failed,
            'total': self.passed + self.failed
        }


class TestRateLimitAnalyzer:
    """æµ‹è¯•æµé‡åˆ†æå™¨"""

    def __init__(self):
        self.passed = 0
        self.failed = 0

    def test_pattern_detection(self):
        """æµ‹è¯•æµé‡æ¨¡å¼æ£€æµ‹"""
        print("\nğŸ” æµ‹è¯•æµé‡æ¨¡å¼æ£€æµ‹...")

        analyzer = RateLimitAnalyzer()

        # æµ‹è¯•æ­£å¸¸æµé‡
        normal_metrics = TrafficMetrics(
            request_count=1000,
            unique_ips=100,
            avg_request_rate=100,
            peak_request_rate=150,
            error_rate=0.01,
            avg_response_time=50,
            bandwidth_usage=10 * 1024 * 1024,
            time_window=60
        )
        pattern = analyzer.classify_traffic_pattern(normal_metrics)

        if pattern == PatternType.NORMAL:
            self.passed += 1
            print(f"  âœ… æ­£å¸¸æµé‡è¯†åˆ«æ­£ç¡®")
        else:
            self.failed += 1
            print(f"  âŒ æ­£å¸¸æµé‡è¯†åˆ«å¤±è´¥: {pattern}")

        # æµ‹è¯•DDoSæ¨¡å¼
        ddos_metrics = TrafficMetrics(
            request_count=100000,
            unique_ips=10,
            avg_request_rate=5000,
            peak_request_rate=10000,
            error_rate=0.5,
            avg_response_time=500,
            bandwidth_usage=1000 * 1024 * 1024,
            time_window=60
        )
        pattern = analyzer.classify_traffic_pattern(ddos_metrics)

        if pattern == PatternType.DDOS:
            self.passed += 1
            print(f"  âœ… DDoSæ”»å‡»è¯†åˆ«æ­£ç¡®")
            return True
        else:
            self.failed += 1
            print(f"  âŒ DDoSæ”»å‡»è¯†åˆ«å¤±è´¥: {pattern}")
            return False

    def test_anomaly_detection(self):
        """æµ‹è¯•å¼‚å¸¸æ£€æµ‹"""
        print("\nğŸ” æµ‹è¯•å¼‚å¸¸æ£€æµ‹...")

        detector = AnomalyDetector()

        # æ·»åŠ æ­£å¸¸æ•°æ®å»ºç«‹åŸºçº¿
        for _ in range(100):
            detector.add_sample(random.uniform(90, 110))

        # æµ‹è¯•æ­£å¸¸å€¼
        normal_score = detector.detect_anomaly(100)
        if normal_score < 0.5:
            self.passed += 1
            print(f"  âœ… æ­£å¸¸å€¼æ£€æµ‹æ­£ç¡®: å¼‚å¸¸åˆ†æ•° {normal_score:.2f}")
        else:
            self.failed += 1
            print(f"  âŒ æ­£å¸¸å€¼æ£€æµ‹å¤±è´¥: å¼‚å¸¸åˆ†æ•° {normal_score:.2f}")

        # æµ‹è¯•å¼‚å¸¸å€¼
        anomaly_score = detector.detect_anomaly(500)
        if anomaly_score > 0.7:
            self.passed += 1
            print(f"  âœ… å¼‚å¸¸å€¼æ£€æµ‹æ­£ç¡®: å¼‚å¸¸åˆ†æ•° {anomaly_score:.2f}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ å¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥: å¼‚å¸¸åˆ†æ•° {anomaly_score:.2f}")
            return False

    def test_user_profiling(self):
        """æµ‹è¯•ç”¨æˆ·è¡Œä¸ºç”»åƒ"""
        print("\nğŸ” æµ‹è¯•ç”¨æˆ·è¡Œä¸ºç”»åƒ...")

        analyzer = RateLimitAnalyzer()

        # æ·»åŠ ç”¨æˆ·è¯·æ±‚å†å²
        user_id = "test_user"
        for i in range(100):
            analyzer.record_request(
                user_id=user_id,
                timestamp=datetime.now() - timedelta(seconds=i),
                path="/api/test",
                response_time=random.uniform(40, 60),
                status_code=200
            )

        # ç”Ÿæˆç”¨æˆ·ç”»åƒ
        profile = analyzer.generate_user_profile(user_id)

        if profile and profile.reputation_score > 0:
            self.passed += 1
            print(f"  âœ… ç”¨æˆ·ç”»åƒç”ŸæˆæˆåŠŸ: ä¿¡èª‰åˆ† {profile.reputation_score:.2f}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ ç”¨æˆ·ç”»åƒç”Ÿæˆå¤±è´¥")
            return False

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•æµé‡åˆ†æå™¨")
        print("="*60)

        test_methods = [
            self.test_pattern_detection,
            self.test_anomaly_detection,
            self.test_user_profiling,
        ]

        for test in test_methods:
            try:
                test()
            except Exception as e:
                self.failed += 1
                print(f"  âŒ æµ‹è¯•å¼‚å¸¸: {e}")

        return {
            'component': 'RateLimitAnalyzer',
            'passed': self.passed,
            'failed': self.failed,
            'total': self.passed + self.failed
        }


class TestAutoCalibrator:
    """æµ‹è¯•å‚æ•°æ ¡å‡†å™¨"""

    def __init__(self):
        self.passed = 0
        self.failed = 0

    def test_parameter_optimization(self):
        """æµ‹è¯•å‚æ•°ä¼˜åŒ–"""
        print("\nğŸ” æµ‹è¯•å‚æ•°ä¼˜åŒ–...")

        calibrator = AutoCalibrator(
            optimization_goal=OptimizationGoal.BALANCE_PERFORMANCE
        )

        # åˆå§‹å‚æ•°
        params = Parameters(
            rate_limit=1000,
            burst_size=100,
            window_size=60,
            max_retries=3,
            backoff_factor=2.0,
            cpu_threshold=0.8,
            memory_threshold=0.85,
            latency_threshold=100,
            error_threshold=0.01
        )

        # ç³»ç»ŸæŒ‡æ ‡
        metrics = SystemMetrics(
            cpu_usage=0.7,
            memory_usage=0.75,
            latency_p50=60,
            latency_p95=120,
            latency_p99=200,
            error_rate=0.02,
            request_rate=800,
            active_connections=500
        )

        # è¯„ä¼°æ€§èƒ½
        score = calibrator.evaluate_performance(params, metrics)

        if score.overall_score > 0:
            self.passed += 1
            print(f"  âœ… æ€§èƒ½è¯„ä¼°æˆåŠŸ: ç»¼åˆè¯„åˆ† {score.overall_score:.2f}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ æ€§èƒ½è¯„ä¼°å¤±è´¥")
            return False

    def test_ab_testing(self):
        """æµ‹è¯•A/Bæµ‹è¯•åŠŸèƒ½"""
        print("\nğŸ” æµ‹è¯•A/Bæµ‹è¯•...")

        calibrator = AutoCalibrator()

        params_a = Parameters(
            rate_limit=1000, burst_size=100, window_size=60,
            max_retries=3, backoff_factor=2.0,
            cpu_threshold=0.8, memory_threshold=0.85,
            latency_threshold=100, error_threshold=0.01
        )

        params_b = params_a.mutate(0.2)  # 20%å˜å¼‚

        # è¿è¡ŒçŸ­æ—¶é—´æµ‹è¯•
        result = calibrator.run_ab_test(
            variant_a=params_a,
            variant_b=params_b,
            duration=1,  # 1ç§’å¿«é€Ÿæµ‹è¯•
            traffic_split=0.5
        )

        if result.status == TestStatus.COMPLETED:
            self.passed += 1
            print(f"  âœ… A/Bæµ‹è¯•å®Œæˆ: è·èƒœè€… {result.winner}, På€¼ {result.p_value:.4f}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ A/Bæµ‹è¯•å¤±è´¥: {result.status.value}")
            return False

    def test_recommendation(self):
        """æµ‹è¯•å‚æ•°å»ºè®®"""
        print("\nğŸ” æµ‹è¯•å‚æ•°å»ºè®®...")

        calibrator = AutoCalibrator()
        analyzer = RateLimitAnalyzer()

        # åˆ›å»ºæµé‡åˆ†æ
        from scripts.rate_limit_analyzer import TrafficAnalysis
        traffic_analysis = TrafficAnalysis(
            pattern=PatternType.SPIKE,
            confidence=0.8,
            anomaly_score=0.2,
            metrics=TrafficMetrics(
                request_count=10000,
                unique_ips=500,
                avg_request_rate=1500,
                peak_request_rate=3000,
                error_rate=0.02,
                avg_response_time=80,
                bandwidth_usage=100 * 1024 * 1024,
                time_window=60
            ),
            recommendations=[]
        )

        # è·å–å»ºè®®
        recommendations = calibrator.get_recommendation(traffic_analysis)

        if recommendations and 'suggestions' in recommendations:
            self.passed += 1
            print(f"  âœ… å»ºè®®ç”ŸæˆæˆåŠŸ: {len(recommendations['suggestions'])} æ¡å»ºè®®")
            for suggestion in recommendations['suggestions'][:3]:
                print(f"    - {suggestion}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ å»ºè®®ç”Ÿæˆå¤±è´¥")
            return False

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•å‚æ•°æ ¡å‡†å™¨")
        print("="*60)

        test_methods = [
            self.test_parameter_optimization,
            self.test_ab_testing,
            self.test_recommendation,
        ]

        for test in test_methods:
            try:
                test()
            except Exception as e:
                self.failed += 1
                print(f"  âŒ æµ‹è¯•å¼‚å¸¸: {e}")

        return {
            'component': 'AutoCalibrator',
            'passed': self.passed,
            'failed': self.failed,
            'total': self.passed + self.failed
        }


class TestPerformanceMonitor:
    """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.passed = 0
        self.failed = 0

    def test_sla_compliance(self):
        """æµ‹è¯•SLAåˆè§„æ€§æ£€æŸ¥"""
        print("\nğŸ” æµ‹è¯•SLAåˆè§„æ€§æ£€æŸ¥...")

        sla_config = SLAConfig(
            availability_target=0.999,
            latency_p95_target=100,
            error_rate_target=0.001
        )
        monitor = PerformanceMonitor(sla_config)

        # æ·»åŠ æ­£å¸¸æŒ‡æ ‡
        for i in range(10):
            snapshot = MetricSnapshot(
                timestamp=datetime.now() - timedelta(seconds=i),
                throughput=1000,
                latency_p50=40,
                latency_p95=80,
                latency_p99=120,
                error_rate=0.0005,
                cpu_usage=0.6,
                memory_usage=0.65,
                rate_limited_requests=10,
                total_requests=1000,
                active_connections=500
            )
            monitor.add_metric_snapshot(snapshot)

        compliance = monitor.check_sla_compliance()

        if compliance == ComplianceStatus.COMPLIANT:
            self.passed += 1
            print(f"  âœ… SLAåˆè§„æ£€æŸ¥æ­£ç¡®: {compliance.value}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ SLAåˆè§„æ£€æŸ¥å¤±è´¥: {compliance.value}")
            return False

    def test_impact_monitoring(self):
        """æµ‹è¯•å½±å“ç›‘æ§"""
        print("\nğŸ” æµ‹è¯•å½±å“ç›‘æ§...")

        monitor = PerformanceMonitor()

        # æ·»åŠ é€€åŒ–æŒ‡æ ‡
        for i in range(10):
            snapshot = MetricSnapshot(
                timestamp=datetime.now() - timedelta(seconds=i),
                throughput=500,  # ä½ååé‡
                latency_p50=100,
                latency_p95=200,  # é«˜å»¶è¿Ÿ
                latency_p99=500,
                error_rate=0.05,  # é«˜é”™è¯¯ç‡
                cpu_usage=0.9,
                memory_usage=0.85,
                rate_limited_requests=300,  # å¤§é‡é™æµ
                total_requests=1000,
                active_connections=500
            )
            monitor.add_metric_snapshot(snapshot)

        # ç›‘æ§å½±å“
        report = monitor.monitor_impact()

        if report.impact_level in [ImpactLevel.HIGH, ImpactLevel.SEVERE]:
            self.passed += 1
            print(f"  âœ… å½±å“è¯„ä¼°æ­£ç¡®: {report.impact_level.value}")
            if report.recommendations:
                print(f"    å»ºè®®: {report.recommendations[0]}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ å½±å“è¯„ä¼°å¤±è´¥: {report.impact_level.value}")
            return False

    def test_alert_generation(self):
        """æµ‹è¯•å‘Šè­¦ç”Ÿæˆ"""
        print("\nğŸ” æµ‹è¯•å‘Šè­¦ç”Ÿæˆ...")

        monitor = PerformanceMonitor()

        # æ·»åŠ è§¦å‘å‘Šè­¦çš„æŒ‡æ ‡
        for i in range(5):
            snapshot = MetricSnapshot(
                timestamp=datetime.now() - timedelta(seconds=i),
                throughput=100,
                latency_p50=200,
                latency_p95=500,
                latency_p99=1000,
                error_rate=0.1,
                cpu_usage=0.95,
                memory_usage=0.92,
                rate_limited_requests=500,
                total_requests=1000,
                active_connections=500
            )
            monitor.add_metric_snapshot(snapshot)

        alerts = monitor.generate_alerts()

        if alerts and any(a.severity in [AlertSeverity.ERROR, AlertSeverity.CRITICAL] for a in alerts):
            self.passed += 1
            print(f"  âœ… å‘Šè­¦ç”ŸæˆæˆåŠŸ: {len(alerts)} ä¸ªå‘Šè­¦")
            for alert in alerts[:3]:
                print(f"    [{alert.severity.value}] {alert.message}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ å‘Šè­¦ç”Ÿæˆå¤±è´¥")
            return False

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨")
        print("="*60)

        test_methods = [
            self.test_sla_compliance,
            self.test_impact_monitoring,
            self.test_alert_generation,
        ]

        for test in test_methods:
            try:
                test()
            except Exception as e:
                self.failed += 1
                print(f"  âŒ æµ‹è¯•å¼‚å¸¸: {e}")

        return {
            'component': 'PerformanceMonitor',
            'passed': self.passed,
            'failed': self.failed,
            'total': self.passed + self.failed
        }


class TestIntegration:
    """é›†æˆæµ‹è¯•"""

    def __init__(self):
        self.passed = 0
        self.failed = 0

    def test_end_to_end(self):
        """ç«¯åˆ°ç«¯æµ‹è¯•"""
        print("\nğŸ” è¿è¡Œç«¯åˆ°ç«¯é›†æˆæµ‹è¯•...")

        # åˆ›å»ºæ‰€æœ‰ç»„ä»¶
        config = RateLimitConfig(
            algorithm="adaptive_window",
            rate_limit=1000,
            enable_adaptation=True
        )
        limiter = AdaptiveRateLimiter(config)
        analyzer = RateLimitAnalyzer()
        calibrator = AutoCalibrator()
        monitor = PerformanceMonitor()

        # æ¨¡æ‹Ÿæµé‡
        print("  æ¨¡æ‹Ÿæµé‡å¤„ç†...")
        for i in range(100):
            # åˆ›å»ºè¯·æ±‚
            request = Request(
                id=f"req_{i}",
                user_id=f"user_{i % 10}",
                ip=f"192.168.1.{i % 256}",
                path="/api/test",
                timestamp=datetime.now()
            )

            # è·å–ç³»ç»ŸæŒ‡æ ‡
            metrics = SystemMetrics(
                cpu_usage=random.uniform(0.5, 0.8),
                memory_usage=random.uniform(0.6, 0.8),
                latency_p50=random.uniform(40, 60),
                latency_p95=random.uniform(80, 120),
                latency_p99=random.uniform(150, 200),
                error_rate=random.uniform(0.001, 0.01),
                request_rate=random.uniform(800, 1200),
                active_connections=random.randint(400, 600)
            )

            # é™æµå†³ç­–
            decision = limiter.should_allow_request(request, metrics)

            # è®°å½•æŒ‡æ ‡
            snapshot = MetricSnapshot(
                timestamp=datetime.now(),
                throughput=metrics.request_rate,
                latency_p50=metrics.latency_p50,
                latency_p95=metrics.latency_p95,
                latency_p99=metrics.latency_p99,
                error_rate=metrics.error_rate,
                cpu_usage=metrics.cpu_usage,
                memory_usage=metrics.memory_usage,
                rate_limited_requests=0 if decision.allowed else 1,
                total_requests=1,
                active_connections=metrics.active_connections
            )
            monitor.add_metric_snapshot(snapshot)

            time.sleep(0.01)

        # åˆ†ææµé‡
        print("  åˆ†ææµé‡æ¨¡å¼...")
        analysis = analyzer.analyze_traffic(60)

        # æ£€æŸ¥SLA
        print("  æ£€æŸ¥SLAåˆè§„æ€§...")
        compliance = monitor.check_sla_compliance()

        # ç”Ÿæˆå½±å“æŠ¥å‘Š
        print("  ç”Ÿæˆå½±å“æŠ¥å‘Š...")
        impact = monitor.monitor_impact()

        # éªŒè¯ç»“æœ
        if analysis and compliance and impact:
            self.passed += 1
            print(f"  âœ… ç«¯åˆ°ç«¯æµ‹è¯•æˆåŠŸ")
            print(f"    - æµé‡æ¨¡å¼: {analysis.pattern.value}")
            print(f"    - SLAçŠ¶æ€: {compliance.value}")
            print(f"    - å½±å“çº§åˆ«: {impact.impact_level.value}")
            return True
        else:
            self.failed += 1
            print(f"  âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥")
            return False

    def test_workflow(self):
        """æµ‹è¯•CI/CDå·¥ä½œæµ"""
        print("\nğŸ” æµ‹è¯•CI/CDå·¥ä½œæµ...")

        workflow_file = ".github/workflows/adaptive-rate-limit-monitor.yml"

        if Path(workflow_file).exists():
            with open(workflow_file, 'r') as f:
                content = f.read()

            required_jobs = [
                'analyze-traffic-patterns',
                'check-performance-impact',
                'calibrate-parameters',
                'load-test-validation',
                'monitor-dashboard',
                'rollback-on-failure'
            ]

            missing_jobs = []
            for job in required_jobs:
                if job not in content:
                    missing_jobs.append(job)

            if not missing_jobs:
                self.passed += 1
                print(f"  âœ… CI/CDå·¥ä½œæµé…ç½®æ­£ç¡®")
                return True
            else:
                self.failed += 1
                print(f"  âŒ CI/CDå·¥ä½œæµç¼ºå°‘job: {', '.join(missing_jobs)}")
                return False
        else:
            self.failed += 1
            print(f"  âŒ CI/CDå·¥ä½œæµæ–‡ä»¶ä¸å­˜åœ¨")
            return False

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ“Š é›†æˆæµ‹è¯•")
        print("="*60)

        test_methods = [
            self.test_end_to_end,
            self.test_workflow,
        ]

        for test in test_methods:
            try:
                test()
            except Exception as e:
                self.failed += 1
                print(f"  âŒ æµ‹è¯•å¼‚å¸¸: {e}")

        return {
            'component': 'Integration',
            'passed': self.passed,
            'failed': self.failed,
            'total': self.passed + self.failed
        }


def test_command_line():
    """æµ‹è¯•å‘½ä»¤è¡Œå·¥å…·"""
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•å‘½ä»¤è¡Œå·¥å…·")
    print("="*60)

    commands = [
        ("python3 scripts/adaptive_rate_limiter.py --help", "é™æµå™¨å¸®åŠ©"),
        ("python3 scripts/rate_limit_analyzer.py --help", "åˆ†æå™¨å¸®åŠ©"),
        ("python3 scripts/auto_calibrator.py --help", "æ ¡å‡†å™¨å¸®åŠ©"),
        ("python3 scripts/performance_monitor.py --help", "ç›‘æ§å™¨å¸®åŠ©"),
    ]

    passed = 0
    failed = 0

    for cmd, description in commands:
        success, stdout, stderr = run_command(cmd)
        if success:
            passed += 1
            print(f"  âœ… {description}: å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
        else:
            failed += 1
            print(f"  âŒ {description}: å‘½ä»¤æ‰§è¡Œå¤±è´¥")
            print(f"    é”™è¯¯: {stderr[:100]}")

    return {
        'component': 'CommandLine',
        'passed': passed,
        'failed': failed,
        'total': passed + failed
    }


def generate_test_report(results: List[Dict[str, Any]]):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("="*60)

    total_passed = sum(r['passed'] for r in results)
    total_failed = sum(r['failed'] for r in results)
    total_tests = sum(r['total'] for r in results)

    print(f"\næ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡: {total_passed}")
    print(f"å¤±è´¥: {total_failed}")
    print(f"é€šè¿‡ç‡: {(total_passed/total_tests*100):.1f}%")

    print("\nç»„ä»¶æµ‹è¯•ç»“æœ:")
    for result in results:
        status = "âœ…" if result['failed'] == 0 else "âŒ"
        print(f"  {status} {result['component']}: {result['passed']}/{result['total']} é€šè¿‡")

    # ç”ŸæˆJSONæŠ¥å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'total_tests': total_tests,
            'passed': total_passed,
            'failed': total_failed,
            'pass_rate': total_passed / total_tests * 100
        },
        'components': results
    }

    with open('test_report.json', 'w') as f:
        json.dump(report, f, indent=2)

    print("\næµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ° test_report.json")

    if total_failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è‡ªé€‚åº”é™æµç³»ç»Ÿæ­£å¸¸å·¥ä½œï¼")
        return True
    else:
        print(f"\nâš ï¸ {total_failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯• Day 8-10 è‡ªé€‚åº”é™æµç³»ç»Ÿ...")
    print("="*60)

    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 8):
        print("âš ï¸ è­¦å‘Š: Pythonç‰ˆæœ¬ä½äº3.8ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å…¼å®¹")

    results = []

    # è¿è¡Œç»„ä»¶æµ‹è¯•
    testers = [
        TestAdaptiveRateLimiter(),
        TestRateLimitAnalyzer(),
        TestAutoCalibrator(),
        TestPerformanceMonitor(),
        TestIntegration(),
    ]

    for tester in testers:
        result = tester.run_all_tests()
        results.append(result)

    # æµ‹è¯•å‘½ä»¤è¡Œå·¥å…·
    cmd_result = test_command_line()
    results.append(cmd_result)

    # ç”ŸæˆæŠ¥å‘Š
    success = generate_test_report(results)

    # ç”Ÿæˆä½¿ç”¨å»ºè®®
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. å¯åŠ¨ç›‘æ§:")
    print("   python3 scripts/performance_monitor.py --monitor")
    print("\n2. åˆ†ææµé‡:")
    print("   python3 scripts/rate_limit_analyzer.py --time-window 300")
    print("\n3. è¿è¡Œæ ¡å‡†:")
    print("   python3 scripts/auto_calibrator.py --goal balance")
    print("\n4. å¯ç”¨CI/CD:")
    print("   åœ¨GitHubä»“åº“å¯ç”¨ adaptive-rate-limit-monitor.yml å·¥ä½œæµ")

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
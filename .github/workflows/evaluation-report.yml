name: Evaluation Report

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      min_combined:
        description: 'Minimum combined score threshold'
        required: false
        default: '0.8'
      min_vision:
        description: 'Minimum vision score threshold'
        required: false
        default: '0.65'
      min_ocr:
        description: 'Minimum OCR score threshold'
        required: false
        default: '0.9'

env:
  PYTHON_VERSION: '3.11'
  REPORT_PATH: reports/eval_history/report
  ARTIFACT_RETENTION_DAYS: 30
  # Optimize Matplotlib in CI
  MPLCONFIGDIR: /tmp/matplotlib
  XDG_CACHE_HOME: /tmp/cache

jobs:
  evaluate:
    name: Run Evaluation and Generate Report
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for git info

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        # Install optional dependencies for enhanced validation
        pip install jsonschema==4.21.1

    - name: Run combined evaluation
      id: evaluation
      run: |
        # Use workflow inputs if provided, otherwise use defaults
        MIN_COMBINED="${{ github.event.inputs.min_combined || '0.8' }}"
        MIN_VISION="${{ github.event.inputs.min_vision || '0.65' }}"
        MIN_OCR="${{ github.event.inputs.min_ocr || '0.9' }}"

        echo "Running evaluation with thresholds:"
        echo "  Combined: $MIN_COMBINED"
        echo "  Vision: $MIN_VISION"
        echo "  OCR: $MIN_OCR"

        # Run evaluation and save history
        python3 scripts/evaluate_vision_ocr_combined.py \
          --save-history \
          --min-combined $MIN_COMBINED \
          --min-vision $MIN_VISION \
          --min-ocr $MIN_OCR | tee eval_output.txt

        # Extract scores for job summary
        echo "combined_score=$(grep -oP 'Combined Score: \K[0-9.]+' eval_output.txt || echo '0')" >> $GITHUB_OUTPUT
        echo "vision_score=$(grep -oP 'Vision Score: \K[0-9.]+' eval_output.txt || echo '0')" >> $GITHUB_OUTPUT
        echo "ocr_score=$(grep -oP 'OCR Score: \K[0-9.]+' eval_output.txt || echo '0')" >> $GITHUB_OUTPUT

    - name: Validate evaluation history
      run: |
        python3 scripts/validate_eval_history.py --dir reports/eval_history

    - name: Generate trend charts
      run: |
        python3 scripts/eval_trend.py --out reports/eval_history/plots
      continue-on-error: true  # Don't fail if charts can't be generated

    - name: Generate HTML report
      run: |
        python3 scripts/generate_eval_report.py \
          --out ${{ env.REPORT_PATH }}

    - name: Download Chart.js for integrity check
      run: |
        # Download Chart.js from CDN to local path for integrity verification
        mkdir -p reports/eval_history/report/assets
        curl -sL "https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js" \
          -o reports/eval_history/report/assets/chart.min.js

    - name: Check file integrity (strict mode)
      run: |
        echo "Checking Chart.js integrity..."
        # Run in strict mode since we're in CI environment
        python3 scripts/check_integrity.py --strict --verbose
        echo "âœ… Integrity check passed"

    - name: Validate history with JSON Schema
      run: |
        echo "Validating evaluation history against schema..."
        # With jsonschema installed, this will perform full validation
        python3 scripts/validate_eval_history.py --dir reports/eval_history --summary
        echo "âœ… Schema validation passed"

    - name: Generate insights and detect anomalies
      id: insights
      run: |
        echo "Analyzing evaluation trends and anomalies..."
        python3 scripts/analyze_eval_insights.py --days 30 --output reports/insights.md

        # Check for anomalies
        if python3 scripts/analyze_eval_insights.py --days 7 --threshold 0.1 --narrative-only | grep -q "Anomalies Detected"; then
          echo "has_anomalies=true" >> $GITHUB_OUTPUT
          echo "âš ï¸ Anomalies detected in recent evaluations"
        else
          echo "has_anomalies=false" >> $GITHUB_OUTPUT
          echo "âœ… No anomalies detected"
        fi

    - name: Export metrics for monitoring
      run: |
        echo "Exporting metrics in multiple formats..."
        python3 scripts/export_eval_metrics.py --format prometheus --output reports/metrics.prom
        python3 scripts/export_eval_metrics.py --format json --output reports/metrics.json
        echo "âœ… Metrics exported"

    - name: Security audit
      id: security
      continue-on-error: true
      run: |
        echo "Running security audit..."
        if python3 scripts/security_audit.py --severity medium --json > reports/security_audit.json 2>&1; then
          echo "security_status=pass" >> $GITHUB_OUTPUT
          echo "âœ… Security audit passed"
        else
          echo "security_status=fail" >> $GITHUB_OUTPUT
          echo "âš ï¸ Security issues found (non-blocking)"
        fi

    - name: Upload evaluation report
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-report-${{ github.run_number }}
        path: ${{ env.REPORT_PATH }}
        retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

    - name: Upload evaluation history
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-history-${{ github.run_number }}
        path: reports/eval_history/*.json
        retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

    - name: Send notifications
      if: always()
      env:
        EVAL_SLACK_WEBHOOK: ${{ secrets.EVAL_SLACK_WEBHOOK }}
        EVAL_SMTP_HOST: ${{ secrets.EVAL_SMTP_HOST }}
        EVAL_SMTP_PORT: ${{ secrets.EVAL_SMTP_PORT }}
        EVAL_EMAIL_FROM: ${{ secrets.EVAL_EMAIL_FROM }}
        EVAL_EMAIL_TO: ${{ secrets.EVAL_EMAIL_TO }}
        EVAL_EMAIL_PASSWORD: ${{ secrets.EVAL_EMAIL_PASSWORD }}
      run: |
        # Determine report URL
        if [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]]; then
          REPORT_URL="https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/"
        else
          REPORT_URL="https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        fi

        # Send notifications (only on threshold breach or scheduled runs)
        if [[ "${{ github.event_name }}" == "schedule" ]]; then
          python3 scripts/notify_eval_results.py --channel all --report-url "$REPORT_URL"
        else
          python3 scripts/notify_eval_results.py --channel all --threshold-breach-only --report-url "$REPORT_URL"
        fi
      continue-on-error: true

    - name: Create job summary
      run: |
        MIN_COMBINED="${{ github.event.inputs.min_combined || '0.8' }}"
        MIN_VISION="${{ github.event.inputs.min_vision || '0.65' }}"
        MIN_OCR="${{ github.event.inputs.min_ocr || '0.9' }}"

        echo "# ðŸ“Š Evaluation Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Scores" >> $GITHUB_STEP_SUMMARY
        echo "- **Combined Score**: ${{ steps.evaluation.outputs.combined_score }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Vision Score**: ${{ steps.evaluation.outputs.vision_score }}" >> $GITHUB_STEP_SUMMARY
        echo "- **OCR Score**: ${{ steps.evaluation.outputs.ocr_score }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Thresholds" >> $GITHUB_STEP_SUMMARY
        echo "- Combined: $MIN_COMBINED" >> $GITHUB_STEP_SUMMARY
        echo "- Vision: $MIN_VISION" >> $GITHUB_STEP_SUMMARY
        echo "- OCR: $MIN_OCR" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- [Download Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "*Generated at $(date -u '+%Y-%m-%d %H:%M:%S UTC')*" >> $GITHUB_STEP_SUMMARY

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const combined = parseFloat('${{ steps.evaluation.outputs.combined_score }}' || '0');
          const vision = parseFloat('${{ steps.evaluation.outputs.vision_score }}' || '0');
          const ocr = parseFloat('${{ steps.evaluation.outputs.ocr_score }}' || '0');

          const minCombined = parseFloat('${{ github.event.inputs.min_combined || '0.8' }}');
          const minVision = parseFloat('${{ github.event.inputs.min_vision || '0.65' }}');
          const minOcr = parseFloat('${{ github.event.inputs.min_ocr || '0.9' }}');

          const combinedStatus = combined >= minCombined ? 'âœ… Pass' : 'âŒ Fail';
          const visionStatus = vision >= minVision ? 'âœ… Pass' : 'âŒ Fail';
          const ocrStatus = ocr >= minOcr ? 'âœ… Pass' : 'âŒ Fail';

          const overallStatus = (combined >= minCombined && vision >= minVision && ocr >= minOcr)
            ? 'âœ… **All checks passed!**'
            : 'âš ï¸ **Some checks failed - review required**';

          const hasAnomalies = '${{ steps.insights.outputs.has_anomalies }}' === 'true';
          const securityStatus = '${{ steps.security.outputs.security_status }}';

          const body = `## ðŸ“Š CAD ML Platform - Evaluation Results

          ${overallStatus}

          ### Scores
          | Module | Score | Threshold | Status |
          |--------|-------|-----------|--------|
          | **Combined** | ${combined.toFixed(3)} | ${minCombined} | ${combinedStatus} |
          | **Vision** | ${vision.toFixed(3)} | ${minVision} | ${visionStatus} |
          | **OCR** | ${ocr.toFixed(3)} | ${minOcr} | ${ocrStatus} |

          ### Formula
          \`Combined Score = 0.5 Ã— Vision + 0.5 Ã— OCR_normalized\`

          ### Additional Analysis
          | Check | Status |
          |-------|--------|
          | **Anomaly Detection** | ${hasAnomalies ? 'âš ï¸ Anomalies detected' : 'âœ… No anomalies'} |
          | **Security Audit** | ${securityStatus === 'pass' ? 'âœ… Passed' : 'âš ï¸ Issues found'} |

          ### Quick Actions
          - ðŸ“‹ [View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - ðŸ“ˆ [Download Artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}#artifacts)
          - ðŸ” [Check Logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/jobs)

          ---
          *Updated: ${new Date().toISOString().replace('T', ' ').substring(0, 19)} UTC*
          *Commit: ${context.sha.substring(0, 7)}*`;

          // Find existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number
          });

          const botComment = comments.find(comment =>
            comment.user.type === 'Bot' &&
            comment.body.includes('CAD ML Platform - Evaluation Results')
          );

          // Update or create comment
          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: body
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
          }

  deploy-pages:
    name: Deploy Report to GitHub Pages
    runs-on: ubuntu-latest
    needs: evaluate
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    # Pages deployment is optional - don't fail workflow if Pages not enabled
    continue-on-error: true

    permissions:
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    - name: Download report artifact
      uses: actions/download-artifact@v4
      with:
        name: evaluation-report-${{ github.run_number }}
        path: ./public

    - name: Setup Pages
      uses: actions/configure-pages@v4
      continue-on-error: true

    - name: Upload to Pages
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./public

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
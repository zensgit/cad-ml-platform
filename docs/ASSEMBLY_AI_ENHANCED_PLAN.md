# ğŸš€ è£…é…ç†è§£AIå¢å¼ºç‰ˆå®æ–½æ–¹æ¡ˆ

> åŸºäºä¸“ä¸šåé¦ˆçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå¼ºåŒ–è¯æ®é©±åŠ¨ã€å¯è§£é‡Šæ€§å’Œè¯„æµ‹åŸºçº¿

---

## ğŸ“Š æ ¸å¿ƒå¢å¼ºç‚¹

### 1. è¯æ®é“¾ç³»ç»Ÿï¼ˆEvidence Chainï¼‰

#### 1.1 è¯æ®æ•°æ®ç»“æ„
```python
# src/models/evidence.py
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from enum import Enum

class EvidenceType(str, Enum):
    """è¯æ®ç±»å‹"""
    GEOMETRIC = "geometric"      # å‡ ä½•è¯æ®ï¼ˆæ¥è§¦é¢ã€è½´çº¿ï¼‰
    DIMENSIONAL = "dimensional"   # å°ºå¯¸è¯æ®ï¼ˆæ ‡æ³¨ã€å…¬å·®ï¼‰
    TEXTUAL = "textual"          # æ–‡æœ¬è¯æ®ï¼ˆæ ‡ç­¾ã€è¯´æ˜ï¼‰
    RULE_BASED = "rule_based"    # è§„åˆ™æ¨ç†
    LEARNED = "learned"          # æœºå™¨å­¦ä¹ æ¨æ–­

class Evidence(BaseModel):
    """è¯æ®å¯¹è±¡"""
    type: EvidenceType
    source: str                  # è¯æ®æ¥æºï¼ˆface_123, dimension_456ï¼‰
    confidence: float            # ç½®ä¿¡åº¦ [0,1]
    description: str             # äººç±»å¯è¯»æè¿°
    raw_data: Optional[Dict[str, Any]] = None  # åŸå§‹æ•°æ®

    class Config:
        json_schema_extra = {
            "example": {
                "type": "geometric",
                "source": "face_123_124_contact",
                "confidence": 0.95,
                "description": "åœ†æŸ±é¢123ä¸124åŒè½´æ¥è§¦ï¼Œåˆ¤å®šä¸ºè½´æ‰¿é…åˆ",
                "raw_data": {
                    "contact_area": 314.15,
                    "axis_deviation": 0.01
                }
            }
        }

class EvidencedRelation(BaseModel):
    """å¸¦è¯æ®çš„è£…é…å…³ç³»"""
    id: str
    source_part: str
    target_part: str
    relation_type: str
    evidence_chain: List[Evidence]  # æ”¯æ’‘æ­¤å…³ç³»çš„æ‰€æœ‰è¯æ®
    overall_confidence: float       # ç»¼åˆç½®ä¿¡åº¦

    def calculate_confidence(self) -> float:
        """è®¡ç®—ç»¼åˆç½®ä¿¡åº¦"""
        if not self.evidence_chain:
            return 0.0
        # åŠ æƒå¹³å‡ï¼Œå‡ ä½•è¯æ®æƒé‡æ›´é«˜
        weights = {
            EvidenceType.GEOMETRIC: 0.4,
            EvidenceType.DIMENSIONAL: 0.3,
            EvidenceType.TEXTUAL: 0.1,
            EvidenceType.RULE_BASED: 0.15,
            EvidenceType.LEARNED: 0.05
        }
        total = sum(e.confidence * weights.get(e.type, 0.1)
                   for e in self.evidence_chain)
        return min(total, 1.0)
```

#### 1.2 è¯æ®æ”¶é›†å™¨å®ç°
```python
# src/assembly/evidence_collector.py
import numpy as np
from typing import List, Dict, Any, Tuple
from src.models.evidence import Evidence, EvidenceType

class EvidenceCollector:
    """è£…é…è¯æ®æ”¶é›†å™¨"""

    def collect_evidence(
        self,
        part1: Dict,
        part2: Dict,
        geometry_data: Dict
    ) -> List[Evidence]:
        """æ”¶é›†ä¸¤ä¸ªé›¶ä»¶é—´çš„æ‰€æœ‰è¯æ®"""

        evidence_list = []

        # 1. å‡ ä½•è¯æ®
        geometric_evidence = self._collect_geometric_evidence(part1, part2, geometry_data)
        evidence_list.extend(geometric_evidence)

        # 2. å°ºå¯¸è¯æ®
        dimensional_evidence = self._collect_dimensional_evidence(part1, part2)
        evidence_list.extend(dimensional_evidence)

        # 3. æ–‡æœ¬è¯æ®
        textual_evidence = self._collect_textual_evidence(part1, part2)
        evidence_list.extend(textual_evidence)

        return evidence_list

    def _collect_geometric_evidence(
        self,
        part1: Dict,
        part2: Dict,
        geometry_data: Dict
    ) -> List[Evidence]:
        """æ”¶é›†å‡ ä½•è¯æ®"""

        evidence = []

        # æ£€æŸ¥æ¥è§¦é¢
        contact_faces = self._find_contact_faces(part1, part2, geometry_data)
        for face_pair in contact_faces:
            evidence.append(Evidence(
                type=EvidenceType.GEOMETRIC,
                source=f"face_{face_pair[0]}_{face_pair[1]}",
                confidence=self._calculate_contact_confidence(face_pair),
                description=f"æ£€æµ‹åˆ°æ¥è§¦é¢ {face_pair[0]} å’Œ {face_pair[1]}",
                raw_data={
                    "face1_id": face_pair[0],
                    "face2_id": face_pair[1],
                    "contact_type": self._identify_contact_type(face_pair)
                }
            ))

        # æ£€æŸ¥åŒè½´æ€§
        if self._check_coaxiality(part1, part2):
            evidence.append(Evidence(
                type=EvidenceType.GEOMETRIC,
                source="axis_alignment",
                confidence=0.9,
                description="æ£€æµ‹åˆ°åŒè½´å…³ç³»",
                raw_data={
                    "axis_deviation": self._calculate_axis_deviation(part1, part2)
                }
            ))

        return evidence

    def _collect_dimensional_evidence(
        self,
        part1: Dict,
        part2: Dict
    ) -> List[Evidence]:
        """æ”¶é›†å°ºå¯¸è¯æ®"""

        evidence = []

        # æ£€æŸ¥é…åˆå°ºå¯¸
        if "dimensions" in part1 and "dimensions" in part2:
            matching_dims = self._find_matching_dimensions(
                part1["dimensions"],
                part2["dimensions"]
            )

            for dim_match in matching_dims:
                evidence.append(Evidence(
                    type=EvidenceType.DIMENSIONAL,
                    source=f"dimension_match_{dim_match['id']}",
                    confidence=dim_match["confidence"],
                    description=f"å°ºå¯¸åŒ¹é…: {dim_match['value']}",
                    raw_data=dim_match
                ))

        return evidence

    def _collect_textual_evidence(
        self,
        part1: Dict,
        part2: Dict
    ) -> List[Evidence]:
        """æ”¶é›†æ–‡æœ¬è¯æ®"""

        evidence = []

        # ä»æ ‡ç­¾æå–
        label1 = part1.get("label", "")
        label2 = part2.get("label", "")

        # å…³é”®è¯åŒ¹é…
        keywords = {
            "gear": ["é½¿è½®", "gear", "Z="],
            "bearing": ["è½´æ‰¿", "bearing", "6201", "6202"],
            "shaft": ["è½´", "shaft", "axis"]
        }

        for key, words in keywords.items():
            if any(word in label1 for word in words) and \
               any(word in label2 for word in words):
                evidence.append(Evidence(
                    type=EvidenceType.TEXTUAL,
                    source="label_analysis",
                    confidence=0.7,
                    description=f"æ ‡ç­¾ä¸­æ£€æµ‹åˆ° {key} ç›¸å…³å…³é”®è¯",
                    raw_data={"keyword": key, "labels": [label1, label2]}
                ))

        return evidence

    def _calculate_contact_confidence(self, face_pair: Tuple) -> float:
        """è®¡ç®—æ¥è§¦ç½®ä¿¡åº¦"""
        # åŸºäºæ¥è§¦é¢ç§¯ã€æ³•å‘é‡ç­‰è®¡ç®—
        return 0.85  # ç®€åŒ–å®ç°

    def _identify_contact_type(self, face_pair: Tuple) -> str:
        """è¯†åˆ«æ¥è§¦ç±»å‹"""
        # å¹³é¢æ¥è§¦ã€åœ†æŸ±é¢æ¥è§¦ç­‰
        return "cylindrical"  # ç®€åŒ–å®ç°
```

---

### 2. è£…é…å›¾è§„èŒƒåŒ–ï¼ˆCanonicalizationï¼‰

#### 2.1 è§„èŒƒåŒ–å¤„ç†å™¨
```python
# src/assembly/graph_normalizer.py
import numpy as np
from typing import Dict, List, Any
import hashlib

class AssemblyGraphNormalizer:
    """è£…é…å›¾è§„èŒƒåŒ–å¤„ç†å™¨"""

    def __init__(self):
        self.unit_conversion = {
            "mm": 1.0,
            "cm": 10.0,
            "m": 1000.0,
            "inch": 25.4
        }

    def normalize(self, assembly_graph: Dict) -> Dict:
        """è§„èŒƒåŒ–è£…é…å›¾"""

        normalized = assembly_graph.copy()

        # 1. åæ ‡ç³»å¯¹é½
        normalized = self._align_coordinate_system(normalized)

        # 2. å•ä½ç»Ÿä¸€
        normalized = self._unify_units(normalized)

        # 3. å»é‡å¤„ç†
        normalized = self._remove_duplicates(normalized)

        # 4. ç‰¹å¾IDç¨³å®šåŒ–
        normalized = self._stabilize_feature_ids(normalized)

        # 5. è®¡ç®—è§„èŒƒå“ˆå¸Œ
        normalized["canonical_hash"] = self._compute_canonical_hash(normalized)

        return normalized

    def _align_coordinate_system(self, graph: Dict) -> Dict:
        """å¯¹é½åæ ‡ç³»åˆ°æ ‡å‡†æ–¹å‘"""

        # æ‰¾åˆ°ä¸»è½´ï¼ˆæœ€é•¿çš„è½´ç±»é›¶ä»¶ï¼‰
        main_axis = self._find_main_axis(graph)

        if main_axis:
            # è®¡ç®—æ—‹è½¬çŸ©é˜µï¼Œä½¿ä¸»è½´ä¸Xè½´å¯¹é½
            rotation_matrix = self._compute_alignment_matrix(main_axis)

            # åº”ç”¨å˜æ¢åˆ°æ‰€æœ‰é›¶ä»¶
            for part in graph.get("parts", []):
                if "center_of_mass" in part:
                    part["center_of_mass"] = self._transform_point(
                        part["center_of_mass"],
                        rotation_matrix
                    )
                if "bounding_box" in part:
                    part["bounding_box"] = self._transform_bbox(
                        part["bounding_box"],
                        rotation_matrix
                    )

        return graph

    def _unify_units(self, graph: Dict) -> Dict:
        """ç»Ÿä¸€åˆ°æ¯«ç±³å•ä½"""

        detected_unit = self._detect_unit(graph)
        conversion_factor = self.unit_conversion.get(detected_unit, 1.0)

        if conversion_factor != 1.0:
            for part in graph.get("parts", []):
                # è½¬æ¢æ‰€æœ‰å°ºå¯¸ç›¸å…³å­—æ®µ
                if "dimensions" in part:
                    part["dimensions"] = {
                        k: v * conversion_factor
                        for k, v in part["dimensions"].items()
                    }
                if "volume" in part:
                    part["volume"] *= (conversion_factor ** 3)

        graph["units"] = "mm"
        return graph

    def _remove_duplicates(self, graph: Dict) -> Dict:
        """å»é™¤é‡å¤çš„é¢å’Œè½´"""

        # å»é‡æ¥è§¦é¢
        unique_mates = []
        seen_pairs = set()

        for mate in graph.get("mates", []):
            # åˆ›å»ºæ ‡å‡†åŒ–çš„é…å¯¹IDï¼ˆé¡ºåºæ— å…³ï¼‰
            pair_id = tuple(sorted([mate["part1"], mate["part2"]]))

            if pair_id not in seen_pairs:
                seen_pairs.add(pair_id)
                unique_mates.append(mate)

        graph["mates"] = unique_mates
        return graph

    def _stabilize_feature_ids(self, graph: Dict) -> Dict:
        """ç¨³å®šåŒ–ç‰¹å¾ID"""

        # åŸºäºå‡ ä½•ç‰¹å¾ç”Ÿæˆç¨³å®šID
        for part in graph.get("parts", []):
            # åŸºäºå½¢çŠ¶ç‰¹å¾ç”Ÿæˆç¨³å®šID
            shape_hash = self._compute_shape_hash(part)
            part["stable_id"] = f"part_{shape_hash[:8]}"

        # æ›´æ–°å¼•ç”¨
        for mate in graph.get("mates", []):
            mate["part1"] = self._get_stable_id(graph, mate["part1"])
            mate["part2"] = self._get_stable_id(graph, mate["part2"])

        return graph

    def _compute_canonical_hash(self, graph: Dict) -> str:
        """è®¡ç®—è§„èŒƒå“ˆå¸Œå€¼"""

        # æå–å…³é”®ç‰¹å¾
        canonical_features = {
            "part_count": len(graph.get("parts", [])),
            "mate_count": len(graph.get("mates", [])),
            "part_types": sorted([p.get("type", "unknown")
                                for p in graph.get("parts", [])]),
            "mate_types": sorted([m.get("type", "unknown")
                                for m in graph.get("mates", [])])
        }

        # è®¡ç®—å“ˆå¸Œ
        feature_str = str(canonical_features)
        return hashlib.sha256(feature_str.encode()).hexdigest()[:16]

    def _detect_unit(self, graph: Dict) -> str:
        """æ£€æµ‹å½“å‰å•ä½"""

        # åŸºäºå°ºå¯¸èŒƒå›´å¯å‘å¼åˆ¤æ–­
        all_dims = []
        for part in graph.get("parts", []):
            if "bounding_box" in part:
                bbox = part["bounding_box"]
                size = max(bbox.get("max", [0])) - min(bbox.get("min", [0]))
                all_dims.append(size)

        if all_dims:
            avg_dim = np.mean(all_dims)
            if avg_dim < 10:  # å¯èƒ½æ˜¯ç±³
                return "m"
            elif avg_dim > 1000:  # å¯èƒ½æ˜¯å¾®ç±³
                return "um"
            else:  # é»˜è®¤æ¯«ç±³
                return "mm"

        return "mm"
```

---

### 3. ç‰ˆæœ¬åŒ–çŸ¥è¯†åº“

#### 3.1 çŸ¥è¯†åº“ç»“æ„
```yaml
# knowledge_base/assembly/rules/v1.0.0/gear_meshing.yaml
version: 1.0.0
name: gear_meshing_rules
description: é½¿è½®å•®åˆè§„åˆ™åº“
created: 2025-01-10
author: CAD-ML-Platform Team

rules:
  - id: spur_gear_mesh
    name: ç›´é½¿è½®å•®åˆ
    conditions:
      - type: both_parts_are_gears
      - type: parallel_axes
      - type: center_distance_matches_pitch_circles
    parameters:
      module:
        required: true
        unit: mm
      pressure_angle:
        default: 20
        unit: degree
      backlash:
        min: 0.05
        max: 0.3
        unit: mm
    evidence_requirements:
      - geometric: cylindrical_contact
      - dimensional: matching_module
    confidence_weight: 0.9

  - id: helical_gear_mesh
    name: æ–œé½¿è½®å•®åˆ
    conditions:
      - type: both_parts_are_gears
      - type: parallel_or_crossing_axes
      - type: helix_angle_matches
    parameters:
      helix_angle:
        required: true
        unit: degree
      hand:
        options: [left, right]
    evidence_requirements:
      - geometric: helical_contact_pattern
    confidence_weight: 0.85

  - id: bevel_gear_mesh
    name: é”¥é½¿è½®å•®åˆ
    conditions:
      - type: both_parts_are_gears
      - type: intersecting_axes
      - type: cone_angles_sum_to_shaft_angle
    parameters:
      shaft_angle:
        default: 90
        unit: degree
    confidence_weight: 0.8

mappings:
  # CADè½¯ä»¶mateç±»å‹åˆ°æ ‡å‡†å…³èŠ‚çš„æ˜ å°„
  solidworks:
    - mate_type: "Gear"
      maps_to: gear_mesh
      extract_params:
        - ratio: from_property("GearRatio")
        - module: from_dimension("Module")

  fusion360:
    - mate_type: "Motion:Rotation"
      with_conditions:
        - has_teeth_geometry
      maps_to: gear_mesh
      extract_params:
        - ratio: calculate_from_teeth_count

  creo:
    - constraint_type: "Gear Pair"
      maps_to: gear_mesh
      extract_params:
        - module: from_parameter("d10")
```

#### 3.2 çŸ¥è¯†åº“ç®¡ç†å™¨
```python
# src/assembly/knowledge_manager.py
import yaml
import semver
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç‰ˆæœ¬ç®¡ç†å™¨"""

    def __init__(self, kb_path: str = "knowledge_base/assembly"):
        self.kb_path = Path(kb_path)
        self.current_version = None
        self.loaded_rules = {}
        self.change_log = []

    def load_version(self, version: str = "latest") -> Dict:
        """åŠ è½½æŒ‡å®šç‰ˆæœ¬çš„çŸ¥è¯†åº“"""

        if version == "latest":
            version = self._get_latest_version()

        version_path = self.kb_path / "rules" / f"v{version}"

        if not version_path.exists():
            raise ValueError(f"Version {version} not found")

        # åŠ è½½æ‰€æœ‰è§„åˆ™æ–‡ä»¶
        rules = {}
        for rule_file in version_path.glob("*.yaml"):
            with open(rule_file, 'r', encoding='utf-8') as f:
                rule_data = yaml.safe_load(f)
                rules[rule_data["name"]] = rule_data

        self.current_version = version
        self.loaded_rules = rules

        # åŠ è½½å˜æ›´æ—¥å¿—
        self._load_changelog(version)

        return rules

    def _get_latest_version(self) -> str:
        """è·å–æœ€æ–°ç‰ˆæœ¬å·"""

        versions = []
        rules_path = self.kb_path / "rules"

        for version_dir in rules_path.iterdir():
            if version_dir.is_dir() and version_dir.name.startswith("v"):
                version_str = version_dir.name[1:]  # å»æ‰'v'å‰ç¼€
                try:
                    versions.append(semver.VersionInfo.parse(version_str))
                except:
                    continue

        if versions:
            latest = max(versions)
            return str(latest)

        return "1.0.0"

    def _load_changelog(self, version: str):
        """åŠ è½½å˜æ›´æ—¥å¿—"""

        changelog_file = self.kb_path / "CHANGELOG.md"
        if changelog_file.exists():
            with open(changelog_file, 'r', encoding='utf-8') as f:
                self.change_log = f.readlines()

    def get_rule(self, rule_name: str, rule_id: str) -> Optional[Dict]:
        """è·å–ç‰¹å®šè§„åˆ™"""

        if rule_name in self.loaded_rules:
            rules = self.loaded_rules[rule_name].get("rules", [])
            for rule in rules:
                if rule["id"] == rule_id:
                    return rule

        return None

    def get_mapping(self, cad_system: str, mate_type: str) -> Optional[Dict]:
        """è·å–CADç³»ç»Ÿæ˜ å°„"""

        for rule_set in self.loaded_rules.values():
            mappings = rule_set.get("mappings", {})
            if cad_system in mappings:
                for mapping in mappings[cad_system]:
                    if mapping["mate_type"] == mate_type:
                        return mapping

        return None

    def validate_rule_update(self, new_rules: Dict) -> Dict:
        """éªŒè¯è§„åˆ™æ›´æ–°"""

        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }

        # æ£€æŸ¥å‘åå…¼å®¹æ€§
        for rule_name, rule_data in new_rules.items():
            if rule_name in self.loaded_rules:
                old_version = self.loaded_rules[rule_name].get("version", "0.0.0")
                new_version = rule_data.get("version", "0.0.0")

                if semver.compare(new_version, old_version) <= 0:
                    validation_result["errors"].append(
                        f"Version must be higher than {old_version}"
                    )
                    validation_result["valid"] = False

        return validation_result
```

---

### 4. è¯„æµ‹åŸºçº¿ä¸CIé›†æˆ

#### 4.1 è¯„æµ‹æŒ‡æ ‡å®šä¹‰
```python
# src/evaluation/metrics.py
import numpy as np
from typing import List, Dict, Tuple
from sklearn.metrics import precision_recall_fscore_support

class AssemblyMetrics:
    """è£…é…åˆ†æè¯„æµ‹æŒ‡æ ‡"""

    def __init__(self):
        self.results = []

    def evaluate(self, predicted: Dict, ground_truth: Dict) -> Dict:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬"""

        metrics = {
            "graph_quality": self._evaluate_graph_quality(predicted, ground_truth),
            "physics_consistency": self._evaluate_physics(predicted, ground_truth),
            "evidence_quality": self._evaluate_evidence(predicted),
            "performance": self._evaluate_performance(predicted)
        }

        # è®¡ç®—æ€»åˆ†
        metrics["overall_score"] = self._calculate_overall_score(metrics)

        return metrics

    def _evaluate_graph_quality(self, pred: Dict, truth: Dict) -> Dict:
        """è¯„æµ‹è£…é…å›¾è´¨é‡"""

        # æå–è¾¹ç±»å‹
        pred_edges = [(e["part1"], e["part2"], e["type"])
                     for e in pred.get("mates", [])]
        truth_edges = [(e["part1"], e["part2"], e["type"])
                      for e in truth.get("mates", [])]

        # è®¡ç®—å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1
        true_positives = len(set(pred_edges) & set(truth_edges))
        false_positives = len(set(pred_edges) - set(truth_edges))
        false_negatives = len(set(truth_edges) - set(pred_edges))

        precision = true_positives / (true_positives + false_positives + 1e-10)
        recall = true_positives / (true_positives + false_negatives + 1e-10)
        f1 = 2 * precision * recall / (precision + recall + 1e-10)

        # å…³èŠ‚ç±»å‹å‡†ç¡®ç‡
        joint_accuracy = self._calculate_joint_type_accuracy(pred, truth)

        # è¿é€šæ€§æ£€æŸ¥
        connectivity_score = self._check_connectivity(pred, truth)

        return {
            "edge_precision": precision,
            "edge_recall": recall,
            "edge_f1": f1,
            "joint_type_accuracy": joint_accuracy,
            "connectivity_score": connectivity_score
        }

    def _evaluate_physics(self, pred: Dict, truth: Dict) -> Dict:
        """è¯„æµ‹ç‰©ç†ä¸€è‡´æ€§"""

        physics_metrics = {
            "simulation_ready": False,
            "dof_match": 0.0,
            "transmission_ratio_error": 0.0
        }

        # æ£€æŸ¥æ˜¯å¦å¯ä»¿çœŸ
        if "urdf" in pred or "simulation_ready" in pred:
            physics_metrics["simulation_ready"] = True

        # è‡ªç”±åº¦åŒ¹é…
        pred_dof = pred.get("degrees_of_freedom", -1)
        truth_dof = truth.get("degrees_of_freedom", -1)
        if pred_dof == truth_dof and pred_dof >= 0:
            physics_metrics["dof_match"] = 1.0

        # ä¼ åŠ¨æ¯”è¯¯å·®
        pred_ratio = self._extract_transmission_ratio(pred)
        truth_ratio = self._extract_transmission_ratio(truth)
        if pred_ratio and truth_ratio:
            error = abs(pred_ratio - truth_ratio) / truth_ratio
            physics_metrics["transmission_ratio_error"] = 1.0 - min(error, 1.0)

        return physics_metrics

    def _evaluate_evidence(self, pred: Dict) -> Dict:
        """è¯„æµ‹è¯æ®è´¨é‡"""

        evidence_metrics = {
            "evidence_coverage": 0.0,
            "average_confidence": 0.0,
            "evidence_diversity": 0.0
        }

        all_evidence = []
        for mate in pred.get("mates", []):
            if "evidence_chain" in mate:
                all_evidence.extend(mate["evidence_chain"])

        if all_evidence:
            # è¯æ®è¦†ç›–ç‡
            mates_with_evidence = sum(1 for m in pred.get("mates", [])
                                     if "evidence_chain" in m)
            total_mates = len(pred.get("mates", []))
            evidence_metrics["evidence_coverage"] = mates_with_evidence / (total_mates + 1e-10)

            # å¹³å‡ç½®ä¿¡åº¦
            confidences = [e.get("confidence", 0) for e in all_evidence]
            evidence_metrics["average_confidence"] = np.mean(confidences)

            # è¯æ®å¤šæ ·æ€§
            evidence_types = set(e.get("type", "") for e in all_evidence)
            evidence_metrics["evidence_diversity"] = len(evidence_types) / 5.0  # å‡è®¾5ç§ç±»å‹

        return evidence_metrics

    def _evaluate_performance(self, pred: Dict) -> Dict:
        """è¯„æµ‹æ€§èƒ½æŒ‡æ ‡"""

        return {
            "processing_time": pred.get("processing_time", -1),
            "cache_hit": pred.get("cache_hit", False),
            "cost": pred.get("cost", 0.0)
        }

    def _calculate_overall_score(self, metrics: Dict) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""

        weights = {
            "edge_f1": 0.3,
            "joint_type_accuracy": 0.2,
            "simulation_ready": 0.15,
            "evidence_coverage": 0.15,
            "average_confidence": 0.1,
            "connectivity_score": 0.1
        }

        score = 0.0
        for key, weight in weights.items():
            # é€’å½’æŸ¥æ‰¾æŒ‡æ ‡å€¼
            value = self._find_metric_value(metrics, key)
            if value is not None:
                score += value * weight

        return score

    def _find_metric_value(self, metrics: Dict, key: str) -> Optional[float]:
        """é€’å½’æŸ¥æ‰¾æŒ‡æ ‡å€¼"""

        for k, v in metrics.items():
            if k == key:
                return float(v) if isinstance(v, bool) else v
            elif isinstance(v, dict):
                result = self._find_metric_value(v, key)
                if result is not None:
                    return result

        return None
```

#### 4.2 CIé›†æˆé…ç½®
```yaml
# .github/workflows/assembly_tests.yml
name: Assembly AI Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.9

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov

    - name: Run unit tests
      run: |
        pytest tests/assembly/ -v --cov=src/assembly

    - name: Run evaluation baseline
      run: |
        python scripts/run_baseline_evaluation.py

    - name: Check metrics threshold
      run: |
        python scripts/check_metrics.py --min-f1 0.75 --min-confidence 0.7

    - name: Generate report
      run: |
        python scripts/generate_evaluation_report.py > evaluation_report.md

    - name: Upload artifacts
      uses: actions/upload-artifact@v2
      with:
        name: evaluation-report
        path: evaluation_report.md
```

#### 4.3 åŸºå‡†æµ‹è¯•é›†
```python
# tests/assembly/golden_cases.py
"""é»„é‡‘æµ‹è¯•ç”¨ä¾‹é›†"""

GOLDEN_CASES = [
    {
        "name": "simple_gear_train",
        "description": "ç®€å•é½¿è½®ç³»",
        "input": {
            "parts": [
                {"id": "gear1", "type": "gear", "teeth": 20},
                {"id": "gear2", "type": "gear", "teeth": 60},
                {"id": "shaft1", "type": "shaft"},
                {"id": "shaft2", "type": "shaft"}
            ],
            "mates": [
                {"part1": "gear1", "part2": "gear2", "type": "gear_mesh"},
                {"part1": "gear1", "part2": "shaft1", "type": "fixed"},
                {"part1": "gear2", "part2": "shaft2", "type": "fixed"}
            ]
        },
        "expected": {
            "function": "é½¿è½®å‡é€Ÿå™¨",
            "transmission_ratio": 3.0,
            "degrees_of_freedom": 1,
            "is_valid": True
        }
    },
    {
        "name": "belt_drive",
        "description": "çš®å¸¦ä¼ åŠ¨",
        "input": {
            "parts": [
                {"id": "pulley1", "type": "pulley", "diameter": 100},
                {"id": "pulley2", "type": "pulley", "diameter": 200},
                {"id": "belt", "type": "belt"}
            ],
            "mates": [
                {"part1": "pulley1", "part2": "belt", "type": "belt_contact"},
                {"part1": "pulley2", "part2": "belt", "type": "belt_contact"}
            ]
        },
        "expected": {
            "function": "çš®å¸¦ä¼ åŠ¨ç³»ç»Ÿ",
            "transmission_ratio": 2.0,
            "is_valid": True
        }
    },
    {
        "name": "bearing_support",
        "description": "è½´æ‰¿æ”¯æ’‘",
        "input": {
            "parts": [
                {"id": "shaft", "type": "shaft", "diameter": 30},
                {"id": "bearing1", "type": "bearing", "inner_diameter": 30},
                {"id": "bearing2", "type": "bearing", "inner_diameter": 30},
                {"id": "housing", "type": "housing"}
            ],
            "mates": [
                {"part1": "shaft", "part2": "bearing1", "type": "bearing_fit"},
                {"part1": "shaft", "part2": "bearing2", "type": "bearing_fit"},
                {"part1": "bearing1", "part2": "housing", "type": "fixed"},
                {"part1": "bearing2", "part2": "housing", "type": "fixed"}
            ]
        },
        "expected": {
            "function": "è½´æ‰¿æ”¯æ’‘ç³»ç»Ÿ",
            "is_valid": True,
            "constraints": ["proper_bearing_spacing", "adequate_support"]
        }
    }
]

def get_golden_case(name: str):
    """è·å–æŒ‡å®šçš„é»„é‡‘æµ‹è¯•ç”¨ä¾‹"""
    for case in GOLDEN_CASES:
        if case["name"] == name:
            return case
    return None

def run_golden_tests():
    """è¿è¡Œæ‰€æœ‰é»„é‡‘æµ‹è¯•"""
    from src.assembly.assembly_graph_builder import AssemblyGraphBuilder
    from src.evaluation.metrics import AssemblyMetrics

    builder = AssemblyGraphBuilder()
    evaluator = AssemblyMetrics()

    results = []
    for case in GOLDEN_CASES:
        print(f"Testing {case['name']}...")

        # æ„å»ºè£…é…å›¾
        predicted = builder.build_from_parsed_data(case["input"])

        # è¯„æµ‹
        metrics = evaluator.evaluate(predicted, case["expected"])

        results.append({
            "case": case["name"],
            "metrics": metrics,
            "passed": metrics["overall_score"] >= 0.75
        })

    return results
```

---

### 5. å¢å¼ºçš„APIå¥‘çº¦

```python
# src/api/v1/assembly_enhanced.py
from fastapi import APIRouter, UploadFile, File, Header
from typing import Optional, Dict
from src.models.evidence import EvidencedRelation
from pydantic import BaseModel
import hashlib

router = APIRouter(prefix="/assembly", tags=["assembly-enhanced"])

class EnhancedAnalysisRequest(BaseModel):
    """å¢å¼ºåˆ†æè¯·æ±‚"""
    idempotency_key: Optional[str] = None
    enable_evidence: bool = True
    enable_normalization: bool = True
    confidence_threshold: float = 0.7
    cache_mode: str = "auto"  # auto, force_refresh, cache_only

class EnhancedAnalysisResponse(BaseModel):
    """å¢å¼ºåˆ†æå“åº”"""
    request_id: str
    input_hash: str
    assembly_graph: Dict
    evidence: List[EvidencedRelation]
    uncertainty: Dict[str, float]
    canonical_hash: str
    suggestions: List[Dict]
    engine_support_matrix: Dict
    cache_hit: bool
    delta_report: Optional[Dict] = None

@router.post("/analyze", response_model=EnhancedAnalysisResponse)
async def analyze_with_evidence(
    file: UploadFile = File(...),
    request: EnhancedAnalysisRequest = None,
    idempotency_key: Optional[str] = Header(None)
):
    """å¸¦è¯æ®é“¾çš„è£…é…åˆ†æ"""

    # å¹‚ç­‰æ€§å¤„ç†
    if idempotency_key:
        cached = await check_idempotency(idempotency_key)
        if cached:
            return cached

    # è®¡ç®—è¾“å…¥å“ˆå¸Œ
    content = await file.read()
    input_hash = hashlib.sha256(content).hexdigest()

    # æ£€æŸ¥ç¼“å­˜
    if request.cache_mode != "force_refresh":
        cached_result = await get_from_cache(input_hash)
        if cached_result:
            cached_result["cache_hit"] = True

            # ç”ŸæˆDeltaæŠ¥å‘Š
            if request.cache_mode == "auto":
                cached_result["delta_report"] = await generate_delta(
                    cached_result,
                    input_hash
                )

            return cached_result

    # æ‰§è¡Œåˆ†æ...
    # (å®ç°ç»†èŠ‚çœç•¥)

    return response

@router.get("/engine-support-matrix")
async def get_engine_support():
    """è·å–å¼•æ“æ”¯æŒçŸ©é˜µ"""

    return {
        "urdf": {
            "supported_joints": ["fixed", "revolute", "prismatic", "continuous"],
            "unsupported": ["gear", "belt", "chain"],
            "workarounds": {
                "gear": "Use revolute with transmission ratio annotation",
                "belt": "Use coupled revolute joints"
            }
        },
        "pybullet": {
            "supported_joints": ["all_urdf", "custom_constraints"],
            "performance": "fast",
            "accuracy": "medium"
        },
        "chrono": {
            "supported_joints": ["all", "gear_pairs", "belt_drives"],
            "performance": "medium",
            "accuracy": "high"
        },
        "mujoco": {
            "supported_joints": ["all_urdf", "tendons", "actuators"],
            "performance": "very_fast",
            "accuracy": "high"
        }
    }
```

---

## ğŸ“Š å®æ–½è·¯çº¿å›¾ï¼ˆ1å‘¨å†…å¯å®Œæˆï¼‰

### Day 1-2: è¯æ®ç³»ç»Ÿ
- [ ] å®ç°Evidenceæ•°æ®æ¨¡å‹
- [ ] å¼€å‘EvidenceCollector
- [ ] é›†æˆåˆ°ç°æœ‰åˆ†ææµç¨‹

### Day 3: è§„èŒƒåŒ–å¤„ç†
- [ ] å®ç°AssemblyGraphNormalizer
- [ ] æ·»åŠ å•ä½è½¬æ¢å’Œåæ ‡å¯¹é½
- [ ] ç”Ÿæˆè§„èŒƒå“ˆå¸Œ

### Day 4: çŸ¥è¯†åº“ä¸æ˜ å°„
- [ ] åˆ›å»ºYAMLæ ¼å¼çŸ¥è¯†åº“
- [ ] å®ç°ç‰ˆæœ¬ç®¡ç†
- [ ] æ·»åŠ CADç³»ç»Ÿæ˜ å°„è¡¨

### Day 5: è¯„æµ‹åŸºçº¿
- [ ] å®šä¹‰6ä¸ªé»„é‡‘æµ‹è¯•ç”¨ä¾‹
- [ ] å®ç°è¯„æµ‹æŒ‡æ ‡è®¡ç®—
- [ ] å»ºç«‹F1â‰¥0.75åŸºçº¿

### Day 6: APIå¢å¼º
- [ ] æ·»åŠ è¯æ®è¾“å‡º
- [ ] å®ç°å¹‚ç­‰æ€§
- [ ] ç¼“å­˜ä¸DeltaæŠ¥å‘Š

### Day 7: CIé›†æˆ
- [ ] é…ç½®GitHub Actions
- [ ] è‡ªåŠ¨è¿è¡Œè¯„æµ‹
- [ ] ç”ŸæˆæŠ¥å‘Š

---

## ğŸ¯ é¢„æœŸæˆæœ

é€šè¿‡è¿™äº›å¢å¼ºï¼Œç³»ç»Ÿå°†è¾¾åˆ°ï¼š

1. **å¯è§£é‡Šæ€§**ï¼šæ¯ä¸ªè£…é…å…³ç³»éƒ½æœ‰å®Œæ•´è¯æ®é“¾
2. **ç¨³å®šæ€§**ï¼šè§„èŒƒåŒ–ç¡®ä¿ç»“æœä¸€è‡´æ€§
3. **å¯ç»´æŠ¤æ€§**ï¼šç‰ˆæœ¬åŒ–çŸ¥è¯†åº“ä¾¿äºè¿­ä»£
4. **å¯æµ‹é‡æ€§**ï¼šé‡åŒ–æŒ‡æ ‡é©±åŠ¨æ”¹è¿›
5. **ç”Ÿäº§å°±ç»ª**ï¼šå¹‚ç­‰æ€§ã€ç¼“å­˜ã€DeltaæŠ¥å‘Š

---

**æ‚¨çš„å»ºè®®éå¸¸ä¸“ä¸šï¼Œè¿™ä¸ªå¢å¼ºç‰ˆæ–¹æ¡ˆå°†è®©è£…é…ç†è§£AIçœŸæ­£è¾¾åˆ°ç”Ÿäº§çº§åˆ«ï¼**
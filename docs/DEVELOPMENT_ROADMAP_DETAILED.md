# CAD ML Platform - è¯¦ç»†å¼€å‘è·¯çº¿å›¾ (6å¤©Sprint)

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025-11-24  
> **Sprintå‘¨æœŸ**: Day 1 AM (å·²å®Œæˆ) â†’ Day 6 (ç¼“å†²éªŒè¯)

---

## ğŸ“Š æ€»ä½“è¿›åº¦æ¦‚è§ˆ

| é˜¶æ®µ | å¤©æ•° | çŠ¶æ€ | å®Œæˆåº¦ |
|------|------|------|--------|
| **Phase A** - ç¨³å®šä¸è¡¥æµ‹ | Day 1 | âœ… 50% (AMå®Œæˆ) | 5/10 tasks |
| **Phase B** - å¯è§‚æµ‹æ€§ä¸è°ƒä¼˜ | Day 2 | ğŸ”„ å¾…å¼€å§‹ | 0/8 tasks |
| **Phase C** - å®‰å…¨ä¸æ¨¡å‹å¯ä¿¡ | Day 3 | â³ å¾…å¼€å§‹ | 0/6 tasks |
| **Phase D** - v4çœŸå®ç‰¹å¾ | Day 4 | â³ å¾…å¼€å§‹ | 0/5 tasks |
| **Phase E** - æ–‡æ¡£ä¸è§„åˆ™ | Day 5 | â³ å¾…å¼€å§‹ | 0/7 tasks |
| **Phase F** - ç¼“å†²ä¸å›å½’ | Day 6 | â³ å¾…å¼€å§‹ | 0/4 tasks |

---

## âœ… Day 1 AM - å®Œæˆæƒ…å†µéªŒè¯

### å·²å®Œæˆä»»åŠ¡ (100%)

#### âœ… Task 1.1: Rediså®•æœºå­¤å„¿æ¸…ç†æµ‹è¯•
**æ–‡ä»¶**: `/tests/unit/test_orphan_cleanup_redis_down.py` (236è¡Œ)

**æµ‹è¯•è¦†ç›–**:
- âœ… å®Œå…¨è¿æ¥å¤±è´¥åœºæ™¯
- âœ… è¶…æ—¶åœºæ™¯å¤„ç†
- âœ… è¿è¡Œä¸­éƒ¨åˆ†å¤±è´¥
- âœ… é”™è¯¯å“åº”ç»“æ„éªŒè¯
- âœ… é”™è¯¯æ¶ˆæ¯ä¸­çš„å»ºè®®ä¿¡æ¯
- âœ… å¤±è´¥æ—¶çš„æŒ‡æ ‡è¡Œä¸º
- âœ… Redisæ¢å¤åçš„æµ‹è¯•

**éªŒæ”¶**: âœ… 7ä¸ªæµ‹è¯•å…¨éƒ¨é€šè¿‡

---

#### âœ… Task 1.2: Faissæ‰¹é‡ç›¸ä¼¼åº¦é™çº§æµ‹è¯•
**æ–‡ä»¶**: `/tests/unit/test_faiss_degraded_batch.py` (396è¡Œ)

**æµ‹è¯•è¦†ç›–**:
- âœ… Faissä¸å¯ç”¨(å¯¼å…¥å¤±è´¥)
- âœ… åˆå§‹åŒ–å¤±è´¥
- âœ… æŸ¥è¯¢å¼‚å¸¸
- âœ… éƒ¨åˆ†/é—´æ­‡æ€§å¤±è´¥
- âœ… FallbackæŒ‡æ ‡è®°å½•
- âœ… å“åº”ç»“æ„éªŒè¯
- âœ… é™çº§æ—¶è¿‡æ»¤å™¨å¤„ç†
- âœ… æ€§èƒ½é€€åŒ–æ£€æŸ¥(<10%)
- âœ… ç³»ç»Ÿæ¢å¤

**ä»£ç å˜æ›´**:
- âœ… `src/api/v1/vectors.py`: æ·»åŠ `fallback`å­—æ®µåˆ°`BatchSimilarityResponse`
- âœ… æ·»åŠ fallbackæ£€æµ‹é€»è¾‘
- âœ… æ·»åŠ `vector_query_backend_total{backend="memory_fallback"}`æŒ‡æ ‡

**éªŒæ”¶**: âœ… 9ä¸ªæµ‹è¯•å…¨éƒ¨é€šè¿‡

---

#### âœ… Task 1.3: ç»´æŠ¤ç«¯ç‚¹é”™è¯¯ç»“æ„åŒ–
**æ–‡ä»¶**: `/src/api/v1/maintenance.py` (å·²æ›´æ–°)

**æ”¹è¿›**:
- âœ… ç»Ÿä¸€ä½¿ç”¨`build_error`ç»“æ„åŒ–é”™è¯¯
- âœ… Redisè¿æ¥å¤±è´¥å¤„ç† + é”™è¯¯ç 
- âœ… éƒ¨åˆ†å¤±è´¥æ£€æµ‹(10ä¸ªRedisé”™è¯¯åä¸­æ­¢)
- âœ… æ·»åŠ `vector_orphan_total`æŒ‡æ ‡è·Ÿè¸ª
- âœ… æ”¹è¿›é”™è¯¯ä¸Šä¸‹æ–‡å’Œå»ºè®®

**æ›´æ–°ç«¯ç‚¹**:
- âœ… `/maintenance/orphans` (DELETE)
- âœ… `/maintenance/cache/clear` (POST)
- âœ… `/maintenance/stats` (GET)
- âœ… `/maintenance/vectors/backend/reload` (POST)

**éªŒæ”¶**: âœ… ç»“æ„åŒ–é”™è¯¯æ ¼å¼ + 100%æµ‹è¯•è¦†ç›–

---

### Day 1 AM å…³é”®æˆæœ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ–°å¢æµ‹è¯• | 16ä¸ª (7 Redis + 9 Faiss) |
| æ–°å»ºæ–‡ä»¶ | 2ä¸ªæµ‹è¯•æ–‡ä»¶ |
| ä¿®æ”¹æ–‡ä»¶ | 2ä¸ª (vectors.py, maintenance.py) |
| ä»£ç è¦†ç›–ç‡ | 100% (æ–°å¢åˆ†æ”¯) |
| é”™è¯¯å¤„ç† | å®Œå…¨ç»“æ„åŒ– |

---

## ğŸ”„ Day 1 PM - å¾…æ‰§è¡Œä»»åŠ¡

### Task 1.4: æ¨¡å‹å›æ»šå¥åº·æµ‹è¯•

**ç›®æ ‡**: ç¡®ä¿æ¨¡å‹å®‰å…¨å¤±è´¥åå¥åº·ç«¯ç‚¹æ­£ç¡®æŠ¥å‘Šå›æ»šçŠ¶æ€

**å®æ–½æ­¥éª¤**:

1. **åˆ›å»ºæµ‹è¯•æ–‡ä»¶**: `/tests/unit/test_model_rollback_health.py`

```python
# æµ‹è¯•åœºæ™¯
- test_model_reload_security_fail_health_shows_rollback_level
- test_model_reload_success_health_shows_loaded
- test_model_rollback_to_prev2_health_reflects_level
- test_model_health_after_double_failure
- test_model_health_last_error_tracking
- test_model_health_absent_model_initial_state
```

2. **ä¿®æ”¹**: `/src/api/v1/health.py`
   - æ‰©å±•`/health/model`ç«¯ç‚¹è¿”å›å­—æ®µ
   - æ·»åŠ  `rollback_level: int | None`
   - æ·»åŠ  `last_error: Dict[str, Any] | None`
   - æ·»åŠ  `security_checks_passed: bool`

3. **ä¿®æ”¹**: `/src/ml/classifier.py`
   - æ·»åŠ å…¨å±€å˜é‡è·Ÿè¸ªå›æ»šå±‚çº§
   - æ›´æ–°`reload_model`è®°å½•å¤±è´¥åŸå› 

**éªŒæ”¶æ ‡å‡†**:
- âœ… 6ä¸ªæ–°æµ‹è¯•å…¨éƒ¨é€šè¿‡
- âœ… `/health/model`è¿”å›æ‰©å±•å­—æ®µ
- âœ… `model_health_checks_total{status}`æŒ‡æ ‡åŒ…å«3ç§çŠ¶æ€: ok/absent/error

**é¢„ä¼°å·¥æ—¶**: 3å°æ—¶

**ä¾èµ–**: éœ€è¦ç†è§£`classifier.py`çš„å›æ»šæœºåˆ¶

---

### Task 1.5: åç«¯é‡è½½å¤±è´¥æµ‹è¯•

**ç›®æ ‡**: æµ‹è¯•å‘é‡å­˜å‚¨åç«¯é‡è½½å„ç§å¤±è´¥åœºæ™¯

**å®æ–½æ­¥éª¤**:

1. **åˆ›å»ºæµ‹è¯•æ–‡ä»¶**: `/tests/unit/test_backend_reload_failures.py`

```python
# æµ‹è¯•åœºæ™¯
- test_reload_invalid_backend_env_var
- test_reload_faiss_import_failure
- test_reload_memory_to_faiss_dimension_mismatch
- test_reload_no_api_key_returns_401
- test_reload_metric_tracking_on_error
- test_reload_error_response_structure
```

2. **ä¿®æ”¹**: `/src/api/v1/maintenance.py`
   - ç°æœ‰endpoint: `/maintenance/vectors/backend/reload`
   - å¢å¼ºé”™è¯¯æ£€æµ‹: æ— æ•ˆbackendç¯å¢ƒå˜é‡
   - æ·»åŠ ç»´åº¦ä¸åŒ¹é…æ£€æµ‹

3. **æ–°å¢æŒ‡æ ‡**: 
   - `vector_store_reload_total{status="invalid_backend"}`
   - `vector_store_reload_total{status="dimension_mismatch"}`

**éªŒæ”¶æ ‡å‡†**:
- âœ… 6-8ä¸ªæµ‹è¯•è¦†ç›–æ‰€æœ‰å¤±è´¥è·¯å¾„
- âœ… ç»“æ„åŒ–é”™è¯¯å“åº”åŒ…å«`ErrorCode`å’Œ`stage`
- âœ… `vector_store_reload_total`æŒ‡æ ‡è‡³å°‘3ç§çŠ¶æ€

**é¢„ä¼°å·¥æ—¶**: 2.5å°æ—¶

**é£é™©**: éœ€è¦mock FAISSå¯¼å…¥å¤±è´¥ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æµ‹è¯•ç¯å¢ƒ

---

## ğŸ“ˆ Day 2 AM - è‡ªé€‚åº”ç¼“å­˜è°ƒä¼˜

### Task 2.1: ç¼“å­˜è°ƒä¼˜ç«¯ç‚¹å®ç°

**ç›®æ ‡**: æä¾›åŸºäºå†å²å‘½ä¸­ç‡çš„ç¼“å­˜å®¹é‡å’ŒTTLæ¨è

**å®æ–½æ­¥éª¤**:

1. **æ–°å¢APIç«¯ç‚¹**: `/api/v1/features/cache/tuning`

```python
# Request Model
class CacheTuningRequest(BaseModel):
    recent_hit_rate: float  # æœ€è¿‘æ»‘åŠ¨çª—å£å‘½ä¸­ç‡ [0,1]
    current_capacity: int   # å½“å‰ç¼“å­˜å®¹é‡
    current_ttl_seconds: int  # å½“å‰TTL
    request_rate_per_hour: int  # è¯·æ±‚é€Ÿç‡(å¯é€‰)

# Response Model
class CacheTuningResponse(BaseModel):
    recommended_capacity: int
    recommended_ttl_seconds: int
    rationale: str  # æ¨èç†ç”±
    expected_hit_rate_improvement: float | None
    calculation_snapshot: Dict[str, Any]
```

2. **è°ƒä¼˜é€»è¾‘** (`src/core/feature_cache.py`):

```python
def recommend_tuning(hit_rate, capacity, ttl, request_rate=None):
    if hit_rate < 0.4:
        # å‘½ä¸­ç‡è¿‡ä½ï¼Œå¢åŠ å®¹é‡
        return {
            "capacity": int(capacity * 1.5),
            "ttl": ttl,
            "rationale": "Low hit rate suggests cache size insufficient"
        }
    elif 0.4 <= hit_rate < 0.7:
        # ä¸­ç­‰å‘½ä¸­ç‡ï¼Œè°ƒæ•´TTL
        return {
            "capacity": capacity,
            "ttl": int(ttl * 1.2),
            "rationale": "Moderate hit rate, increasing TTL to improve retention"
        }
    elif hit_rate >= 0.85:
        # å‘½ä¸­ç‡è¿‡é«˜å¯èƒ½æµªè´¹ï¼Œå¯é™ä½å®¹é‡
        return {
            "capacity": max(int(capacity * 0.8), 100),  # æœ€ä½100
            "ttl": ttl,
            "rationale": "High hit rate allows capacity reduction to save memory"
        }
    else:
        # è‰¯å¥½èŒƒå›´ï¼Œä¿æŒä¸å˜
        return {
            "capacity": capacity,
            "ttl": ttl,
            "rationale": "Hit rate in optimal range"
        }
```

3. **æ–°å¢æŒ‡æ ‡**:
   - `feature_cache_tuning_requests_total{status="success|error"}`
   - `feature_cache_tuning_recommendation_applied_total`

4. **å•å…ƒæµ‹è¯•**: `/tests/unit/test_cache_tuning.py`

```python
# æµ‹è¯•åœºæ™¯
- test_tuning_low_hit_rate_increases_capacity
- test_tuning_moderate_hit_rate_increases_ttl
- test_tuning_high_hit_rate_decreases_capacity
- test_tuning_optimal_range_no_change
- test_tuning_with_request_rate_consideration
- test_tuning_response_structure
- test_tuning_invalid_input_returns_422
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… APIç«¯ç‚¹è¿”å›æ­£ç¡®çš„æ¨è
- âœ… å‘½ä¸­ç‡åŒºé—´æµ‹è¯•è¦†ç›–: <0.4, 0.4-0.7, 0.7-0.85, >0.85
- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥90%
- âœ… æŒ‡æ ‡æ­£ç¡®é€’å¢

**é¢„ä¼°å·¥æ—¶**: 4å°æ—¶

**ä¾èµ–**: éœ€è¦è®¿é—®`feature_cache.py`äº†è§£ç°æœ‰å®ç°

---

### Task 2.2: è¿ç§»ç»´åº¦å·®å¼‚ç›´æ–¹å›¾

**ç›®æ ‡**: è·Ÿè¸ªå‘é‡è¿ç§»å‰åçš„ç»´åº¦å˜åŒ–åˆ†å¸ƒ

**å®æ–½æ­¥éª¤**:

1. **æ–°å¢æŒ‡æ ‡** (`src/utils/analysis_metrics.py`):

```python
from prometheus_client import Histogram

vector_migrate_dimension_delta = Histogram(
    'vector_migrate_dimension_delta',
    'Distribution of dimension changes during migration',
    buckets=[-20, -10, -5, -2, 0, 2, 5, 10, 20, 50]
)
```

2. **ä¿®æ”¹**: `/src/api/v1/vectors.py` - `migrate_vectors`å‡½æ•°
   - åœ¨è¿ç§»åè®°å½•ç»´åº¦å·®å¼‚: `dimension_after - dimension_before`
   - è°ƒç”¨ `vector_migrate_dimension_delta.observe(delta)`

3. **å•å…ƒæµ‹è¯•**: æ‰©å±• `/tests/unit/test_vector_migrate.py`

```python
- test_migrate_dimension_delta_metric_recorded
- test_migrate_v1_to_v3_positive_delta
- test_migrate_v3_to_v2_negative_delta
- test_migrate_same_version_zero_delta
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… æŒ‡æ ‡åœ¨`/metrics`ç«¯ç‚¹å¯è§
- âœ… æ­£å‘è¿ç§»(å¦‚v1â†’v3)è®°å½•æ­£delta
- âœ… é™çº§(å¦‚v3â†’v1)è®°å½•è´Ÿdelta
- âœ… åŒç‰ˆæœ¬è¿ç§»delta=0

**é¢„ä¼°å·¥æ—¶**: 1.5å°æ—¶

---

## ğŸ“Š Day 2 PM - Grafana Dashboardä¸Prometheusè§„åˆ™

### Task 2.3: Grafana Dashboardæ›´æ–°

**ç›®æ ‡**: æ·»åŠ æ–°å¢æŒ‡æ ‡çš„å¯è§†åŒ–é¢æ¿

**å®æ–½æ­¥éª¤**:

1. **æ›´æ–°æ–‡ä»¶**: `/config/grafana/dashboard_cad_analysis_metrics.json`

**æ–°å¢é¢æ¿**:

| é¢æ¿åç§° | PromQLæŸ¥è¯¢ | ç±»å‹ |
|---------|-----------|------|
| è¿ç§»çŠ¶æ€æ±‡æ€» | `sum by (status)(rate(vector_migrate_total[5m]))` | é¥¼å›¾ |
| ç»´åº¦å·®å¼‚åˆ†å¸ƒ | `histogram_quantile(0.5, rate(vector_migrate_dimension_delta_bucket[10m]))` | ç›´æ–¹å›¾ |
| ç¼“å­˜è°ƒä¼˜è¯·æ±‚ | `rate(feature_cache_tuning_requests_total[5m])` | æ—¶åºå›¾ |
| v4é‡‡çº³ç‡ | `sum(vector_migrate_total{to_version="v4"}) / sum(vector_migrate_total)` | ä»ªè¡¨ç›˜ |
| æ‰¹é‡ç›¸ä¼¼åº¦P95/P99 | `histogram_quantile(0.95, rate(vector_query_batch_latency_seconds_bucket[5m]))` | æ—¶åºå›¾ |
| Driftåˆ·æ–°è§¦å‘é¥¼å›¾ | `sum by (trigger)(drift_baseline_refresh_total)` | é¥¼å›¾ |

2. **Dashboardç»“æ„**:

```json
{
  "dashboard": {
    "title": "CAD Analysis Metrics (Enhanced)",
    "panels": [
      {
        "title": "Vector Migration Status",
        "type": "piechart",
        "targets": [...]
      },
      {
        "title": "Migration Dimension Delta Distribution",
        "type": "histogram",
        "targets": [...]
      },
      // ... å…¶ä»–é¢æ¿
    ]
  }
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… Dashboard JSONæ ¼å¼æ­£ç¡® (`promtool check dashboard`)
- âœ… æ‰€æœ‰PromQLæŸ¥è¯¢æœ‰æ•ˆ
- âœ… è‡³å°‘6ä¸ªæ–°é¢æ¿

**é¢„ä¼°å·¥æ—¶**: 3å°æ—¶

---

### Task 2.4: Prometheuså½•åˆ¶è§„åˆ™ä¸å‘Šè­¦

**ç›®æ ‡**: å®šä¹‰æ€§èƒ½å’Œå®‰å…¨å‘Šè­¦è§„åˆ™

**å®æ–½æ­¥éª¤**:

1. **åˆ›å»ºæ–‡ä»¶**: `/config/prometheus/alert_rules_enhanced.yml`

```yaml
groups:
  - name: cad_ml_platform_alerts
    interval: 30s
    rules:
      # v4ç‰¹å¾å»¶è¿Ÿå¼‚å¸¸
      - alert: V4FeatureExtractionSlow
        expr: |
          histogram_quantile(0.95, 
            rate(feature_extraction_latency_seconds_bucket{version="v4"}[5m])
          ) > 
          histogram_quantile(0.95, 
            rate(feature_extraction_latency_seconds_bucket{version="v3"}[5m])
          ) * 1.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "v4 feature extraction 95th percentile >50% slower than v3"
          description: "v4: {{ $value }}s, check implementation efficiency"

      # Opcodeé˜»æ–­å‘Šè­¦
      - alert: ModelOpcodeBlocked
        expr: increase(model_security_fail_total{reason="opcode_blocked"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Model pickle opcode blocked - potential security threat"
          description: "{{ $value }} blocked attempts in 5 minutes"

      # ç¼“å­˜å‘½ä¸­ç‡ä½
      - alert: FeatureCacheHitRateLow
        expr: |
          rate(feature_cache_hits_total[1h]) / 
          (rate(feature_cache_hits_total[1h]) + rate(feature_cache_misses_total[1h])) 
          < 0.35
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Feature cache hit rate below 35% for 15 minutes"
          description: "Current rate: {{ $value | humanizePercentage }}"
```

2. **å½•åˆ¶è§„åˆ™**: `/config/prometheus/recording_rules_enhanced.yml`

```yaml
groups:
  - name: cad_ml_aggregations
    interval: 1m
    rules:
      # ç¼“å­˜å‘½ä¸­ç‡é¢„è®¡ç®—
      - record: feature_cache:hit_rate:1h
        expr: |
          rate(feature_cache_hits_total[1h]) / 
          (rate(feature_cache_hits_total[1h]) + rate(feature_cache_misses_total[1h]))

      # æŒ‰ç‰ˆæœ¬åˆ†ç»„çš„æå–å»¶è¿ŸP95
      - record: feature_extraction:latency:p95_by_version
        expr: |
          histogram_quantile(0.95, 
            sum by (le, version)(rate(feature_extraction_latency_seconds_bucket[5m]))
          )
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… `promtool check rules alert_rules_enhanced.yml` é€šè¿‡
- âœ… `promtool check rules recording_rules_enhanced.yml` é€šè¿‡
- âœ… è‡³å°‘3ä¸ªå‘Šè­¦è§„åˆ™ + 2ä¸ªå½•åˆ¶è§„åˆ™

**é¢„ä¼°å·¥æ—¶**: 2.5å°æ—¶

---

## ğŸ”’ Day 3 AM - å®‰å…¨å¢å¼º

### Task 3.1: Pickle Opcodeç™½åå•æ¨¡å¼

**ç›®æ ‡**: æ”¯æŒç™½åå•/é»‘åå•åŒæ¨¡å¼çš„opcodeå®‰å…¨æ£€æŸ¥

**å®æ–½æ­¥éª¤**:

1. **æ–°å¢ç¯å¢ƒå˜é‡**:
   - `MODEL_OPCODE_MODE=whitelist|blocklist|permissive` (é»˜è®¤blocklist)
   - `MODEL_OPCODE_WHITELIST=GLOBAL,BUILD,REDUCE` (å¯é…ç½®)

2. **ä¿®æ”¹**: `/src/ml/classifier.py`

```python
# æ·»åŠ ç™½åå•æ£€æŸ¥å‡½æ•°
def _check_opcode_whitelist(pickled_data: bytes) -> Dict[str, Any]:
    import pickletools
    mode = os.getenv("MODEL_OPCODE_MODE", "blocklist")
    
    if mode == "permissive":
        return {"passed": True, "mode": "permissive"}
    
    allowed = set(os.getenv("MODEL_OPCODE_WHITELIST", "PROTO,FRAME,GLOBAL,BUILD,REDUCE").split(","))
    found_opcodes = set()
    
    for opcode, arg, pos in pickletools.genops(pickled_data):
        found_opcodes.add(opcode.name)
    
    if mode == "whitelist":
        forbidden = found_opcodes - allowed
        if forbidden:
            return {
                "passed": False,
                "mode": "whitelist",
                "forbidden_opcodes": list(forbidden)
            }
    
    return {"passed": True, "mode": mode, "checked_opcodes": len(found_opcodes)}
```

3. **æ–°å¢æŒ‡æ ‡**:

```python
model_opcode_mode = Gauge(
    'model_opcode_mode',
    'Current opcode checking mode (0=permissive, 1=blocklist, 2=whitelist)',
    labelnames=[]
)
```

4. **å•å…ƒæµ‹è¯•**: `/tests/unit/test_model_whitelist_mode.py`

```python
# æµ‹è¯•åœºæ™¯
- test_whitelist_mode_allows_only_whitelisted_opcodes
- test_blocklist_mode_blocks_dangerous_opcodes
- test_permissive_mode_allows_all
- test_mode_switch_via_env_var
- test_whitelist_rejection_metric_increment
- test_opcode_mode_gauge_reflects_current_mode
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… 3ç§æ¨¡å¼(whitelist/blocklist/permissive)æµ‹è¯•è¦†ç›–
- âœ… æ¨¡å¼åˆ‡æ¢ä¸éœ€è¦é‡å¯æœåŠ¡
- âœ… `model_opcode_mode` Gaugeæ­£ç¡®åæ˜ å½“å‰æ¨¡å¼
- âœ… é”™è¯¯å“åº”åŒ…å«è¢«é˜»æ–­çš„opcodeåˆ—è¡¨

**é¢„ä¼°å·¥æ—¶**: 4å°æ—¶

---

### Task 3.2: å®‰å…¨æ–‡æ¡£ä¸å¿«é€Ÿæ’é”™

**ç›®æ ‡**: ä¸ºè¿ç»´äººå‘˜æä¾›å®‰å…¨æ£€æŸ¥å¤±è´¥çš„æ’é”™æŒ‡å—

**å®æ–½æ­¥éª¤**:

1. **åˆ›å»ºæ–‡æ¡£**: `/docs/SECURITY_MODEL_RELOAD.md`

**å†…å®¹ç»“æ„**:
```markdown
# æ¨¡å‹å®‰å…¨é‡è½½æŒ‡å—

## å®‰å…¨æ£€æŸ¥æµç¨‹å›¾

[Mermaidæµç¨‹å›¾]

## å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### 1. Hash Mismatch (hash_mismatch)
**é”™è¯¯æ¶ˆæ¯**: Model hash not in whitelist
**åŸå› **: æ¨¡å‹æ–‡ä»¶å“ˆå¸Œä¸åœ¨ALLOWED_MODEL_HASHESä¸­
**è§£å†³**: 
  1. è®¡ç®—æ¨¡å‹SHA-256: `sha256sum models/classifier.pkl`
  2. æ·»åŠ åˆ°ç¯å¢ƒå˜é‡: `ALLOWED_MODEL_HASHES=abc123,def456`
  
### 2. Opcode Blocked (opcode_blocked)
**é”™è¯¯æ¶ˆæ¯**: Forbidden opcode detected: GLOBAL
**åŸå› **: æ¨¡å‹pickleåŒ…å«å±é™©æ“ä½œç 
**è§£å†³**:
  1. æ£€æŸ¥æ¨¡å‹æ¥æºæ˜¯å¦å¯ä¿¡
  2. ä½¿ç”¨ `pickletools.dis()` åˆ†æopcode
  3. è€ƒè™‘åˆ‡æ¢åˆ°permissiveæ¨¡å¼(ä»…å¼€å‘ç¯å¢ƒ)

### 3. Magic Number Invalid (magic_number_invalid)
**é”™è¯¯æ¶ˆæ¯**: Invalid pickle magic number
**åŸå› **: æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„pickleæ ¼å¼
**è§£å†³**:
  1. éªŒè¯æ–‡ä»¶æ ¼å¼: `file models/classifier.pkl`
  2. é‡æ–°å¯¼å‡ºæ¨¡å‹ç¡®ä¿æ ¼å¼æ­£ç¡®
```

2. **æ›´æ–°README.md** - æ·»åŠ å®‰å…¨ç« èŠ‚

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ–‡æ¡£åŒ…å«æµç¨‹å›¾
- âœ… è‡³å°‘5ä¸ªå¸¸è§é”™è¯¯åœºæ™¯ + è§£å†³æ–¹æ¡ˆ
- âœ… READMEå®‰å…¨ç« èŠ‚é“¾æ¥åˆ°è¯¦ç»†æ–‡æ¡£
- âœ… Markdown linteræ— é”™è¯¯

**é¢„ä¼°å·¥æ—¶**: 2å°æ—¶

---

## ğŸ”„ Day 3 PM - æ¨¡å‹å¯ä¿¡æ€§å¼ºåŒ–

### Task 3.3: æ¥å£æ ¡éªŒæ‰©å±•

**ç›®æ ‡**: å¢å¼ºæ¨¡å‹æ¥å£éªŒè¯ï¼Œé˜²æ­¢æ¶æ„æ¨¡å‹æš´éœ²å±é™©å±æ€§

**å®æ–½æ­¥éª¤**:

1. **ä¿®æ”¹**: `/src/ml/classifier.py` - `reload_model`å‡½æ•°

```python
def _validate_model_interface(model: Any) -> Dict[str, Any]:
    """æ‰©å±•æ¥å£éªŒè¯"""
    issues = []
    
    # å¿…é¡»æœ‰predictæ–¹æ³•
    if not hasattr(model, 'predict'):
        issues.append("missing_predict_method")
    
    # æ£€æŸ¥æ˜¯å¦æš´éœ²å±é™©å±æ€§
    dangerous_attrs = ['__reduce__', '__reduce_ex__', '__setstate__']
    for attr in dangerous_attrs:
        if hasattr(model, attr) and callable(getattr(model, attr)):
            # å…è®¸ä½†è®°å½•è­¦å‘Š
            logger.warning(f"Model has callable {attr} - potential security risk")
    
    # æ£€æŸ¥å±æ€§å›¾å¤§å°(é˜²æ­¢å·¨å¤§åµŒå¥—å¯¹è±¡)
    try:
        import sys
        size = sys.getsizeof(model)
        if size > 100 * 1024 * 1024:  # >100MB
            issues.append("model_too_large_in_memory")
    except Exception:
        pass
    
    return {
        "passed": len(issues) == 0,
        "issues": issues
    }
```

2. **æ–°å¢æŒ‡æ ‡**:

```python
model_interface_validation_fail_total = Counter(
    'model_interface_validation_fail_total',
    'Model interface validation failures',
    labelnames=['reason']
)
```

3. **å•å…ƒæµ‹è¯•**: `/tests/unit/test_model_interface_validation.py`

```python
# æµ‹è¯•åœºæ™¯
- test_model_without_predict_fails_validation
- test_model_with_dangerous_attrs_logs_warning
- test_model_oversized_fails_validation
- test_valid_model_passes_all_checks
- test_interface_validation_metric_increment
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ¥å£éªŒè¯æ•è·ç¼ºå¤±predictæ–¹æ³•
- âœ… å±é™©å±æ€§æ£€æµ‹è®°å½•æ—¥å¿—ä½†ä¸é˜»æ–­
- âœ… è¿‡å¤§æ¨¡å‹å¯¹è±¡è¢«æ‹’ç»
- âœ… `model_interface_validation_fail_total{reason}`æŒ‡æ ‡å®Œæ•´

**é¢„ä¼°å·¥æ—¶**: 3å°æ—¶

---

### Task 3.4: ä¸‰å±‚å›æ»šæœºåˆ¶

**ç›®æ ‡**: æ‰©å±•è‡³ä¿ç•™å‰ä¸‰æ¬¡æˆåŠŸæ¨¡å‹ï¼Œå¢å¼ºæ•…éšœæ¢å¤èƒ½åŠ›

**å®æ–½æ­¥éª¤**:

1. **ä¿®æ”¹**: `/src/ml/classifier.py`

```python
# å…¨å±€å˜é‡æ‰©å±•
_MODEL_PREV3: Dict[str, Any] | None = None
_MODEL_PREV3_HASH: str | None = None
_MODEL_PREV3_VERSION: str | None = None

def reload_model(...):
    global _MODEL_PREV, _MODEL_PREV2, _MODEL_PREV3
    
    # æˆåŠŸåŠ è½½åæ¨è¿›å†å²
    if success:
        _MODEL_PREV3 = _MODEL_PREV2
        _MODEL_PREV3_HASH = _MODEL_PREV2_HASH
        
        _MODEL_PREV2 = _MODEL_PREV
        _MODEL_PREV2_HASH = _MODEL_PREV_HASH
        
        _MODEL_PREV = _MODEL
        _MODEL_PREV_HASH = _MODEL_HASH
        
        _MODEL = new_model
        _MODEL_HASH = new_hash
```

2. **æ‰©å±•**: `/src/api/v1/health.py`

```python
@router.get("/health/model")
async def model_health():
    return {
        "current": {...},
        "rollback_available": {
            "level_1": _MODEL_PREV is not None,
            "level_2": _MODEL_PREV2 is not None,
            "level_3": _MODEL_PREV3 is not None
        },
        "rollback_history": [
            {"level": 1, "version": _MODEL_PREV_VERSION, "hash": _MODEL_PREV_HASH},
            {"level": 2, "version": _MODEL_PREV2_VERSION, "hash": _MODEL_PREV2_HASH},
            {"level": 3, "version": _MODEL_PREV3_VERSION, "hash": _MODEL_PREV3_HASH}
        ]
    }
```

3. **å•å…ƒæµ‹è¯•**: æ‰©å±• `/tests/unit/test_model_rollback_health.py`

```python
# æ–°å¢æµ‹è¯•
- test_three_successive_reloads_populate_all_levels
- test_failure_after_three_reloads_rolls_to_level_1
- test_double_failure_rolls_to_level_2
- test_triple_failure_rolls_to_level_3
- test_health_endpoint_shows_all_rollback_levels
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… è¿ç»­3æ¬¡æˆåŠŸreloadåï¼Œ3å±‚å†å²å…¨éƒ¨å¡«å……
- âœ… å¤±è´¥æ—¶æŒ‰å±‚çº§å›æ»š
- âœ… `/health/model`æ˜¾ç¤º3å±‚rollbackå¯ç”¨æ€§
- âœ… å•å…ƒæµ‹è¯•éªŒè¯å±‚çº§æ¨è¿›/å›é€€é€»è¾‘

**é¢„ä¼°å·¥æ—¶**: 3å°æ—¶

---

## ğŸ§ª Day 4 AM - v4çœŸå®ç‰¹å¾å®ç°

### Task 4.1: v4ç‰¹å¾çœŸå®ç®—æ³•

**ç›®æ ‡**: å°†v4çš„surface_countå’Œshape_entropyä»å ä½å®ç°æ›¿æ¢ä¸ºçœŸå®å‡ ä½•è®¡ç®—

**å®æ–½æ­¥éª¤**:

1. **ä¿®æ”¹**: `/src/core/feature_extractor.py`

```python
def _extract_v4_features(self, doc: CadDocument) -> List[float]:
    """v4å®éªŒç‰¹å¾: çœŸå®surface_count + shape_entropy"""
    
    # Surface count: åŸºäºå®ä½“å‡ ä½•ç»†åˆ†
    surface_count = 0
    for entity in doc.entities:
        if entity.type in ['SOLID', '3DSOLID']:
            # ç®€åŒ–: æ¯ä¸ªsolidä¼°ç®—6ä¸ªé¢(ç«‹æ–¹ä½“)
            surface_count += 6
        elif entity.type in ['FACE', '3DFACE']:
            surface_count += 1
        elif entity.type == 'MESH':
            # ç½‘æ ¼é¢ç‰‡æ•°
            surface_count += getattr(entity, 'face_count', 0)
    
    # Shape entropy: åŠ æƒç±»å‹é¢‘ç‡ç†µ
    type_counts = {}
    for entity in doc.entities:
        t = entity.type
        type_counts[t] = type_counts.get(t, 0) + 1
    
    total = sum(type_counts.values())
    if total == 0:
        entropy = 0.0
    else:
        # Laplaceå¹³æ»‘é¿å…log(0)
        smoothed_counts = {k: v + 1 for k, v in type_counts.items()}
        smoothed_total = total + len(smoothed_counts)
        
        entropy = 0.0
        for count in smoothed_counts.values():
            p = count / smoothed_total
            entropy -= p * math.log2(p)
        
        # å½’ä¸€åŒ–åˆ°[0, 1]: log2(n_types)æ˜¯æœ€å¤§ç†µ
        max_entropy = math.log2(len(smoothed_counts))
        entropy = entropy / max_entropy if max_entropy > 0 else 0.0
    
    return [float(surface_count), float(entropy)]
```

2. **æ€§èƒ½æµ‹è¯•**: ç¡®ä¿v4æå–è€—æ—¶å¢åŠ  â‰¤5%

```python
# /tests/performance/test_v4_extraction_performance.py
- test_v4_vs_v3_latency_difference_under_5_percent
- test_v4_surface_count_accuracy_on_known_models
- test_v4_entropy_range_validation
```

3. **æ–°å¢æŒ‡æ ‡**: å·²å­˜åœ¨`feature_extraction_latency_seconds{version}`ï¼Œç¡®ä¿v4è¢«è®°å½•

**éªŒæ”¶æ ‡å‡†**:
- âœ… v4ç‰¹å¾æå–åœ¨å¤æ‚æ¨¡å‹(>500å®ä½“)ä¸Šä» <5% æ€§èƒ½é€€åŒ–
- âœ… å•å…ƒæµ‹è¯•è¦†ç›–: ç©ºå®ä½“ã€å•å®ä½“ã€å¤šå®ä½“é«˜å¤šæ ·æ€§
- âœ… shape_entropy âˆˆ [0, 1]
- âœ… surface_countéè´Ÿæ•´æ•°

**é¢„ä¼°å·¥æ—¶**: 5å°æ—¶

**é£é™©**: 
- å¤æ‚CADæ¨¡å‹å‡ ä½•è®¡ç®—å¯èƒ½è€—æ—¶
- ç¼“è§£: æ·»åŠ `FEATURE_V4_ENABLE_STRICT=0`å¼€å…³ç¦ç”¨

---

## ğŸ”§ Day 4 PM - è¿ç§»å·¥å…·æ‰©å±•

### Task 4.2: è¿ç§»é¢„è§ˆä¸è¶‹åŠ¿

**ç›®æ ‡**: æä¾›è¿ç§»å‰çš„å½±å“é¢„è§ˆå’Œå†å²è¶‹åŠ¿åˆ†æ

**å®æ–½æ­¥éª¤**:

1. **æ–°å¢ç«¯ç‚¹**: `/api/v1/vectors/migrate/preview`

```python
class MigratePreviewRequest(BaseModel):
    ids: list[str]
    to_version: str

class MigratePreviewResponse(BaseModel):
    total: int
    dimension_changes: Dict[str, int]  # {"7->23": 10, "12->23": 5}
    top_dimension_deltas: List[Dict[str, int]]  # [{"from": 7, "to": 23, "count": 10}]
    estimated_duration_seconds: float
    warnings: List[str]  # ["5 vectors will be downgraded"]
```

2. **æ–°å¢ç«¯ç‚¹**: `/api/v1/vectors/migrate/trends`

```python
class MigrateTrendsResponse(BaseModel):
    last_k_migrations: int  # Kæ¬¡è¿ç§»
    average_migrated_ratio: float  # migrated / total
    average_skipped_ratio: float
    v4_adoption_rate: float  # to_version=v4çš„æ¯”ä¾‹
    most_common_migration: str  # "v1->v3"
```

3. **å•å…ƒæµ‹è¯•**: `/tests/unit/test_migrate_preview_trends.py`

```python
# æµ‹è¯•åœºæ™¯
- test_preview_shows_dimension_changes_summary
- test_preview_warns_on_downgrade
- test_preview_estimated_duration_reasonable
- test_trends_calculates_correct_ratios
- test_trends_v4_adoption_rate
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… Previewç«¯ç‚¹è¿”å›å®Œæ•´ç»´åº¦å˜åŒ–æ±‡æ€»
- âœ… Trendsç«¯ç‚¹åŸºäº`_VECTOR_MIGRATION_HISTORY`è®¡ç®—
- âœ… è­¦å‘ŠåŒ…å«é™çº§æç¤º

**é¢„ä¼°å·¥æ—¶**: 3å°æ—¶

---

## ğŸ“š Day 5 AM - æ–‡æ¡£å…¨é¢æ›´æ–°

### Task 5.1: ç»Ÿä¸€é”™è¯¯Schemaæ–‡æ¡£

**ç›®æ ‡**: å®Œæ•´çš„é”™è¯¯ç ã€stageã€ä¸Šä¸‹æ–‡å­—æ®µæ–‡æ¡£åŒ–

**å®æ–½æ­¥éª¤**:

1. **åˆ›å»ºæ–‡æ¡£**: `/docs/API_ERROR_CODES.md`

```markdown
# APIé”™è¯¯ç å‚è€ƒ

## é”™è¯¯å“åº”ç»“æ„

æ‰€æœ‰é”™è¯¯éµå¾ªç»Ÿä¸€æ ¼å¼:

```json
{
  "detail": {
    "code": "ERROR_CODE",
    "stage": "processing_stage",
    "message": "Human-readable message",
    "source": "input|system|external",
    "severity": "info|warning|error|critical",
    // ä¸Šä¸‹æ–‡å­—æ®µ(æ ¹æ®é”™è¯¯ç±»å‹ä¸åŒ)
    "id": "...",
    "expected": 23,
    "found": 12
  }
}
```

## é”™è¯¯ç ç´¢å¼•

| Code | HTTP Status | Stage | è¯´æ˜ | ä¸Šä¸‹æ–‡å­—æ®µ |
|------|-------------|-------|------|-----------|
| INPUT_VALIDATION_FAILED | 422 | batch_similarity | æ‰¹é‡å¤§å°è¶…é™ | batch_size, max_batch |
| DATA_NOT_FOUND | 404 | vector_delete | å‘é‡ä¸å­˜åœ¨ | id |
| DIMENSION_MISMATCH | 409 | vector_update | ç»´åº¦ä¸åŒ¹é… | expected, found, id |
| SERVICE_UNAVAILABLE | 503 | orphan_cleanup | Redisä¸å¯ç”¨ | detail, suggestion |
| ... | ... | ... | ... | ... |

## å¸¸è§Stage

- `routing`: è·¯ç”±å±‚(å¦‚410åºŸå¼ƒç«¯ç‚¹)
- `batch_similarity`: æ‰¹é‡ç›¸ä¼¼åº¦æŸ¥è¯¢
- `vector_migrate`: å‘é‡è¿ç§»
- `orphan_cleanup`: å­¤å„¿æ¸…ç†
- `model_reload`: æ¨¡å‹çƒ­é‡è½½
- `security`: å®‰å…¨æ£€æŸ¥

## ç¤ºä¾‹

### æ‰¹é‡æŸ¥è¯¢è¶…é™
```json
{
  "detail": {
    "code": "INPUT_VALIDATION_FAILED",
    "stage": "batch_similarity",
    "message": "Batch size exceeds limit",
    "batch_size": 350,
    "max_batch": 200
  }
}
```
```

2. **æ›´æ–°**: `/README.md` - é”™è¯¯å¤„ç†ç« èŠ‚

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ–‡æ¡£åŒ…å«è‡³å°‘15ä¸ªé”™è¯¯ç 
- âœ… æ¯ä¸ªé”™è¯¯ç éƒ½æœ‰ç¤ºä¾‹
- âœ… Stageæšä¸¾å®Œæ•´
- âœ… Markdownæ ¼å¼æ­£ç¡®(æ— linteré”™è¯¯)

**é¢„ä¼°å·¥æ—¶**: 3å°æ—¶

---

### Task 5.2: ç«¯ç‚¹çŸ©é˜µä¸æŒ‡æ ‡ç´¢å¼•

**ç›®æ ‡**: æä¾›APIç«¯ç‚¹çŠ¶æ€çŸ©é˜µå’ŒæŒ‡æ ‡å¿«é€ŸæŸ¥è¯¢è¡¨

**å®æ–½æ­¥éª¤**:

1. **åˆ›å»ºæ–‡æ¡£**: `/docs/API_ENDPOINT_MATRIX.md`

```markdown
# APIç«¯ç‚¹çŠ¶æ€çŸ©é˜µ

| æ¨¡å— | è·¯å¾„ | æ–¹æ³• | çŠ¶æ€ | Sunsetæ—¥æœŸ | æ›¿ä»£ç«¯ç‚¹ |
|------|------|------|------|-----------|---------|
| analyze | /api/v1/analyze | POST | âœ… Active | - | - |
| vectors | /api/v1/vectors | GET | âœ… Active | - | - |
| vectors | /api/v1/vectors/update | POST | âœ… Active | - | - |
| vectors | /api/v1/vectors/migrate | POST | âœ… Active | - | - |
| vectors | /api/v1/vectors/migrate/preview | GET | âœ… Implemented | avg_delta, median_delta, warnings | - |
| vectors | /api/v1/vectors/similarity/batch | POST | âœ… Active | - | - |
| analyze | /api/v1/analyze/vectors/distribution | GET | â›” Deprecated | 2024-11-24 | /api/v1/vectors_stats/distribution |
| ... | ... | ... | ... | ... | ... |
```

2. **åˆ›å»ºæ–‡æ¡£**: `/docs/METRICS_INDEX.md`

```markdown
# PrometheusæŒ‡æ ‡ç´¢å¼•

## åˆ†æé˜¶æ®µ

| æŒ‡æ ‡å | ç±»å‹ | æ ‡ç­¾ | PromQLç¤ºä¾‹ | è¯´æ˜ |
|--------|------|------|-----------|------|
| analysis_requests_total | Counter | status | `rate(analysis_requests_total{status="success"}[5m])` | åˆ†æè¯·æ±‚æ€»æ•° |
| analysis_stage_duration_seconds | Histogram | stage | `histogram_quantile(0.95, rate(..._bucket[5m]))` | å„é˜¶æ®µè€—æ—¶ |
| ... | ... | ... | ... | ... |

## å‘é‡å­˜å‚¨

| æŒ‡æ ‡å | ç±»å‹ | æ ‡ç­¾ | PromQLç¤ºä¾‹ | è¯´æ˜ |
|--------|------|------|-----------|------|
| vector_migrate_total | Counter | status | `sum by (status)(rate(vector_migrate_total[10m]))` | è¿ç§»çŠ¶æ€ç»Ÿè®¡ |
| vector_migrate_dimension_delta | Histogram | - | `histogram_quantile(0.5, ...)` | ç»´åº¦å˜åŒ–åˆ†å¸ƒ |
| ... | ... | ... | ... | ... |
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… ç«¯ç‚¹çŸ©é˜µåŒ…å«æ‰€æœ‰å·²çŸ¥ç«¯ç‚¹(â‰¥30ä¸ª)
- âœ… æŒ‡æ ‡ç´¢å¼•åŒ…å«æ‰€æœ‰è‡ªå®šä¹‰æŒ‡æ ‡(â‰¥40ä¸ª)
- âœ… æ¯ä¸ªæŒ‡æ ‡éƒ½æœ‰PromQLç¤ºä¾‹

**é¢„ä¼°å·¥æ—¶**: 3å°æ—¶

---

## âš™ï¸ Day 5 PM - Prometheusè§„åˆ™å›å½’

### Task 5.3: è§„åˆ™éªŒè¯ä¸CIé›†æˆ

**ç›®æ ‡**: ç¡®ä¿æ‰€æœ‰Prometheusè§„åˆ™æ–‡ä»¶é€šè¿‡promtooléªŒè¯

**å®æ–½æ­¥éª¤**:

1. **éªŒè¯è„šæœ¬**: `/scripts/validate_all_prometheus_rules.sh`

```bash
#!/bin/bash
set -e

echo "Validating Prometheus rules..."

# Find all rule files
RULE_FILES=$(find config/prometheus docs/prometheus -name "*.yml" -o -name "*.yaml")

for file in $RULE_FILES; do
  echo "Checking $file..."
  promtool check rules "$file" || {
    echo "âŒ Validation failed for $file"
    exit 1
  }
done

echo "âœ… All rules validated successfully"
```

2. **CIé›†æˆ**: æ›´æ–° `.github/workflows/ci.yml`

```yaml
- name: Validate Prometheus Rules
  run: |
    docker run --rm -v $(pwd):/rules prom/prometheus:latest \
      promtool check rules /rules/config/prometheus/alert_rules_enhanced.yml
```

3. **Makefileç›®æ ‡**: æ·»åŠ åˆ°Makefile

```makefile
prom-validate-all: ## éªŒè¯æ‰€æœ‰Prometheusè§„åˆ™æ–‡ä»¶
	@echo "$(GREEN)Validating all Prometheus rules...$(NC)"
	bash scripts/validate_all_prometheus_rules.sh
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… è„šæœ¬éªŒè¯æ‰€æœ‰.ymlè§„åˆ™æ–‡ä»¶
- âœ… CIæµç¨‹åŒ…å«è§„åˆ™éªŒè¯æ­¥éª¤
- âœ… å¤±è´¥æ—¶æ˜ç¡®æŒ‡å‡ºé”™è¯¯æ–‡ä»¶

**é¢„ä¼°å·¥æ—¶**: 1.5å°æ—¶

---

### Task 5.4: æŒ‡æ ‡ä¸€è‡´æ€§æ£€æŸ¥

**ç›®æ ‡**: è‡ªåŠ¨æ£€æµ‹æºç ä¸­å®šä¹‰çš„æŒ‡æ ‡ä¸`__all__`å¯¼å‡ºåˆ—è¡¨ä¸€è‡´æ€§

**å®æ–½æ­¥éª¤**:

1. **æ£€æŸ¥è„šæœ¬**: `/scripts/check_metrics_consistency.py`

```python
#!/usr/bin/env python3
"""éªŒè¯æŒ‡æ ‡å®šä¹‰ä¸å¯¼å‡ºä¸€è‡´æ€§"""

import re
import sys
from pathlib import Path

def find_metric_definitions(file_path):
    """æ‰«ææ–‡ä»¶ä¸­çš„æŒ‡æ ‡å®šä¹‰"""
    with open(file_path) as f:
        content = f.read()
    
    # æŸ¥æ‰¾ Counter/Histogram/Gauge å®šä¹‰
    pattern = r'(\w+)\s*=\s*(Counter|Histogram|Gauge|Summary|Info)\('
    return {match[0] for match in re.findall(pattern, content)}

def find_exported_metrics(file_path):
    """æ‰«æ__all__åˆ—è¡¨"""
    with open(file_path) as f:
        content = f.read()
    
    # æŸ¥æ‰¾ __all__ = [...]
    match = re.search(r'__all__\s*=\s*\[([\s\S]*?)\]', content)
    if not match:
        return set()
    
    exports = match.group(1)
    return {name.strip().strip('"').strip("'") for name in exports.split(',')}

def main():
    metrics_file = Path("src/utils/analysis_metrics.py")
    
    defined = find_metric_definitions(metrics_file)
    exported = find_exported_metrics(metrics_file)
    
    missing = defined - exported
    extra = exported - defined
    
    if missing:
        print(f"âŒ Metrics defined but not exported: {missing}")
        sys.exit(1)
    
    if extra:
        print(f"âš ï¸  Metrics exported but not defined: {extra}")
    
    print(f"âœ… All {len(defined)} metrics properly exported")
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

2. **Makefileé›†æˆ**:

```makefile
metrics-consistency: ## æ£€æŸ¥æŒ‡æ ‡ä¸€è‡´æ€§
	@echo "$(GREEN)Checking metrics consistency...$(NC)"
	$(PYTHON) scripts/check_metrics_consistency.py
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… è„šæœ¬æ£€æµ‹åˆ°æœªå¯¼å‡ºçš„æŒ‡æ ‡æ—¶å¤±è´¥
- âœ… è¾“å‡ºç¼ºå¤±æŒ‡æ ‡åˆ—è¡¨
- âœ… å¯é›†æˆåˆ°pre-commit hook

**é¢„ä¼°å·¥æ—¶**: 2å°æ—¶

---

## ğŸ§ª Day 6 - ç¼“å†²ä¸å›å½’éªŒè¯

### Task 6.1: æ€§èƒ½åŸºçº¿æµ‹è¯•

**ç›®æ ‡**: å»ºç«‹å…³é”®æ“ä½œçš„æ€§èƒ½åŸºå‡†

**å®æ–½æ­¥éª¤**:

1. **æ€§èƒ½æµ‹è¯•è„šæœ¬**: `/tests/performance/benchmark_suite.py`

```python
import time
from typing import List, Dict

def benchmark_single_file_analysis():
    """å•æ–‡ä»¶åˆ†æåŸºå‡†"""
    # æµ‹è¯•100æ¬¡
    times = []
    for _ in range(100):
        start = time.time()
        # è°ƒç”¨åˆ†æAPI
        times.append(time.time() - start)
    
    return {
        "operation": "single_file_analysis",
        "p50": percentile(times, 50),
        "p95": percentile(times, 95),
        "p99": percentile(times, 99)
    }

def benchmark_batch_similarity():
    """æ‰¹é‡ç›¸ä¼¼åº¦åŸºå‡†"""
    # æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°
    results = {}
    for batch_size in [5, 20, 50, 100]:
        times = []
        for _ in range(50):
            start = time.time()
            # è°ƒç”¨æ‰¹é‡ç›¸ä¼¼åº¦API
            times.append(time.time() - start)
        
        results[f"batch_{batch_size}"] = {
            "p50": percentile(times, 50),
            "p95": percentile(times, 95)
        }
    
    return results

def benchmark_vector_migration():
    """å‘é‡è¿ç§»åŸºå‡†"""
    # v1->v3è¿ç§»100ä¸ªå‘é‡
    start = time.time()
    # è°ƒç”¨è¿ç§»API
    duration = time.time() - start
    
    return {
        "operation": "migrate_100_vectors_v1_to_v3",
        "duration": duration,
        "per_vector": duration / 100
    }

if __name__ == "__main__":
    print("Running performance benchmarks...")
    
    results = {
        "single_analysis": benchmark_single_file_analysis(),
        "batch_similarity": benchmark_batch_similarity(),
        "migration": benchmark_vector_migration()
    }
    
    # ä¿å­˜ç»“æœ
    import json
    with open("reports/performance_baseline.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print("âœ… Baseline saved to reports/performance_baseline.json")
```

2. **Makefileç›®æ ‡**:

```makefile
perf-baseline: ## è¿è¡Œæ€§èƒ½åŸºçº¿æµ‹è¯•
	@echo "$(GREEN)Running performance baseline...$(NC)"
	$(PYTHON) tests/performance/benchmark_suite.py
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ€§èƒ½åŸºçº¿JSONç”Ÿæˆ
- âœ… åŒ…å«P50/P95/P99å»¶è¿Ÿ
- âœ… æ‰¹é‡ç›¸ä¼¼åº¦ä¸åŒå¤§å°éƒ½æœ‰åŸºå‡†

**é¢„ä¼°å·¥æ—¶**: 3å°æ—¶

---

### Task 6.2: å›å½’æµ‹è¯•å¥—ä»¶

**ç›®æ ‡**: ç¡®ä¿æ ¸å¿ƒåŠŸèƒ½æ— çŠ¶æ€è€¦åˆé—®é¢˜

**å®æ–½æ­¥éª¤**:

1. **å›å½’æµ‹è¯•é€‰æ‹©**: `/tests/regression/critical_path_tests.py`

```python
import pytest
import random

# å®šä¹‰30ä¸ªå…³é”®æµ‹è¯•
CRITICAL_TESTS = [
    "test_analyze_dxf_basic",
    "test_vector_migrate_v1_to_v3",
    "test_batch_similarity_basic",
    "test_orphan_cleanup_dry_run",
    "test_model_reload_with_security",
    # ... 25 more
]

@pytest.mark.regression
def test_random_order_execution():
    """éšæœºé¡ºåºæ‰§è¡Œå…³é”®æµ‹è¯•ï¼Œæ£€æµ‹çŠ¶æ€ä¾èµ–"""
    shuffled = CRITICAL_TESTS.copy()
    random.shuffle(shuffled)
    
    for test_name in shuffled:
        # åŠ¨æ€è°ƒç”¨æµ‹è¯•
        result = pytest.main(["-v", f"-k {test_name}"])
        assert result == 0, f"Test {test_name} failed in random order"
```

2. **è¿è¡Œå‘½ä»¤**:

```bash
make regression-test ## è¿è¡Œå›å½’æµ‹è¯•å¥—ä»¶
pytest -v tests/regression/critical_path_tests.py
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… 30ä¸ªå…³é”®æµ‹è¯•éšæœºé¡ºåºæ‰§è¡Œ5æ¬¡å…¨éƒ¨é€šè¿‡
- âœ… æ— çŠ¶æ€ä¾èµ–å¤±è´¥

**é¢„ä¼°å·¥æ—¶**: 2å°æ—¶

---

## ğŸ“‹ ä¾èµ–å…³ç³»å›¾

```mermaid
graph TD
    A[Day 1 AM âœ…] --> B[Day 1 PM]
    B --> C[Day 2 AM]
    C --> D[Day 2 PM]
    D --> E[Day 3 AM]
    E --> F[Day 3 PM]
    F --> G[Day 4 AM]
    G --> H[Day 4 PM]
    H --> I[Day 5 AM]
    I --> J[Day 5 PM]
    J --> K[Day 6]
    
    B -.-> E[æ¨¡å‹å¥åº·ç«¯ç‚¹éœ€è¦å…ˆå®ç°]
    C -.-> D[Dashboardä¾èµ–æ–°æŒ‡æ ‡]
    E -.-> F[å®‰å…¨å¢å¼ºä¾èµ–å®‰å…¨æ–‡æ¡£]
    G -.-> H[è¿ç§»å·¥å…·ä¾èµ–v4ç‰¹å¾]
```

---

## âš ï¸ é£é™©ä¸ç¼“è§£ç­–ç•¥

| é£é™© | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|------|---------|
| v4æ€§èƒ½å›é€€ | ä¸­ | é«˜ | æ·»åŠ `FEATURE_V4_ENABLE_STRICT=0`å¼€å…³ |
| å®‰å…¨ç™½åå•è¿‡ä¸¥ | ä½ | ä¸­ | æä¾›`permissive`å›é€€æ¨¡å¼ |
| Faissæµ‹è¯•ç¯å¢ƒé—®é¢˜ | ä¸­ | ä½ | Mock FAISSå¯¼å…¥ï¼Œä¸ä¾èµ–å®é™…å®‰è£… |
| ç¼“å­˜è°ƒä¼˜å»ºè®®è¯¯å¯¼ | ä½ | ä¸­ | æ ‡æ³¨`experimental=true`å­—æ®µ |
| Dashboard JSONæ ¼å¼é”™è¯¯ | ä½ | ä½ | ä½¿ç”¨promtoolæå‰éªŒè¯ |

---

## âœ… æ€»ä½“éªŒæ”¶æ ‡å‡†

### ä»£ç è´¨é‡
- âœ… æ‰€æœ‰æ–°å¢ç«¯ç‚¹æœ‰å®Œæ•´çš„ç±»å‹æç¤º
- âœ… ç»“æ„åŒ–é”™è¯¯ç»Ÿä¸€ä½¿ç”¨`build_error`
- âœ… flake8é€šè¿‡ (â‰¤100åˆ—)
- âœ… mypyæ— æ–°å¢é”™è¯¯
- âœ… black/isortæ ¼å¼åŒ–é€šè¿‡

### æµ‹è¯•è¦†ç›–
- âœ… æ–°å¢è¡Œè¦†ç›–ç‡ â‰¥80%
- âœ… å…³é”®è¿ç§»ä¸å®‰å…¨åˆ†æ”¯ â‰¥90%
- âœ… æ‰€æœ‰æ–°ç«¯ç‚¹æœ‰é›†æˆæµ‹è¯•

### æŒ‡æ ‡
- âœ… æ–°å¢æŒ‡æ ‡åœ¨`/metrics`å¯è§
- âœ… `__all__`å¯¼å‡ºä¸€è‡´æ€§
- âœ… æ— æ‹¼å†™å·®å¼‚

### æ–‡æ¡£
- âœ… CHANGELOGæ›´æ–°
- âœ… READMEæ¸²æŸ“æ­£å¸¸
- âœ… APIæ–‡æ¡£å®Œæ•´

---

## ğŸ“Š æ—¶é—´é¢„ç®—æ€»è§ˆ

| é˜¶æ®µ | é¢„ä¼°å·¥æ—¶ | ç¼“å†²æ—¶é—´ | æ€»è®¡ |
|------|---------|---------|------|
| Day 1 PM | 5.5h | 1h | 6.5h |
| Day 2 | 11h | 1.5h | 12.5h |
| Day 3 | 12h | 1.5h | 13.5h |
| Day 4 | 8h | 1h | 9h |
| Day 5 | 9.5h | 1.5h | 11h |
| Day 6 | 5h | 3h | 8h (å…¨å¤©ç¼“å†²) |

**æ€»è®¡**: ~60.5å°æ—¶ (çº¦7.5ä¸ªå·¥ä½œæ—¥ï¼Œå«20%ç¼“å†²)

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³å¼€å§‹ (Day 1 PM)
1. âœ… éªŒè¯Day 1 AMå®ŒæˆçŠ¶æ€
2. ğŸ”„ åˆ›å»º `test_model_rollback_health.py`
3. ğŸ”„ æ‰©å±• `/health/model` ç«¯ç‚¹
4. ğŸ”„ åˆ›å»º `test_backend_reload_failures.py`

### å‡†å¤‡å·¥ä½œ
- [ ] ç¡®è®¤å¼€å‘ç¯å¢ƒ: Python 3.10+, Docker, Redis
- [ ] æ‹‰å–æœ€æ–°ä»£ç åˆ†æ”¯
- [ ] è¿è¡Œç°æœ‰æµ‹è¯•ç¡®ä¿åŸºçº¿é€šè¿‡: `make test`
- [ ] æ£€æŸ¥Prometheus/Grafanaå¯è®¿é—®æ€§

---

**æ–‡æ¡£ç»´æŠ¤**: æœ¬æ–‡æ¡£åº”åœ¨æ¯ä¸ªTaskå®Œæˆåæ›´æ–°è¿›åº¦æ ‡è®°

**é—®é¢˜åé¦ˆ**: å¦‚é‡åˆ°é˜»å¡æˆ–éœ€æ±‚å˜æ›´ï¼ŒåŠæ—¶æ›´æ–°é£é™©è¡¨å¹¶è°ƒæ•´è®¡åˆ’
